import{_ as t,c as i,I as o,j as e,a as n,a8 as s,D as l,o as r}from"./chunks/framework.CLo04awk.js";const I=JSON.parse(`{"title":"Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels","description":"","frontmatter":{"head":[["script",{"charset":"UTF-8","id":"LA_COLLECT","src":"//sdk.51.la/js-sdk-pro.min.js"}],["script",{},"typeof LA !== 'undefined' && LA.init({\\"id\\":\\"3LPXyA1ZitpV3O1s\\",\\"ck\\":\\"3LPXyA1ZitpV3O1s\\",\\"autoTrack\\":true,\\"hashMode\\":true})"]]},"headers":[],"relativePath":"column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.md","filePath":"column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.md","lastUpdated":1742900169000}`),c={name:"column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.md"},h=e("h1",{id:"towards-open-vocabulary-semantic-segmentation-without-semantic-labels",tabindex:"-1"},[n("Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels "),e("a",{class:"header-anchor",href:"#towards-open-vocabulary-semantic-segmentation-without-semantic-labels","aria-label":'Permalink to "Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"'},"​")],-1),d=s('<p>韩国科学技术院、Korea University、Google Research</p><div class="tip custom-block"><p class="custom-block-title">TIP</p><p>晦涩</p></div><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which additionally require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption supervised methods in open-vocabulary semantic segmentation. Project page is available at <a href="https://cvlab-kaist.github.io/PixelCLIP" target="_blank" rel="noreferrer">https://cvlab-kaist.github.io/PixelCLIP</a></p><h2 id="翻译" tabindex="-1">翻译 <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;翻译&quot;">​</a></h2><p>像CLIP这样的大规模视觉语言模型已经为图像级任务展示了令人印象深刻的开放词汇表能力，在识别存在的对象方面表现出色。然而，他们在像语义分割这样的像素级识别任务中挣扎，这还需要理解物体的位置。在这项工作中，我们提出了一种新的方法，PixelCLIP，通过引导模型在哪里来调整CLIP图像编码器以进行像素级理解，这是使用从视觉基础模型(如SAM和DINO)生成的未标记图像和掩码来实现的。为了解决在没有语义标签的情况下利用掩码的挑战，我们设计了一种使用可学习类名来获取一般语义概念的在线聚类算法。在开放词汇语义分割中，与字幕监督方法相比，PixelCLIP表现出了显著的性能改进和竞争结果。项目页面可访问<a href="https://cvlab-kaist.github.io/PixelCLIP" target="_blank" rel="noreferrer">https://cvlab-kaist.github.io/PixelCLIP</a></p><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><p><strong>语义分割</strong>是计算机视觉中的基础任务，旨在为图像中每个像素识别类别标签。但传统分割数据集获取密集标注的语义标签需大量人力，限制了其可扩展性。 近年来，大规模预训练视觉-语言模型（如CLIP、ALIGN）推动了开放词汇语义分割的发展，能将语义分割推广到无限类别的范围。然而，这些模型在进行语义分割时仍需像素级语义标签。 部分研究尝试在无密集标注语义标签的情况下进行开放词汇语义分割，利用图像级语义标签（如图像标题）增强预训练视觉 - 语言模型。但图像标题通常只提供图像中有什么，而不包含物体位置信息，导致模型只能隐式学习物体位置，性能不佳或需大量图像-标题对来弥补弱监督。 因此，本文提出一种新方法PixelCLIP，不依赖语义标签，而是利用视觉基础模型（如DINO、SAM）生成的掩码引导预训练视觉 - 语言模型（如CLIP）关注物体位置，以实现开放词汇语义分割，解决现有方法的不足。</p><h2 id="研究现状" tabindex="-1">研究现状 <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;研究现状&quot;">​</a></h2><ul><li><strong>大模型助力开放词汇语义分割</strong>：大规模预训练视觉 - 语言模型，如CLIP和ALIGN，推动了开放词汇语义分割的发展，可将语义分割推广到无限类别的范围。</li><li><strong>弱监督方法兴起</strong>：部分研究尝试在无密集标注语义标签的情况下进行开放词汇语义分割，利用图像级语义标签（如图像字幕）增强预训练模型，但存在信息缺失问题。</li><li><strong>视觉基础模型应用</strong>：如DINO和SAM等视觉基础模型可生成细粒度掩码，但掩码无语义标签。</li></ul><h2 id="提出的模型" tabindex="-1">提出的模型 <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;提出的模型&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-25_16-05-19.png" alt="Snipaste_2025-03-25_16-05-19" loading="lazy"></p><ol><li>掩码集成到CLIP特征 <ul><li>由于没有语义标签，作者通过掩码池化将无标签掩码集成到CLIP图像特征图中，得到每个掩码的CLIP特征。</li><li>利用这些特征计算图像 - 掩码相似度图，并使用二元掩码损失对模型进行监督，以微调CLIP图像编码器。</li><li>为了缓解相似度图和掩码之间的分辨率差距，使用轻量级解码器进行上采样。</li></ul></li><li>掩码的语义聚类 <ul><li><strong>在线聚类</strong>：为了解决DINO和SAM生成的掩码过度分割问题，作者提出使用可学习的类提示将语义相似的掩码聚类成全局共享的簇。每个簇由CLIP文本特征表示为质心，通过优化图像 - 文本相似度和熵正则化来确定掩码的分配。</li><li><strong>动量编码器</strong>：为了稳定训练过程，使用动量编码器来提取掩码池化特征，避免训练过程中的不稳定性和预训练知识的遗忘。</li></ul></li></ol><h2 id="实验-compared-with-sota" tabindex="-1">实验（Compared with SOTA） <a class="header-anchor" href="#实验-compared-with-sota" aria-label="Permalink to &quot;实验（Compared with SOTA）&quot;">​</a></h2><ol><li><strong>开放词汇语义分割</strong>：PixelCLIP在所有基准测试中均显著优于CLIP，平均mIoU提高了16.2。与使用图像级监督的方法相比，PixelCLIP在不使用语义标签的情况下表现出竞争力，甚至在某些基准测试中超过了这些方法。</li><li><strong>零样本掩码分类</strong>：PixelCLIP可以直接应用于现有的使用CLIP作为零样本掩码分类器的框架，通过简单替换CLIP模型和权重，即可带来即时的性能提升。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-25_16-08-46.png" alt="Snipaste_2025-03-25_16-08-46" loading="lazy"></p><h2 id="实验-ablation-experiments" tabindex="-1">实验（Ablation Experiments）🥇 <a class="header-anchor" href="#实验-ablation-experiments" aria-label="Permalink to &quot;实验（Ablation Experiments）:1st_place_medal:&quot;">​</a></h2><ol><li><strong>消融研究</strong>：通过消融研究验证了模型设计的关键组件，包括全局语义聚类、可学习类提示和动量编码器的重要性。同时，研究了聚类数量、可学习提示令牌长度等因素对模型性能的影响。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-25_16-09-10.png" alt="Snipaste_2025-03-25_16-09-10" loading="lazy"></p><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>作者提出了<strong>PixelCLIP框架用于开放词汇语义分割</strong>，得出以下结论：</p><ol><li><strong>方法有效性</strong>：PixelCLIP通过利用无标签图像和掩码微调预训练<strong>视觉-语言模型</strong>，在开放词汇语义分割任务中显著提升了CLIP的性能，平均mIoU提高了16.2，且优于使用图像级语义标签的方法。</li><li><strong>聚类与提示学习作用</strong>：提出的全局语义聚类和可学习类提示方法，能有效解决无标签掩码带来的挑战，使模型学习到通用语义概念，对框架性能提升至关重要。</li><li><strong>适用性与改进</strong>：PixelCLIP可应用于现有利用CLIP进行零样本掩码分类的框架，简单替换CLIP即可带来即时改进。</li></ol>',22);function m(p,g,u,P,b,_){const a=l("ArticleMetadata");return r(),i("div",null,[h,o(a),d])}const C=t(c,[["render",m]]);export{I as __pageData,C as default};
