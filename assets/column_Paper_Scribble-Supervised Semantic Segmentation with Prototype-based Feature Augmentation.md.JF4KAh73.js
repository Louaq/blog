import{_ as a,c as r,I as n,j as e,a as i,a8 as o,D as s,o as l}from"./chunks/framework.CLo04awk.js";const A=JSON.parse(`{"title":"Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation","description":"","frontmatter":{"head":[["script",{"charset":"UTF-8","id":"LA_COLLECT","src":"//sdk.51.la/js-sdk-pro.min.js"}],["script",{},"typeof LA !== 'undefined' && LA.init({\\"id\\":\\"3LPXyA1ZitpV3O1s\\",\\"ck\\":\\"3LPXyA1ZitpV3O1s\\",\\"autoTrack\\":true,\\"hashMode\\":true})"]]},"headers":[],"relativePath":"column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.md","filePath":"column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.md","lastUpdated":1744265722000}`),g={name:"column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.md"},p=e("h1",{id:"scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation",tabindex:"-1"},[i("Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation "),e("a",{class:"header-anchor",href:"#scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation","aria-label":'Permalink to "Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"'},"​")],-1),h=o('<p><strong>Hohai University, Nanjing, China</strong></p><p><strong>RMIT University, Melbourne, Australia</strong></p><h2 id="摘要" tabindex="-1"><strong>摘要</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要**&quot;">​</a></h2><p><strong>Scribble-supervised semantic segmentation</strong> presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at <a href="https://github.com/TranquilChan/PFA" target="_blank" rel="noreferrer">https://github.com/TranquilChan/PFA</a>.</p><h2 id="翻译" tabindex="-1"><strong>翻译</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译**&quot;">​</a></h2><p>**涂鸦监督语义分割（Scribble-supervised semantic segmentation）**提出了一种经济高效的训练方法，通过使用涂鸦生成的标注进行模型训练。该方法因能在显著降低标注成本的同时实现高性能表现，因此备受研究人员推崇。其核心原理是通过将已标注像素的信息传递至相邻未标注区域，从而完成整幅图像的语义分割。然而，我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性。针对这一局限性，本文提出基于原型（prototype）的特征增强方法，通过挖掘特征原型（feature prototypes）的统计特性来强化涂鸦监督效果。实验表明，我们的方法在 PASCAL VOC 2012 数据集的涂鸦监督语义分割任务中达到了当前最佳水平，相关代码已开源：<a href="https://github.com/TranbilChan/PFA%E3%80%82" target="_blank" rel="noreferrer">https://github.com/TranbilChan/PFA。</a></p><h2 id="研究背景" tabindex="-1"><strong>研究背景</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景**&quot;">​</a></h2><p><strong>标注成本问题</strong>：深度学习技术推动了深度神经网络在图像分割的发展，但是，对于标注像素级别的样本需要大量的人力和财力，并且其标注过程也非常繁琐。因此，研究者越来越关注利用涂鸦标签进行监督学习的方法。<strong>涂鸦标签属于弱监督学习</strong>，相比像素级标注，能显著减少标注工作量、提高效率，且比点、边界框和图像级标签提供更多关键语义信息。</p><p><strong>现有方法的局限性</strong>：现有涂鸦监督语义分割方法主要依赖<strong>正则化损失、一致性损失、伪建议、辅助任务和标签扩散</strong>等，但这些方法存在一定缺陷。例如，正则化方法常忽略利用高层语义信息，一致性损失未在类别层面提供直接监督，伪标签方法耗时，辅助任务会引入额外数据和预测误差，标签扩散主要依赖局部信息，且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用。 基于以上背景，作者提出基于原型的特征增强方法，以解决现有方法的不足，提高涂鸦监督语义分割的性能。</p><h2 id="研究现状" tabindex="-1"><strong>研究现状</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状**&quot;">​</a></h2><ul><li><p><strong>标注方式</strong>：图像语义分割任务训练通常需大量高质量标注样本，像素级标注耗时耗力，因此弱监督学习方法受关注，如使用涂鸦、点、边界框和图像级标签等。其中，涂鸦监督能提供更多关键语义信息，表现更优。</p></li><li><p><strong>现有方法</strong>：现有涂鸦监督语义分割方法主要依赖正则化损失、一致性损失、伪建议、辅助任务和标签扩散等，但这些方法存在一定局限性。</p></li><li><p><strong>原型方法</strong>：特征原型在计算机视觉任务中用于增强模型识别能力，部分方法在弱监督语义分割中探索了原型的使用，但未充分发挥其特征增强和引导作用。</p></li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型**&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" alt="Snipaste_2025-03-11_19-29-04" loading="lazy"></p><ul><li><strong>特征提取</strong>：使用基于Mix Transformer的编码器（Segformer中的MiT-B1）提取初始特征图。</li><li><strong>初始预测</strong>：将特征图输入解码器生成语义分割预测图，通过部分交叉熵损失（partial cross-entropy loss），利用涂鸦标签进行监督，细化预测结果。</li><li><strong>原型提取与更新</strong>：从初始预测图的高置信区域中提取对应特征向量，通过加权平均形成局部原型。在训练迭代中，局部原型动态更新全局原型。</li><li><strong>特征增强</strong>：使用局部和全局原型通过原型特征增强器对初始特征进行增强。</li><li><strong>一致性监督</strong>：将增强后的特征图再次通过解码器生成增强预测图，使用一致性损失（consistency loss）对初始预测图和增强预测图进行约束。</li></ul><h2 id="实验过程-compared-with-sota" tabindex="-1"><strong>实验过程（Compared with SOTA）</strong> <a class="header-anchor" href="#实验过程-compared-with-sota" aria-label="Permalink to &quot;**实验过程（Compared with SOTA）**&quot;">​</a></h2><p>数据集：<strong>PASCAL-Scribble</strong></p><ul><li>选择<strong>MiT-B1</strong>作为骨干网络，与现有方法在<strong>PASCAL VOC 2012</strong>验证集上进行比较。</li><li>与当前最先进的方法TEL相比，尽管MiT-B1骨干网络在全监督数据集上的性能稍弱，但该方法的mIoU仍提高了0.6%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" alt="Snipaste_2025-03-11_19-33-24" loading="lazy"></p><h2 id="实验过程-ablation-experiments" tabindex="-1"><strong>实验过程（Ablation Experiments）</strong> <a class="header-anchor" href="#实验过程-ablation-experiments" aria-label="Permalink to &quot;**实验过程（Ablation Experiments）**&quot;">​</a></h2><ul><li><strong>各组件有效性</strong>：以仅使用部分交叉熵损失作为基线，对<strong>局部原型增强</strong>和<strong>全局原型增强</strong>方法进行消融实验。结果表明，同时使用两种原型增强时性能最佳，mIoU比基线提高了10.4%。</li><li><strong>原型设置</strong>：实验发现，当每个类别的全局原型数量增加到约5时，mIoU的增加趋于饱和；在原型提取时，k百分比为8%时方法性能较好。</li><li><strong>骨干网络影响</strong>：研究了不同骨干网络对方法的影响，发现基于Transformer的骨干网络在效率和性能上限方面表现更优。使用MiT - B5时，mIoU达到81.5%，显著超过现有方法。</li></ul><h2 id="结论" tabindex="-1"><strong>结论</strong> <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;**结论**&quot;">​</a></h2><p>作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割，得出以下结论：</p><ul><li><p>从<strong>涂鸦监督</strong>初始结果的置信部分提取原型，利用这些原型增强初始特征，并根据涂鸦监督的具体情况采用不同原型策略，能以正确分类像素的原型引导错误分类像素的分类，提升预测性能。</p></li><li><p>实验结果表明，该方法在PASCAL VOC 2012数据集上达到了最先进的性能，相比当前最优方法TEL，使用稍弱的骨干网络MiT-B1仍使mIoU提高了0.6%。</p></li><li><p><strong>未来计划将此方法应用于其他任务，以挖掘其巨大潜力和应用价值 （下一个创新点）</strong></p></li></ul>',23);function c(d,m,u,b,S,_){const t=s("ArticleMetadata");return l(),r("div",null,[p,n(t),h])}const P=a(g,[["render",c]]);export{A as __pageData,P as default};
