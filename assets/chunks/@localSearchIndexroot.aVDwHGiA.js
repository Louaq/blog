const e='{"documentCount":160,"nextId":160,"documentIds":{"0":"/blog/api-examples.html#runtime-api-examples","1":"/blog/api-examples.html#results","2":"/blog/api-examples.html#theme-data","3":"/blog/api-examples.html#page-data","4":"/blog/api-examples.html#page-frontmatter","5":"/blog/api-examples.html#more","6":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation","7":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#摘要","8":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#翻译","9":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#研究背景","10":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#研究现状","11":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#提出的模型","12":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#实验-compared-with-sota","13":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#实验-ablation-study","14":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#结论","15":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation","16":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#南京信息工程大学、青海师范大学、澳门大学、中国科学院","17":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#摘要","18":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#翻译","19":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#研究背景","20":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#研究现状","21":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#提出的模型","22":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#实验-compared-with-the-state-of-the-art-models-and-ablation-experiments","23":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#comparison-with-the-state-of-the-arts","24":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#ablation-experiments","25":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#结论","26":"/blog/column/Paper/DGSS.html#stronger-fewer-superior-harnessing-vision-foundation-models-for-domain-generalized-semantic-segmentation-dgss","27":"/blog/column/Paper/DGSS.html#中国科学技术大学-上海人工智能实验室","28":"/blog/column/Paper/High_Quality_Segmentation.html#high-quality-segmentation-for-ultra-high-resolution-images","29":"/blog/column/Paper/High_Quality_Segmentation.html#香港中文大学-adobe-等","30":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#learning-generalized-medical-image-segmentation-from-decoupled-feature-queries","31":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#摘要","32":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#翻译","33":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#研究背景","34":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#研究现状","35":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#提出的模型","36":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#实验-compared-with-sota","37":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#实验-ablation-experiments","38":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#结论","39":"/blog/column/Paper/Night-time_Semantic_Segmentation.html#disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement","40":"/blog/column/Paper/PAT.html#prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation","41":"/blog/column/Paper/PAT.html#中科院","42":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation","43":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#摘要","44":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#翻译","45":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#研究背景","46":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#研究现状","47":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#提出的模型","48":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#实验-compared-with-sota","49":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#实验-alabtion-experiments","50":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#结论","51":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#prompting-multi-modal-image-segmentation-with-semantic-grouping","52":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#university-of-chinese-academy-of-sciences","53":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#摘要","54":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#翻译","55":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#研究背景","56":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#研究现状","57":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#提出的模型","58":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#实验过程-与sota方法的对比","59":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#实验过程-消融实验","60":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation","61":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#摘要","62":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#翻译","63":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#研究背景","64":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#研究现状","65":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#提出的模型","66":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#实验-compared-with-sota","67":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#实验-ablation-experiments","68":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#结论","69":"/blog/column/Paper/SED.html#sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation","70":"/blog/column/Paper/SED.html#天津大学-重庆大学等","71":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#scaling-up-multi-domain-semantic-segmentation-with-sentence-embeddings","72":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#摘要","73":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#翻译","74":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#研究背景","75":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#研究现状","76":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#提出的模型","77":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#实验-compared-with-sota","78":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#实验-ablation-experiments","79":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#结论","80":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation","81":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#摘要","82":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#翻译","83":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#研究背景","84":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#研究现状","85":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#提出的模型","86":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#实验过程-compared-with-sota","87":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#实验过程-ablation-experiments","88":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#结论","89":"/blog/column/Paper/Segment Anything.html#segment-anything","90":"/blog/column/Paper/Segment Anything.html#meta-ai","91":"/blog/column/Paper/Self-supervised_ViT.html#self-supervised-vision-transformers-for-semantic-segmentation","92":"/blog/column/Paper/Self-supervised_ViT.html#摘要","93":"/blog/column/Paper/Self-supervised_ViT.html#翻译","94":"/blog/column/Paper/Self-supervised_ViT.html#研究背景","95":"/blog/column/Paper/Self-supervised_ViT.html#研究现状","96":"/blog/column/Paper/Self-supervised_ViT.html#提出的模型","97":"/blog/column/Paper/Self-supervised_ViT.html#实验-compared-with-sota","98":"/blog/column/Paper/Self-supervised_ViT.html#实验-ablation-experiments","99":"/blog/column/Paper/Self-supervised_ViT.html#结论","100":"/blog/column/Paper/Visual Studio Code latex.html#vscode配置latex环境","101":"/blog/column/Paper/Visual Studio Code latex.html#_1-tex-live-下载与安装","102":"/blog/column/Paper/Visual Studio Code latex.html#_2-vscode下载与安装","103":"/blog/column/Paper/Visual Studio Code latex.html#_3-中文语言环境配置","104":"/blog/column/Paper/Visual Studio Code latex.html#_4-latex的支持插件-latex-workshop安装","105":"/blog/column/Paper/Visual Studio Code latex.html#_5-打开latex环境设置页面","106":"/blog/column/Paper/Visual Studio Code latex.html#_6-latex环境的代码配置","107":"/blog/column/Paper/Visual Studio Code latex.html#_6-1-latex配置代码展示","108":"/blog/column/Paper/Visual Studio Code latex.html#_6-2-latex配置代码解读","109":"/blog/column/Paper/Visual Studio Code latex.html#_7-tex文件编译","110":"/blog/column/Paper/Visual Studio Code latex.html#_7-1-tex测试文件下载","111":"/blog/column/Paper/Visual Studio Code latex.html#_7-2-tex-测试文件编译","112":"/blog/column/Paper/Visual Studio Code latex.html#_8-sumatrapdf-安装设置-可选","113":"/blog/column/Paper/Visual Studio Code latex.html#_8-1-sumatrapdf下载与安装","114":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-使用sumatrapdf查看的代码配置","115":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-1-代码展示","116":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-2-代码解读","117":"/blog/column/Paper/Visual Studio Code latex.html#_9-sumatrapdf-的使用","118":"/blog/column/Paper/Visual Studio Code latex.html#_10-pdf-内部查看与外部查看的切换","119":"/blog/column/Paper/Visual Studio Code latex.html#_11-个人完整配置","120":"/blog/column/Paper/#论文阅读笔记","121":"/blog/column/Pytorch/#pytorch笔记","122":"/blog/column/deepLearning/#深度学习笔记","123":"/blog/column/image_segmentation/20250224-语义分割概述.html#语义分割概述","124":"/blog/column/image_segmentation/20250224-语义分割概述.html#_1-什么是语义分割","125":"/blog/column/image_segmentation/20250224-语义分割概述.html#_2-模型的输入和输出","126":"/blog/column/image_segmentation/20250224-语义分割概述.html#_3-常见的分割模型","127":"/blog/column/image_segmentation/20250224-语义分割概述.html#_4-语义分割的思路","128":"/blog/column/image_segmentation/20250224-语义分割概述.html#_5-评价指标","129":"/blog/column/image_segmentation/20250312-Pytorch教程.html#pytorch教程","130":"/blog/column/image_segmentation/220250224-语义分割上采样.html#上采样","131":"/blog/column/image_segmentation/FCN模型讲解.html#fcn模型讲解","132":"/blog/column/image_segmentation/#图像分割学习笔记","133":"/blog/column/image_segmentation/图像分割基础.html#_1-基本概念","134":"/blog/column/image_segmentation/图像分割基础.html#_1-1-什么是图像分割","135":"/blog/column/image_segmentation/图像分割基础.html#_1-2-图像分割的应用场景","136":"/blog/column/image_segmentation/图像分割基础.html#_1-3-图像分割的前景和背景","137":"/blog/column/image_segmentation/图像分割基础.html#_1-4-图像分割的三个层次","138":"/blog/column/image_segmentation/图像分割基础.html#_2-经典数据集","139":"/blog/column/image_segmentation/图像分割基础.html#_2-1-pascal数据集","140":"/blog/column/image_segmentation/图像分割基础.html#_2-1cityscape-用于自动驾驶场景","141":"/blog/column/image_segmentation/图像分割基础.html#_2-3-coco数据集","142":"/blog/column/image_segmentation/图像分割基础.html#_3-评估指标和优化目标","143":"/blog/column/image_segmentation/图像分割基础.html#_3-1-语义分割评估指标","144":"/blog/column/image_segmentation/图像分割基础.html#_3-2-语义分割常用优化目标","145":"/blog/column/image_segmentation/图像分割基础.html#_4-上采样","146":"/blog/column/image_segmentation/图像分割基础.html#_4-1-图像分割网络的两个模块","147":"/blog/column/image_segmentation/图像分割基础.html#_4-2-上采样实现方法-插值法","148":"/blog/column/image_segmentation/图像分割基础.html#_4-3-典型的图像分割网络","149":"/blog/column/image_segmentation/语义分割基础模型.html#fcn","150":"/blog/column/image_segmentation/语义分割基础模型.html#fcn基本原理","151":"/blog/column/image_segmentation/语义分割基础模型.html#fcn细节","152":"/blog/column/image_segmentation/语义分割基础模型.html#fcn结果","153":"/blog/column/image_segmentation/语义分割基础模型.html#segnet","154":"/blog/column/image_segmentation/语义分割基础模型.html#segnet的基本原理","155":"/blog/column/image_segmentation/语义分割基础模型.html#unet","156":"/blog/markdown-examples.html#markdown-extension-examples","157":"/blog/markdown-examples.html#syntax-highlighting","158":"/blog/markdown-examples.html#custom-containers","159":"/blog/markdown-examples.html#more"},"fieldIds":{"title":0,"titles":1,"text":2},"fieldLength":{"0":[3,1,52],"1":[1,3,1],"2":[2,4,2],"3":[2,4,2],"4":[2,4,2],"5":[1,3,11],"6":[11,1,14],"7":[1,11,162],"8":[1,11,43],"9":[1,11,28],"10":[1,11,28],"11":[1,11,55],"12":[5,11,30],"13":[4,11,7],"14":[1,11,16],"15":[12,1,1],"16":[5,12,1],"17":[2,12,103],"18":[2,12,33],"19":[2,12,34],"20":[2,12,16],"21":[2,12,73],"22":[12,12,1],"23":[6,24,6],"24":[2,24,66],"25":[2,12,24],"26":[15,1,1],"27":[2,15,259],"28":[7,1,1],"29":[3,1,341],"30":[9,1,8],"31":[1,9,164],"32":[1,9,38],"33":[1,9,19],"34":[1,9,15],"35":[1,9,35],"36":[5,9,39],"37":[4,9,25],"38":[1,9,18],"39":[10,1,284],"40":[11,1,1],"41":[1,11,379],"42":[9,1,7],"43":[1,9,130],"44":[1,9,43],"45":[1,9,35],"46":[1,9,14],"47":[1,9,34],"48":[5,9,27],"49":[4,9,58],"50":[1,9,29],"51":[8,1,1],"52":[5,8,1],"53":[2,8,133],"54":[2,8,33],"55":[2,8,25],"56":[2,8,15],"57":[2,8,64],"58":[3,8,10],"59":[3,8,43],"60":[10,1,8],"61":[1,10,109],"62":[1,10,37],"63":[1,10,27],"64":[1,10,26],"65":[1,10,63],"66":[5,10,30],"67":[4,10,25],"68":[1,10,22],"69":[10,1,1],"70":[2,10,332],"71":[9,1,1],"72":[1,9,189],"73":[1,9,56],"74":[1,9,1],"75":[1,9,1],"76":[1,9,1],"77":[5,9,1],"78":[4,9,1],"79":[1,9,1],"80":[9,1,8],"81":[1,9,98],"82":[1,9,34],"83":[1,9,31],"84":[1,9,21],"85":[1,9,28],"86":[5,9,14],"87":[4,9,23],"88":[1,9,19],"89":[2,1,1],"90":[2,2,214],"91":[7,1,1],"92":[1,7,135],"93":[1,7,44],"94":[1,7,26],"95":[1,7,19],"96":[1,7,87],"97":[5,7,25],"98":[4,7,33],"99":[1,7,18],"100":[1,1,63],"101":[4,1,132],"102":[2,1,30],"103":[2,1,33],"104":[4,1,34],"105":[2,1,34],"106":[2,1,1],"107":[3,3,105],"108":[3,3,257],"109":[2,1,1],"110":[3,3,75],"111":[4,3,88],"112":[5,1,31],"113":[3,6,11],"114":[3,6,1],"115":[4,7,45],"116":[3,7,105],"117":[3,1,39],"118":[3,1,9],"119":[2,1,158],"120":[1,1,2],"121":[1,1,2],"122":[1,1,3],"123":[1,1,1],"124":[2,1,28],"125":[2,1,28],"126":[2,1,11],"127":[2,1,7],"128":[2,1,29],"129":[1,1,1],"130":[1,1,94],"131":[1,1,105],"132":[1,1,1],"133":[2,1,1],"134":[2,2,2],"135":[3,2,4],"136":[3,2,2],"137":[3,2,1],"138":[2,1,1],"139":[3,2,1],"140":[4,2,1],"141":[3,2,1],"142":[2,1,1],"143":[3,2,1],"144":[3,2,1],"145":[2,1,1],"146":[3,2,1],"147":[4,2,1],"148":[3,2,1],"149":[1,1,1],"150":[1,1,1],"151":[1,1,1],"152":[1,1,1],"153":[1,1,1],"154":[1,1,1],"155":[1,1,1],"156":[3,1,14],"157":[2,3,27],"158":[2,3,21],"159":[1,3,11]},"averageFieldLength":[2.8687499999999995,5.631250000000002,41.35000000000001],"storedFields":{"0":{"title":"Runtime API Examples","titles":[]},"1":{"title":"Results","titles":["Runtime API Examples"]},"2":{"title":"Theme Data","titles":["Runtime API Examples","Results"]},"3":{"title":"Page Data","titles":["Runtime API Examples","Results"]},"4":{"title":"Page Frontmatter","titles":["Runtime API Examples","Results"]},"5":{"title":"More","titles":["Runtime API Examples"]},"6":{"title":"Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇","titles":[]},"7":{"title":"摘要","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"8":{"title":"翻译","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"9":{"title":"研究背景","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"10":{"title":"研究现状","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"11":{"title":"提出的模型","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"12":{"title":"实验（Compared with SOTA）","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"13":{"title":"实验（Ablation Study）","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"14":{"title":"结论","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"15":{"title":"A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","titles":[]},"16":{"title":"南京信息工程大学、青海师范大学、澳门大学、中国科学院  💯","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"17":{"title":"摘要：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"18":{"title":"翻译：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"19":{"title":"研究背景：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"20":{"title":"研究现状：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"21":{"title":"提出的模型：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"22":{"title":"实验（compared with the state-of-the-art models and ablation experiments）","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"23":{"title":"Comparison with the State-of-the-Arts","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","实验（compared with the state-of-the-art models and ablation experiments）"]},"24":{"title":"ablation experiments","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","实验（compared with the state-of-the-art models and ablation experiments）"]},"25":{"title":"结论：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"26":{"title":"Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）","titles":[]},"27":{"title":"中国科学技术大学，上海人工智能实验室","titles":["Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）"]},"28":{"title":"High Quality Segmentation for Ultra High-resolution Images","titles":[]},"29":{"title":"香港中文大学  Adobe 等","titles":[]},"30":{"title":"Learning Generalized Medical Image Segmentation from Decoupled Feature Queries","titles":[]},"31":{"title":"摘要","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"32":{"title":"翻译","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"33":{"title":"研究背景","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"34":{"title":"研究现状","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"35":{"title":"提出的模型","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"36":{"title":"实验（Compared with SOTA）","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"37":{"title":"实验（Ablation Experiments）🥇","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"38":{"title":"结论","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"39":{"title":"Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement","titles":[]},"40":{"title":"Prompt-and-Transfer：Dynamic Class-Aware Enhancement for Few-Shot Segmentation","titles":[]},"41":{"title":"中科院","titles":["Prompt-and-Transfer：Dynamic Class-Aware Enhancement for Few-Shot Segmentation"]},"42":{"title":"Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation","titles":[]},"43":{"title":"摘要","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"44":{"title":"翻译","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"45":{"title":"研究背景","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"46":{"title":"研究现状","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"47":{"title":"提出的模型","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"48":{"title":"实验（Compared with SOTA）","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"49":{"title":"实验（Alabtion Experiments）","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"50":{"title":"结论","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"51":{"title":"Prompting Multi-Modal Image Segmentation with Semantic Grouping","titles":[]},"52":{"title":"University of Chinese Academy of Sciences","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"53":{"title":"摘要：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"54":{"title":"翻译：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"55":{"title":"研究背景：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"56":{"title":"研究现状：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"57":{"title":"提出的模型：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"58":{"title":"实验过程（与SOTA方法的对比）：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"59":{"title":"实验过程（消融实验）：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"60":{"title":"Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation","titles":[]},"61":{"title":"摘要","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"62":{"title":"翻译","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"63":{"title":"研究背景","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"64":{"title":"研究现状","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"65":{"title":"提出的模型","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"66":{"title":"实验（Compared with SOTA）","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"67":{"title":"实验（Ablation Experiments）","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"68":{"title":"结论","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"69":{"title":"SED:A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation","titles":[]},"70":{"title":"天津大学，重庆大学等","titles":["SED:A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation"]},"71":{"title":"Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings","titles":[]},"72":{"title":"摘要","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"73":{"title":"翻译","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"74":{"title":"研究背景","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"75":{"title":"研究现状","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"76":{"title":"提出的模型","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"77":{"title":"实验（Compared with SOTA）","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"78":{"title":"实验（Ablation Experiments）🥇","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"79":{"title":"结论","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"80":{"title":"Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation","titles":[]},"81":{"title":"摘要","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"82":{"title":"翻译","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"83":{"title":"研究背景","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"84":{"title":"研究现状","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"85":{"title":"提出的模型","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"86":{"title":"实验过程（Compared with SOTA）","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"87":{"title":"实验过程（Ablation Experiments）","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"88":{"title":"结论","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"89":{"title":"Segment Anything","titles":[]},"90":{"title":"Meta AI","titles":["Segment Anything"]},"91":{"title":"Self-supervised vision transformers for semantic segmentation","titles":[]},"92":{"title":"摘要","titles":["Self-supervised vision transformers for semantic segmentation"]},"93":{"title":"翻译","titles":["Self-supervised vision transformers for semantic segmentation"]},"94":{"title":"研究背景","titles":["Self-supervised vision transformers for semantic segmentation"]},"95":{"title":"研究现状","titles":["Self-supervised vision transformers for semantic segmentation"]},"96":{"title":"提出的模型","titles":["Self-supervised vision transformers for semantic segmentation"]},"97":{"title":"实验（Compared with SOTA）🥇","titles":["Self-supervised vision transformers for semantic segmentation"]},"98":{"title":"实验（Ablation Experiments）🥇","titles":["Self-supervised vision transformers for semantic segmentation"]},"99":{"title":"结论","titles":["Self-supervised vision transformers for semantic segmentation"]},"100":{"title":"vscode配置Latex环境","titles":[]},"101":{"title":"1 TeX Live 下载与安装","titles":["vscode配置Latex环境"]},"102":{"title":"2 vscode下载与安装","titles":["vscode配置Latex环境"]},"103":{"title":"3 中文语言环境配置","titles":["vscode配置Latex环境"]},"104":{"title":"4 LaTeX的支持插件 LaTeX Workshop安装","titles":["vscode配置Latex环境"]},"105":{"title":"5 打开LaTeX环境设置页面","titles":["vscode配置Latex环境"]},"106":{"title":"6 LaTeX环境的代码配置","titles":["vscode配置Latex环境"]},"107":{"title":"6.1 LaTeX配置代码展示","titles":["vscode配置Latex环境","6 LaTeX环境的代码配置"]},"108":{"title":"6.2 LaTeX配置代码解读","titles":["vscode配置Latex环境","6 LaTeX环境的代码配置"]},"109":{"title":"7 tex文件编译","titles":["vscode配置Latex环境"]},"110":{"title":"7.1 tex测试文件下载","titles":["vscode配置Latex环境","7 tex文件编译"]},"111":{"title":"7.2 tex 测试文件编译","titles":["vscode配置Latex环境","7 tex文件编译"]},"112":{"title":"8 SumatraPDF 安装设置（可选）","titles":["vscode配置Latex环境"]},"113":{"title":"8.1 SumatraPDF下载与安装","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）"]},"114":{"title":"8.2 使用SumatraPDF查看的代码配置","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）"]},"115":{"title":"8.2.1 代码展示","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）","8.2 使用SumatraPDF查看的代码配置"]},"116":{"title":"8.2.2 代码解读","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）","8.2 使用SumatraPDF查看的代码配置"]},"117":{"title":"9 SumatraPDF 的使用","titles":["vscode配置Latex环境"]},"118":{"title":"10 pdf 内部查看与外部查看的切换","titles":["vscode配置Latex环境"]},"119":{"title":"11 个人完整配置","titles":["vscode配置Latex环境"]},"120":{"title":"论文阅读笔记","titles":[]},"121":{"title":"Pytorch笔记","titles":[]},"122":{"title":"深度学习笔记","titles":[]},"123":{"title":"语义分割概述","titles":[]},"124":{"title":"1.什么是语义分割","titles":["语义分割概述"]},"125":{"title":"2.模型的输入和输出","titles":["语义分割概述"]},"126":{"title":"3.常见的分割模型","titles":["语义分割概述"]},"127":{"title":"4.语义分割的思路","titles":["语义分割概述"]},"128":{"title":"5.评价指标","titles":["语义分割概述"]},"129":{"title":"Pytorch教程","titles":[]},"130":{"title":"上采样","titles":[]},"131":{"title":"FCN模型讲解","titles":[]},"132":{"title":"图像分割学习笔记","titles":[]},"133":{"title":"1. 基本概念","titles":[]},"134":{"title":"1.1 什么是图像分割","titles":["1. 基本概念"]},"135":{"title":"1.2 图像分割的应用场景","titles":["1. 基本概念"]},"136":{"title":"1.3 图像分割的前景和背景","titles":["1. 基本概念"]},"137":{"title":"1.4 图像分割的三个层次","titles":["1. 基本概念"]},"138":{"title":"2.经典数据集","titles":[]},"139":{"title":"2.1 PASCAL数据集","titles":["2.经典数据集"]},"140":{"title":"2.1Cityscape(用于自动驾驶场景)","titles":["2.经典数据集"]},"141":{"title":"2.3 COCO数据集","titles":["2.经典数据集"]},"142":{"title":"3. 评估指标和优化目标","titles":[]},"143":{"title":"3.1 语义分割评估指标","titles":["3. 评估指标和优化目标"]},"144":{"title":"3.2 语义分割常用优化目标","titles":["3. 评估指标和优化目标"]},"145":{"title":"4. 上采样","titles":[]},"146":{"title":"4.1 图像分割网络的两个模块","titles":["4. 上采样"]},"147":{"title":"4.2 上采样实现方法--插值法","titles":["4. 上采样"]},"148":{"title":"4.3 典型的图像分割网络","titles":["4. 上采样"]},"149":{"title":"FCN","titles":[]},"150":{"title":"FCN基本原理","titles":["FCN"]},"151":{"title":"FCN细节","titles":["FCN"]},"152":{"title":"FCN结果","titles":["FCN"]},"153":{"title":"SegNet","titles":[]},"154":{"title":"SegNet的基本原理","titles":["SegNet"]},"155":{"title":"UNet","titles":[]},"156":{"title":"Markdown Extension Examples","titles":[]},"157":{"title":"Syntax Highlighting","titles":["Markdown Extension Examples"]},"158":{"title":"Custom Containers","titles":["Markdown Extension Examples"]},"159":{"title":"More","titles":["Markdown Extension Examples"]}},"dirtCount":0,"index":[["```",{"2":{"157":1}}],["典型的图像分割网络",{"0":{"148":1}}],["典型例证如",{"2":{"93":1}}],["插值法",{"0":{"147":1}}],["插值等操作",{"2":{"130":1}}],["什么是图像分割",{"0":{"134":1}}],["什么是语义分割",{"0":{"124":1}}],["第五个卷积块",{"2":{"131":1}}],["第四个卷积块",{"2":{"131":1}}],["第三个卷积块",{"2":{"131":1}}],["第二个卷积块",{"2":{"131":1}}],["第一个卷积块",{"2":{"131":1}}],["第一个",{"2":{"108":1}}],["填充",{"2":{"131":1}}],["步长",{"2":{"131":1}}],["舍弃最后的全连接层",{"2":{"131":1}}],["修改而来",{"2":{"131":1}}],["修改如下图",{"2":{"116":1}}],["−2∗padding",{"2":{"130":2}}],["∗stride",{"2":{"130":2}}],["补",{"2":{"130":1}}],["情况下的反卷积则体现为",{"2":{"130":1}}],["卷积核元素之间的间距",{"2":{"130":1}}],["卷积核的大小",{"2":{"130":1}}],["卷积核大小",{"2":{"130":1,"131":1}}],["卷积产生的通道数",{"2":{"130":1}}],["卷积步长",{"2":{"130":2}}],["卷积变压器架构",{"2":{"24":1}}],["小特征图",{"2":{"130":1}}],["反卷积是一种特殊的正向卷积",{"2":{"130":1}}],["反池化",{"2":{"130":1}}],["反向同步测试",{"2":{"111":1}}],["映射回一个较大的",{"2":{"130":1}}],["概念一致",{"2":{"128":1}}],["像素分类准确率",{"2":{"128":1}}],["像素分类精度",{"2":{"128":1}}],["像素级",{"2":{"124":1}}],["像素级标注耗时耗力",{"2":{"84":1}}],["评价指标",{"0":{"128":1}}],["评估指标和优化目标",{"0":{"142":1},"1":{"143":1,"144":1}}],["评估设置",{"2":{"29":1}}],["评估并利用视觉基础模型",{"2":{"27":1}}],["评估并应用了多种视觉基础模型",{"2":{"27":1}}],["遍历一些小区域",{"2":{"127":1}}],["滑动窗口的思路可以概括如下",{"2":{"127":1}}],["事实上",{"2":{"125":1}}],["回顾一下之前的全连接网络的分类模型",{"2":{"125":1}}],["那么实际上模型的输出为w×d×n",{"2":{"125":1}}],["那么输出是什么呢",{"2":{"125":1}}],["那么可以点击上图左下角的",{"2":{"101":1}}],["很显然",{"2":{"125":1}}],["很多博主也只是贴上了配置代码",{"2":{"100":1}}],["又要区分出同个类别中的不同实例",{"2":{"124":1}}],["简单的说",{"2":{"124":1}}],["简化训练过程并取得不错效果",{"2":{"46":1}}],["检测",{"2":{"124":1}}],["检查安装是否正常",{"2":{"101":1}}],["图片级",{"2":{"124":1}}],["图像修复等",{"2":{"95":1}}],["图像分割网络的两个模块",{"0":{"146":1}}],["图像分割的三个层次",{"0":{"137":1}}],["图像分割的前景和背景",{"0":{"136":1}}],["图像分割的应用场景",{"0":{"135":1}}],["图像分割学习笔记",{"0":{"132":1}}],["图像分割领域存在多种任务",{"2":{"90":1}}],["图像分割任务",{"2":{"90":1}}],["图像分辨率为2048x1024",{"2":{"39":1}}],["图像分辨率范围2k",{"2":{"29":1}}],["图像分辨率越来越高",{"2":{"29":1}}],["图像语义分割任务训练通常需大量高质量标注样本",{"2":{"84":1}}],["图像编码器学习率多乘以一个",{"2":{"70":1}}],["图像级标签最为经济",{"2":{"45":1}}],["图像裁剪和级联模型",{"2":{"29":1}}],["阅读论文",{"2":{"120":1}}],["阅读功能的同时很轻量",{"2":{"112":1}}],["论文阅读笔记",{"0":{"120":1}}],["论文提出的方法显著优于现有最优方法",{"2":{"36":1}}],["非常感谢",{"2":{"119":1}}],["给笔者一点小小的激励",{"2":{"119":1}}],["希望您能够不吝点赞",{"2":{"119":1}}],["希望能够对大家有所帮助",{"2":{"100":1}}],["另",{"2":{"119":1}}],["另一方面",{"2":{"55":1}}],["另一方面下游任务标注数据往往匮乏",{"2":{"54":1}}],["争取以后学得更扎实再编写这些文字",{"2":{"119":1}}],["欢迎您在评论区批评指正",{"2":{"119":1}}],["预测目标的轮廓",{"2":{"134":1}}],["预览编译好的pdf文件",{"2":{"119":1}}],["预训练解码器",{"2":{"98":1}}],["预训练模型进行全局微调",{"2":{"54":1}}],["预训练骨干网络有固有偏差",{"2":{"19":1}}],["或双击",{"2":{"119":1}}],["或",{"2":{"118":1}}],["或者复制以下代码进行文档的简单编译测试",{"2":{"110":1}}],["或者在前一页面",{"2":{"101":1}}],["内部查看与外部查看的切换",{"0":{"118":1}}],["内置",{"2":{"116":1}}],["变为了在",{"2":{"117":1}}],["变量有两种",{"2":{"108":1}}],["路径修改",{"2":{"116":1}}],["是一张张的图片",{"2":{"125":1}}],["是语义分割和实例分割的结合",{"2":{"124":1}}],["是语义分割或全像素语义分割的子类型",{"2":{"124":1}}],["是触发synctex的扩展名为",{"2":{"119":1}}],["是当触发synctex被触发时",{"2":{"116":1}}],["是生成pdf文件的绝对路径的占位符",{"2":{"116":1,"119":1}}],["是行号",{"2":{"116":1,"119":1}}],["是用于生成pdf文件的绝对路径的占位符",{"2":{"116":1,"119":1}}],["转置卷积",{"2":{"130":2}}],["转发到外部查看器时要执行的命令",{"2":{"116":1}}],["转到",{"2":{"105":1}}],["请注意中间为",{"2":{"116":1}}],["请记得在最后一句",{"2":{"107":1}}],["链接",{"2":{"116":1,"119":1}}],["命令上的",{"2":{"116":1}}],["让您进行查看",{"2":{"115":1}}],["达到与内置",{"2":{"112":1}}],["达到了当前最优性能",{"2":{"14":1}}],["要更加让人舒服一些",{"2":{"112":1}}],["要注意的是",{"2":{"101":1}}],["默认",{"2":{"119":1}}],["默认会放大一些",{"2":{"112":1}}],["默认使用两个块的解码器",{"2":{"98":1}}],["外部查看器了",{"2":{"117":1}}],["外部查看器展示出来的",{"2":{"112":1}}],["外部查看器的优势是能够看到",{"2":{"112":1}}],["外观和拍摄角度差异较大的情况下",{"2":{"66":1}}],["跳转到对应代码",{"2":{"111":1}}],["跳跃连接",{"2":{"98":1}}],["鼠标左键双击或ctrl+鼠标左键单击",{"2":{"111":1}}],["按ctrl+alt+v",{"2":{"117":1}}],["按ctrl+alt+j",{"2":{"111":1}}],["按win",{"2":{"101":1}}],["正向同步和反向同步",{"2":{"117":1}}],["正向同步测试",{"2":{"111":1}}],["正常情况下只需更改磁盘盘符即可",{"2":{"116":1}}],["正则化方法常忽略利用高层语义信息",{"2":{"83":1}}],["快捷键",{"2":{"111":1}}],["左侧工具栏",{"2":{"111":1}}],["左边为设置false情况",{"2":{"108":1}}],["符号",{"2":{"111":1}}],["符号时",{"2":{"111":1}}],["符合word设定",{"2":{"110":1}}],["√",{"2":{"111":1}}],["选中想要跳转行",{"2":{"111":1}}],["选中需要跳转的代码所在行",{"2":{"111":1}}],["选中",{"2":{"111":2}}],["选中tex文件的代码页面",{"2":{"111":1}}],["选择第一个latex",{"2":{"104":1}}],["选择第一个chinese",{"2":{"103":1}}],["选择mit",{"2":{"86":1}}],["选择mgmatting作为掩码引导抠图方法",{"2":{"29":1}}],["选择3次",{"2":{"41":1}}],["选择panopticfcn和entity",{"2":{"29":1}}],["选择cascadepsp作为超高清图像的主要对比方法",{"2":{"29":1}}],["选择clip",{"2":{"27":1}}],["选择方法",{"2":{"27":1}}],["世界",{"2":{"110":1}}],["你好",{"2":{"110":1}}],["\\t",{"2":{"110":2}}],["功能是否比较完整",{"2":{"110":1}}],["清除辅助文件",{"2":{"108":1}}],["速度快",{"2":{"108":1}}],["速度提升",{"2":{"70":1}}],["允许用户使用操作系统字体来代替",{"2":{"108":1}}],["区域为不规则的物体边界",{"2":{"128":1}}],["区域级",{"2":{"124":1}}],["区域级自监督预训练方法",{"2":{"95":1}}],["区别如下",{"2":{"108":1}}],["则为鼠标左键双击",{"2":{"111":1}}],["则无法进行编译",{"2":{"111":1}}],["则会导致编译不出完整结果甚至编译失败",{"2":{"108":1}}],["则该拓展能够从使用的宏包中自动提取命令和环境",{"2":{"108":1}}],["则安装正常",{"2":{"101":1}}],["菜单中多了两个选项",{"2":{"108":1}}],["右侧就会跳转到相应行",{"2":{"111":1}}],["右边为设置true情况",{"2":{"108":1}}],["右键菜单",{"2":{"119":1}}],["右键以管理员身份运行",{"2":{"101":1}}],["右键",{"2":{"101":1}}],["新的",{"2":{"108":1}}],["时操作步骤相同",{"2":{"117":1}}],["时构建项目",{"2":{"108":1}}],["时方法性能较好",{"2":{"87":1}}],["编译工具和命令",{"2":{"119":1}}],["编译出错时设置是否弹出气泡设置",{"2":{"119":1}}],["编译成功",{"2":{"111":1}}],["编译bixtex文件",{"2":{"110":1}}],["编译",{"2":{"108":2}}],["编译模式与",{"2":{"108":1}}],["编译器中能够看到的编译顺序",{"2":{"108":1}}],["编译链的存在是为了更方便编译",{"2":{"108":1}}],["编译链中被使用的编译命令",{"2":{"108":1}}],["编译链自动构建",{"2":{"108":1}}],["编码器",{"2":{"47":1,"64":1}}],["切记",{"2":{"107":1}}],["否则就会报错",{"2":{"107":1}}],["否则后期手动添加比较麻烦",{"2":{"101":1}}],["除了代码块儿最后一句",{"2":{"107":1}}],[">",{"2":{"107":4,"108":4,"119":4}}],["查看效果图",{"2":{"117":1}}],["查看",{"2":{"116":1}}],["查看器查看",{"2":{"116":1}}],["查看器",{"2":{"116":1}}],["查看器设置",{"2":{"107":1}}],["查看具有相同的效果",{"2":{"112":1}}],["查询框架基础上增加了一个无标签分支",{"2":{"65":1}}],["查询由深层特征生成",{"2":{"35":1}}],["查询对的密集对应关系",{"2":{"10":1}}],["配置",{"2":{"119":1}}],["配置代码如下",{"2":{"107":1}}],["配备pat提出的动态类别感知编码器后",{"2":{"41":1}}],["处打开",{"2":{"105":1}}],["处理注意力偏差",{"2":{"25":1}}],["页面定位到代码相应位置",{"2":{"111":1}}],["页面相应位置",{"2":{"111":1}}],["页面",{"2":{"111":1}}],["页面右下角跳出如下弹窗",{"2":{"104":1}}],["页面和笔者所用图片中展示的页面有略微不同",{"2":{"100":1}}],["显示如下",{"2":{"103":1}}],["显著超过现有方法",{"2":{"87":1}}],["显著超越现有监督方法",{"2":{"73":1}}],["显著提升了前景实例的语义一致性",{"2":{"62":1}}],["显著提升泛化性",{"2":{"27":1}}],["显著优于其他单阶段方法",{"2":{"48":1}}],["完成中文环境配置",{"2":{"103":1}}],["完成分割任务",{"2":{"70":1}}],["添加偏置",{"2":{"130":1}}],["添加到path",{"2":{"102":1}}],["添加两个跳跃连接时性能最佳",{"2":{"98":1}}],["记得更改安装路径并记住",{"2":{"113":1}}],["记得修改安装路径",{"2":{"102":1}}],["记为lseg",{"2":{"35":1}}],["官网下载",{"2":{"102":1,"113":1}}],["若使用笔者的代码",{"2":{"111":1}}],["若未选中",{"2":{"111":1}}],["若因网络原因无法连接到github导致无法下载",{"2":{"110":1}}],["若无特殊需求",{"2":{"108":1}}],["若不想了解",{"2":{"104":1}}],["若您感觉此文写得勉强还行",{"2":{"119":1}}],["若您想要更改",{"2":{"111":1}}],["若您想要了解新版本增加的功能",{"2":{"104":1}}],["若您不想要配置外部查看器以及了解内部查看和外部查看之间切换操作",{"2":{"107":1}}],["若您的",{"2":{"100":1}}],["若在安装完该插件之后在",{"2":{"104":1}}],["若输出了一些版本信息",{"2":{"101":1}}],["⑩",{"2":{"101":1}}],["几分钟左右",{"2":{"101":1}}],["⑨",{"2":{"101":1}}],["根据",{"2":{"107":1}}],["根据个人想法可以选择是否在开始菜单文件夹创建",{"2":{"102":1}}],["根据您的需要进行相应的更改",{"2":{"101":1}}],["根据显式分组的语义相似性",{"2":{"57":1}}],["个人完整配置",{"0":{"119":1}}],["个性化安装",{"2":{"101":1}}],["个主流测试集上达到了监督学习的顶尖水平",{"2":{"73":1}}],["⑧",{"2":{"101":1}}],["安装包不到",{"2":{"112":1}}],["安装设置",{"0":{"112":1},"1":{"113":1,"114":1,"115":1,"116":1}}],["安装好之后",{"2":{"102":1}}],["安装",{"2":{"101":1}}],["故而选择xelatex",{"2":{"111":1}}],["故而您可以根据自己的需要更改编译链顺序",{"2":{"108":1}}],["故而笔者使用了onfailed",{"2":{"108":1}}],["故而笔者设置均设置为false",{"2":{"108":1}}],["故而取消",{"2":{"101":1}}],["故此项笔者设置为true",{"2":{"108":1}}],["故笔者写下了此文",{"2":{"100":1}}],["出现如下图页面",{"2":{"117":1}}],["出现编译好的",{"2":{"111":1}}],["出现下图后",{"2":{"101":1}}],["出现基于两阶段和单阶段框架的方法",{"2":{"70":1}}],["⑦",{"2":{"101":1}}],["⑥",{"2":{"101":1,"111":1}}],["⑤",{"2":{"101":1,"103":1,"111":1,"117":1}}],["打开编译出的",{"2":{"117":1}}],["打开测试文件所在文件夹",{"2":{"111":1}}],["打开latex环境设置页面",{"0":{"105":1}}],["打开拓展",{"2":{"103":1,"104":1}}],["打开命令行窗口",{"2":{"101":1}}],["打开运行",{"2":{"101":1}}],["打开",{"2":{"101":1,"102":1}}],["资源管理器",{"2":{"101":1}}],["找到",{"2":{"101":1}}],["找到下载好的压缩包",{"2":{"101":1}}],["找了很多资料",{"2":{"100":1}}],["④",{"2":{"101":1,"103":1,"104":1,"105":1,"111":1,"117":1}}],["直到下载速度在您的可接受范围内即可",{"2":{"101":1}}],["直接上图",{"2":{"100":1}}],["③",{"2":{"101":1,"102":1,"103":1,"104":1,"105":1,"111":1,"117":1}}],["②",{"2":{"101":1,"102":1,"103":1,"104":1,"105":1,"111":1,"117":1}}],["①",{"2":{"101":1,"102":1,"103":1,"104":1,"105":1,"111":1,"117":1}}],["接口与参数说明",{"2":{"130":1}}],["接下来是",{"2":{"101":1}}],["接着就会出现下图",{"2":{"101":1}}],["接着",{"2":{"18":1}}],["系统是",{"2":{"101":1}}],["系列等方法不断发展",{"2":{"29":1}}],["文中如果出现错误的地方",{"2":{"119":1}}],["文档时的默认编译链",{"2":{"108":1}}],["文档编译有时需要用到辅助文件",{"2":{"108":1}}],["文件清理",{"2":{"119":1}}],["文件同步到外部查看器时latex",{"2":{"116":1}}],["文件路径",{"2":{"116":2}}],["文件内后",{"2":{"117":1}}],["文件内",{"2":{"115":1}}],["文件在查看器中的目录",{"2":{"112":1}}],["文件的完整展现效果",{"2":{"112":1}}],["文件可以从",{"2":{"110":1}}],["文件指定位置跳转到",{"2":{"108":1}}],["文件没有正常更新的情况",{"2":{"108":1}}],["文件修改后进行编译时",{"2":{"108":1}}],["文件也会有些字体没有嵌入",{"2":{"108":1}}],["文件默认嵌入所有字体",{"2":{"108":1}}],["文件仍需要根文件完整路径",{"2":{"108":1}}],["文件相应位置",{"2":{"108":1}}],["文件编写规则",{"2":{"107":1}}],["文件中任意的代码",{"2":{"111":2}}],["文件中相应代码所在位置",{"2":{"108":1}}],["文件中",{"2":{"107":1}}],["文件",{"2":{"101":1,"105":1,"108":1,"110":1,"111":2,"117":2}}],["文末有完整的个人配置代码",{"2":{"100":1}}],["文本到掩码任务的探索还不够成熟",{"2":{"90":1}}],["文本到掩码任务待完善",{"2":{"90":1}}],["文本代价图",{"2":{"70":1}}],["文本编码器冻结",{"2":{"70":1}}],["文本数据集中学习",{"2":{"70":1}}],["文本对数据中学习视觉特征",{"2":{"70":1}}],["文本成本图",{"2":{"70":1}}],["您注意更改路径",{"2":{"116":1}}],["您可根据个人适应选择相应的方法",{"2":{"118":1}}],["您可自行选择是否需要设置此部分内容",{"2":{"112":1}}],["您可以通过https",{"2":{"27":1}}],["您无需担心",{"2":{"100":1}}],["注意修改路径",{"2":{"115":3,"116":3,"119":3}}],["注意到",{"2":{"111":1}}],["注意力模块插入和上下文先验等",{"2":{"64":1}}],["注意力机制在目标类别内差异大时",{"2":{"19":1}}],["注意力偏差和空间感知偏差导致的背景干扰问题",{"2":{"25":1}}],["注意力偏差和空间感知偏差导致的对背景噪声敏感的问题",{"2":{"21":1}}],["注意力偏差和空间感知偏差问题",{"2":{"24":1}}],["注意力偏差和空间感知偏差",{"2":{"18":1}}],["注",{"2":{"100":3,"105":2,"107":1,"108":2,"110":1,"111":1,"116":1,"119":1}}],["没有详细的介绍说明",{"2":{"100":1}}],["没有充分利用上下文信息",{"2":{"70":1}}],["笔者会虚心接受这些产生错误的地方",{"2":{"119":1}}],["笔者也只是一个初学者",{"2":{"119":1}}],["笔者将快捷键设置为ctrl+alt+r",{"2":{"111":1}}],["笔者编写了一份简单的",{"2":{"110":1}}],["笔者选择",{"2":{"112":1}}],["笔者选择使用lastused",{"2":{"108":1}}],["笔者选用的",{"2":{"101":1}}],["笔者此处设置为",{"2":{"108":1}}],["笔者觉得菜单多了此选项较方便",{"2":{"108":1}}],["笔者只对几个要点进行提及",{"2":{"102":1}}],["笔者进入了清华大学镜像网站",{"2":{"101":1}}],["笔者配置了好久",{"2":{"100":1}}],["笔者前期使用的是texstudio进行文档的编译的",{"2":{"100":1}}],["都不清除辅助文件",{"2":{"108":1}}],["都不一样",{"2":{"100":1}}],["都选择清除辅助文件",{"2":{"108":1}}],["都需要加上英文状态下的",{"2":{"107":1}}],["都对分割结果有积极贡献",{"2":{"37":1}}],["颜值也很高",{"2":{"100":1}}],["话不多说",{"2":{"100":1}}],["头秃",{"2":{"100":1}}],["至少对笔者而言是如此",{"2":{"100":1}}],["秃头专业",{"2":{"100":1}}],["成对相对位置偏差的效果优于其他位置偏差",{"2":{"98":1}}],["位置编码",{"2":{"98":1}}],["端到端微调结果",{"2":{"97":1}}],["比如区分一个像素是猫还是狗",{"2":{"124":1}}],["比如区分一张图片是猫还是狗",{"2":{"124":1}}],["比如检测一个区域是猫还是狗",{"2":{"124":1}}],["比如编译目录和编译参考文献时",{"2":{"108":1}}],["比较便捷",{"2":{"105":1}}],["比较麻烦",{"2":{"105":1,"108":1}}],["比较了基于边缘和基于cam的两种严格选择不确定特征的方法",{"2":{"49":1}}],["比仅依赖窗口注意力的swin",{"2":{"98":1}}],["比采用swin",{"2":{"97":1}}],["适用于具有挑战性的语义分割任务",{"2":{"96":1}}],["适用于低分辨率训练和超高分辨率测试",{"2":{"29":1}}],["下面给出具体的计算公式",{"2":{"128":1}}],["下面进行代码注释解读",{"2":{"108":1}}],["下文配置需要使用其路径",{"2":{"113":1}}],["下文会进行提及",{"2":{"108":1}}],["下图展示两者区别",{"2":{"108":1}}],["下载步骤如图",{"2":{"110":1}}],["下载",{"2":{"101":1,"110":1}}],["下载页面",{"2":{"101":1}}],["下载与安装",{"0":{"101":1}}],["下均优于大多数现有方法",{"2":{"96":1}}],["下一个创新点",{"2":{"88":1}}],["低样本微调结果",{"2":{"97":1}}],["低样本学习",{"2":{"96":1}}],["低分辨率训练和超高分辨率测试",{"2":{"29":1}}],["低分辨率训练和超高清测试",{"2":{"29":1}}],["微调",{"2":{"96":1}}],["微调方法",{"2":{"27":2}}],["线性探测miou提高了4",{"2":{"98":1}}],["线性探测miou从67",{"2":{"98":1}}],["线性探测结果",{"2":{"97":1}}],["线性探测",{"2":{"96":1}}],["教师网络通过指数移动平均",{"2":{"96":1}}],["教导模型避免学习有标签输入的特定样本偏差",{"2":{"65":1}}],["混合注意力机制",{"2":{"96":1}}],["促进训练过程中浅层的梯度传播",{"2":{"96":1}}],["促进跨模态语义对齐",{"2":{"54":1}}],["组成",{"2":{"96":1}}],["组件分析",{"2":{"24":1,"41":1}}],["密集预测预训练",{"2":{"95":1}}],["密集标注的",{"2":{"70":1}}],["降低对大规模标注数据的需求",{"2":{"94":1}}],["及其在视觉transformer上的应用在计算机视觉领域取得显著进展",{"2":{"94":1}}],["机器人操作等领域应用广泛",{"2":{"94":1}}],["必须通过密集的自监督信号同时实现上述两个目标",{"2":{"93":1}}],["已被广泛应用于各类下游任务",{"2":{"93":1}}],["需要清除辅助文件了",{"2":{"108":1}}],["需要进行路径的更改",{"2":{"101":1}}],["需要结合全局上下文信息",{"2":{"93":1}}],["需要像素级的细粒度理解",{"2":{"93":1,"94":1}}],["需要更多的努力来改进",{"2":{"90":1}}],["需要额外的计算资源",{"2":{"29":1}}],["专门的交互式分割方法通常会优于sam",{"2":{"90":1}}],["放大",{"2":{"90":1}}],["含超10亿掩码",{"2":{"90":1}}],["支持灵活提示",{"2":{"90":1}}],["支持和查询语义的重要性",{"2":{"41":1}}],["边缘检测",{"2":{"90":1}}],["边界框和图像级标签等",{"2":{"84":1}}],["边界框和图像级标签提供更多关键语义信息",{"2":{"83":1}}],["尚无网络规模的数据源",{"2":{"90":1}}],["许多问题缺乏充足的训练数据",{"2":{"90":1}}],["甚至被其他应用程序修改",{"2":{"108":1}}],["甚至在某些情况下表现更好",{"2":{"90":1}}],["甚至超过了一些复杂的多阶段方法",{"2":{"48":1,"50":1}}],["项目",{"2":{"90":1,"108":1}}],["项目代码已开源",{"2":{"32":1}}],["发现它的零样本表现非常优秀",{"2":{"90":1}}],["发现基于transformer的骨干网络在效率和性能上限方面表现更优",{"2":{"87":1}}],["发现所提出的dfq方案在消除非对角元素方面表现最佳",{"2":{"36":1}}],["骨干网络影响",{"2":{"87":1}}],["骨干网络设置消融实验",{"2":{"41":1}}],["尽管mit",{"2":{"86":1}}],["尽管完全没有使用这些测试集的训练图像",{"2":{"73":1}}],["局部原型动态更新全局原型",{"2":{"85":1}}],["局部查询原型生成",{"2":{"65":1}}],["初始预测",{"2":{"85":1}}],["初始时",{"2":{"67":1}}],["点进去之后就可以进行下载了",{"2":{"102":1}}],["点击编辑页面任意位置来选中",{"2":{"117":1}}],["点击选中",{"2":{"111":1}}],["点击下图",{"2":{"105":1}}],["点击设置",{"2":{"105":1}}],["点击设置图标",{"2":{"105":1}}],["点击页面右下角跳出窗口中的",{"2":{"103":1}}],["点击拓展图标",{"2":{"103":1,"104":1}}],["点击关闭即可",{"2":{"101":1}}],["点击",{"2":{"101":1,"103":1,"104":2}}],["点击红框圈画链接进行",{"2":{"101":1}}],["点击图示红框圈画位置进入随机的镜像网站",{"2":{"101":1}}],["点",{"2":{"84":1}}],["点和图像级标签等",{"2":{"45":1}}],["辅助任务会引入额外数据和预测误差",{"2":{"83":1}}],["辅助任务和标签扩散等",{"2":{"83":1,"84":1}}],["辅助建模模态公共统计信息",{"2":{"57":1}}],["伪标签方法耗时",{"2":{"83":1}}],["伪标签细化和分割联合训练",{"2":{"46":1}}],["伪建议",{"2":{"83":1,"84":1}}],["标准字体进行替换",{"2":{"108":1}}],["标记",{"2":{"111":1}}],["标记能够编译文档",{"2":{"108":1}}],["标记使用",{"2":{"108":1}}],["标注方式",{"2":{"84":1}}],["标注成本问题",{"2":{"83":1}}],["标签扩散主要依赖局部信息",{"2":{"83":1}}],["标签",{"2":{"45":1}}],["万张图像的精细标注数据与公开的带噪声弱标注数据",{"2":{"73":1}}],["替代传统类别标签",{"2":{"73":1}}],["替代标准多层感知器",{"2":{"21":1}}],["开始编译文件",{"2":{"111":1}}],["开发跨领域通用的鲁棒语义分割模型成为研究热点",{"2":{"73":1}}],["开放词汇语义分割",{"2":{"70":1}}],["开放词汇语义分割旨在将像素划分到一个开放类别集中的不同语义组",{"2":{"70":1}}],["y",{"2":{"131":3}}],["yvanyin",{"2":{"72":1,"73":1}}],["yield",{"2":{"39":1}}],["效果验证",{"2":{"70":1}}],["效果出乎意料地优于完全参数微调",{"2":{"27":1}}],["缩短了推理时间",{"2":{"70":1}}],["×",{"2":{"70":2,"111":1}}],["次迭代",{"2":{"70":1}}],["共有两种操作方式",{"2":{"118":1}}],["共",{"2":{"70":1}}],["共修正了2554个标签图",{"2":{"39":1}}],["倍的因子",{"2":{"70":1}}],["均为笔者所安装的其余插件以及其余设置所致",{"2":{"100":1}}],["均为",{"2":{"70":1}}],["均超越了当前最先进的语义分割方法",{"2":{"62":1}}],["渐进式融合编码器",{"2":{"70":1}}],["也叫反卷积",{"2":{"130":1}}],["也称为全像素语义分割",{"2":{"124":1}}],["也就是出现在工具栏中的链名称",{"2":{"108":1}}],["也就是上图所完成的功能",{"2":{"101":1}}],["也是众多视觉应用的基础模块",{"2":{"93":1}}],["也会增加计算资源",{"2":{"70":1}}],["也取得了更优的性能",{"2":{"48":1}}],["也取得了令人满意的结果",{"2":{"41":1}}],["运行于单个a6000显卡",{"2":{"70":1}}],["每一块的输出",{"2":{"131":1}}],["每一层在",{"2":{"131":1}}],["每个像素的分类类别均为",{"2":{"125":1}}],["每个代码语句",{"2":{"107":1}}],["每个使用者的",{"2":{"100":1}}],["每个使用者都能够根据自己的需求和想法下载相应的插件",{"2":{"100":1}}],["每个标记与特定实例对应",{"2":{"27":1}}],["每张图像的推理时间为82毫秒",{"2":{"70":1}}],["层对特征图进行上采样",{"2":{"96":1}}],["层次化骨干网络能够更好地捕捉局部空间信息",{"2":{"70":1}}],["层感知器和权重归一化的全连接层组成",{"2":{"47":1}}],["称为sed",{"2":{"70":1}}],["关键是将图像级的模型适应为像素级的分割任务",{"2":{"70":1}}],["关键组件有效性",{"2":{"67":1}}],["重叠区域的特征反复计算",{"2":{"127":1}}],["重启",{"2":{"103":1}}],["重庆大学等",{"0":{"70":1}}],["重建更多细节",{"2":{"29":1}}],["重建图像细节",{"2":{"29":1}}],["天津大学",{"0":{"70":1}}],["综上",{"2":{"68":1}}],["约束前景语义一致性",{"2":{"68":1}}],["干扰查询预测",{"2":{"67":1}}],["准确率反而下降",{"2":{"67":1}}],["数量和质量远超现有数据集",{"2":{"90":1}}],["数量继续增加",{"2":{"67":1}}],["数据创新",{"2":{"90":1}}],["数据增强对性能的影响有限",{"2":{"49":1}}],["数据增强",{"2":{"49":1}}],["数据驱动方法的局限",{"2":{"41":1}}],["数据分布差异",{"2":{"33":1}}],["数据集的创新",{"2":{"90":1}}],["数据集的涂鸦监督语义分割任务中达到了当前最佳水平",{"2":{"82":1}}],["数据集大小和训练计算量的增加而提升",{"2":{"90":1}}],["数据集上的大量实验表明",{"2":{"44":1}}],["数据集贡献",{"2":{"39":1}}],["数据集和对比方法选择",{"2":{"29":1}}],["数据集",{"2":{"12":1,"23":1,"39":1,"48":1,"58":1,"66":1,"70":1,"86":1,"97":1}}],["高宽都增加padding",{"2":{"130":1}}],["高宽都增加2",{"2":{"130":1}}],["高出2",{"2":{"66":1}}],["高斯抑制的作用",{"2":{"41":1}}],["五样本设置下高约2",{"2":{"66":1}}],["五和六个领域的asd指标分别提高了0",{"2":{"36":1}}],["权重β经验性地设置为0",{"2":{"65":1}}],["经典数据集",{"0":{"138":1},"1":{"139":1,"140":1,"141":1}}],["经过对方同意",{"2":{"100":1}}],["经过1×1卷积和激活操作",{"2":{"65":1}}],["经验上掩码比率r",{"2":{"49":1}}],["激活包含目标类对象的像素并停用其他像素",{"2":{"65":1}}],["帮助模型学习保证语义一致性",{"2":{"65":1}}],["帮助缓解空间感知偏差",{"2":{"18":1}}],["少数研究探索了无标签数据的利用",{"2":{"64":1}}],["少样本学习",{"2":{"41":2}}],["少样本分割中无标签数据利用",{"2":{"64":1}}],["少样本分割",{"2":{"19":1,"41":2,"64":1}}],["少样本语义分割范式受到关注",{"2":{"63":1}}],["少样本语义分割和跨域少样本语义分割的几种先进方法进行比较",{"2":{"12":1}}],["少样本语义分割",{"2":{"9":1,"10":1}}],["原本内嵌输出的",{"2":{"117":1}}],["原因在于它们忽略了语义分割的特定属性",{"2":{"94":1}}],["原因是无标签增强效果过强会使特征挖掘注意力转向无标签分支",{"2":{"67":1}}],["原因是随着有标签图像增加",{"2":{"66":1}}],["原型对应",{"2":{"95":1}}],["原型设置",{"2":{"87":1}}],["原型提取与更新",{"2":{"85":1}}],["原型方法",{"2":{"84":1}}],["原型学习方法以较低计算成本取得不错效果",{"2":{"64":1}}],["原始的nightcity是最大的夜间语义分割数据集",{"2":{"39":1}}],["空间相关方法虽保留空间结构",{"2":{"64":1}}],["主干网络对空间信息变得不敏感",{"2":{"70":1}}],["主损失使用dice损失计算查询输入的预测结果与真实标签之间的差异",{"2":{"65":1}}],["主流方法分为原型提取和空间相关两类",{"2":{"64":1}}],["主要是通过输入边缘的",{"2":{"130":1}}],["主要是因为夜间的光照条件复杂且不足",{"2":{"39":1}}],["主要原因在于忽视了该任务的三个核心特性",{"2":{"93":1}}],["主要组件",{"2":{"57":1}}],["主要分为基于对齐和基于聚合的融合方法",{"2":{"56":1}}],["主要有原型匹配",{"2":{"41":1}}],["扩展名为",{"2":{"116":1}}],["扩展实验",{"2":{"29":1}}],["扩大前景和背景的类间差异",{"2":{"63":1}}],["同一类的不同物体也要进行分割",{"2":{"124":1}}],["同一类不同实例存在语义模糊",{"2":{"63":1}}],["同",{"2":{"70":1}}],["同时解决了上述两个问题",{"2":{"108":1}}],["同时引入跳跃连接",{"2":{"96":1}}],["同时使用两种原型增强时性能最佳",{"2":{"87":1}}],["同时在深度估计",{"2":{"73":1}}],["同时两阶段和单阶段的方法都存在不足",{"2":{"70":1}}],["同时保留各模态独有的特征模式",{"2":{"54":1}}],["同时我们提出",{"2":{"44":1}}],["同时也具有重要的临床应用价值",{"2":{"32":1}}],["同时",{"2":{"13":1,"68":1}}],["同时提出整合支持图像的广义逆来优化查询图像的广义逆",{"2":{"11":1}}],["基本概念",{"0":{"133":1},"1":{"134":1,"135":1,"136":1,"137":1}}],["基本更改",{"2":{"101":1}}],["基础模型",{"2":{"90":1}}],["基础模型发展",{"2":{"90":1}}],["基准测试中",{"2":{"62":1}}],["基于掩码图像建模的方法取得了不错的成果",{"2":{"95":1}}],["基于这些问题",{"2":{"94":1}}],["基于这些关键要素",{"2":{"93":1}}],["基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力",{"2":{"90":1}}],["基于以上背景",{"2":{"83":1}}],["基于",{"2":{"70":1}}],["基于分层编码器的代价图",{"2":{"70":1}}],["基于层次编码器的成本图生成使用层次化的骨干网络",{"2":{"70":1}}],["基于层次编码器的成本图生成和逐渐融合的解码器",{"2":{"70":1}}],["基于聚合的融合易忽略模态内传播",{"2":{"55":1}}],["基于对齐的融合因信息交换弱",{"2":{"55":1}}],["基于cam的选择略优于基于边缘的选择",{"2":{"49":1}}],["基于图像级标签的wsss常用方法是先训练图像分类网络",{"2":{"45":1}}],["基于全卷积网络",{"2":{"39":1}}],["基于传播的方法面临计算和内存限制",{"2":{"29":1}}],["基于人类从粗略到精细地逐步区分物体的方式",{"2":{"29":1}}],["基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机",{"2":{"27":1}}],["基于此",{"2":{"27":1}}],["基于此生成前景和背景的预测图并进行监督",{"2":{"8":1}}],["基于像素匹配的方法建立支持像素和查询像素的密集关联",{"2":{"20":1}}],["基于原型的方法用原型代表目标类信息进行匹配预测",{"2":{"20":1}}],["基于度量学习的fss主要分为基于原型和基于像素匹配两类方法",{"2":{"20":1}}],["基于上述问题",{"2":{"9":1,"19":1}}],["之间的多粒度互补关系",{"2":{"62":1}}],["捕捉细节特征",{"2":{"62":1}}],["表示两个区域的交并比",{"2":{"128":1}}],["表现更优",{"2":{"84":1}}],["表征整体类别特征",{"2":{"62":1}}],["表明其在实际场景中能实现更高效的语义分割",{"2":{"97":1}}],["表明注意力加权机制优于平均加权",{"2":{"49":1}}],["表明unc",{"2":{"49":1}}],["表明强化确定特征与unc",{"2":{"49":1}}],["表明强化不确定特征对语义澄清很重要",{"2":{"49":1}}],["表明该编码器能灵活地为不同的新类别生成类别感知特征",{"2":{"41":1}}],["表明从支持和查询图像中提取特定类别的线索能显著增强类别感知能力",{"2":{"41":1}}],["表明dfq使来自不同领域的样本更均匀地混合",{"2":{"36":1}}],["表明引入的模块有效解决了固有偏差",{"2":{"24":1}}],["背景的像素级分类精度不足",{"2":{"62":1}}],["核心模型是多尺度编码器",{"2":{"96":1}}],["核心挑战在于如何从有限标注数据中提取类别本质特征",{"2":{"62":1}}],["核心思想",{"2":{"57":1}}],["就是从像素层面上对图像进行描述",{"2":{"124":1}}],["就要找失败原因了",{"2":{"111":1}}],["就需要进行多次不同命令的转换编译",{"2":{"108":1}}],["就在多个下游多模态图像分割任务中取得了sota性能",{"2":{"59":1}}],["就在多个多模态图像分割基准任务中刷新了最高性能记录",{"2":{"54":1}}],["就能在各项指标上取得优于现有方法的性能",{"2":{"57":1}}],["优化策略",{"2":{"57":1}}],["优于之前的单阶段解决方案",{"2":{"48":1}}],["平衡了模态内和模态间的语义传播",{"2":{"59":1}}],["平衡模态内和模态间的语义传播",{"2":{"57":1}}],["平均交并比",{"2":{"128":1}}],["平均准确率",{"2":{"128":1}}],["平均dsc提高了1",{"2":{"36":1}}],["平均边界准确率",{"2":{"29":1}}],["把标记送入分组提示器",{"2":{"57":1}}],["输入通道数",{"2":{"131":1}}],["输入的每一条边补充0的层数",{"2":{"130":1}}],["输入的图像大小为w×h的",{"2":{"125":1}}],["输入信号的通道数",{"2":{"130":1}}],["输入",{"2":{"103":1,"104":1}}],["输入cmd",{"2":{"101":1}}],["输入处理",{"2":{"57":1}}],["输出通道数",{"2":{"131":1}}],["输出尺寸的计算公式为",{"2":{"130":1}}],["输出边补充0的层数",{"2":{"130":1}}],["输出也可能会比实际输入更大或者更小一些",{"2":{"125":1}}],["输出节点数为",{"2":{"125":1}}],["输出经过调整大小后传递给分类头",{"2":{"65":1}}],["输出步长多设为",{"2":{"29":1}}],["整体架构",{"2":{"57":1,"65":1}}],["参数",{"2":{"116":1}}],["参数多",{"2":{"64":1}}],["参数存储负担大",{"2":{"55":1}}],["参数高效微调",{"2":{"27":2}}],["全景分割",{"2":{"124":1}}],["全景质量",{"2":{"29":1}}],["全局支持原型生成",{"2":{"65":1}}],["全监督语义分割取得显著成功",{"2":{"63":1}}],["全微调虽有效",{"2":{"55":1}}],["全微调方法的局限",{"2":{"55":1}}],["现在编译的结果为内部查看器查看",{"2":{"111":1}}],["现存方法的挑战",{"2":{"55":1}}],["现有涂鸦监督语义分割方法主要依赖正则化损失",{"2":{"83":1,"84":1}}],["现有公开数据集的不同分类标准阻碍了它们的直接融合",{"2":{"73":1}}],["现有多模态分割方法主要分为基于对齐和基于聚合的融合",{"2":{"55":1}}],["现有fss方法的问题",{"2":{"41":1}}],["现有fss模型虽有成果",{"2":{"19":1}}],["现有白天方法在夜间性能会下降",{"2":{"39":1}}],["现有领域泛化医学图像分割方法主要分为学习形状不变特征和明确学习多源域间的域间偏移两类",{"2":{"33":1}}],["现有方法在语义分割任务中效果欠佳",{"2":{"93":1}}],["现有方法",{"2":{"84":1}}],["现有方法通常先加载基于rgb的预训练模型参数",{"2":{"56":1}}],["现有方法的局限性",{"2":{"45":1,"83":1}}],["现有方法分学习形状不变特征和学习域间偏移两类",{"2":{"34":1}}],["现有方法分为基于度量和基于关系两类",{"2":{"10":1}}],["现有方法不足",{"2":{"33":1}}],["现有方法主要聚焦于学习形状不变性表征或实现源域间的特征共识",{"2":{"32":1}}],["现有方法主要基于语义一致性进行预测",{"2":{"21":1}}],["现有方法利用单层注意力机制建立支持集和查询集的关系",{"2":{"21":1}}],["现有方法多依赖语义相关性",{"2":{"19":1}}],["现有主要的cd",{"2":{"9":1}}],["首先是类感知单模态提示器",{"2":{"54":1}}],["首次在",{"2":{"27":1}}],["受视觉变压器启发",{"2":{"64":1}}],["受大语言模型中提示学习方法取得突破的启发",{"2":{"54":1}}],["受此启发",{"2":{"41":1}}],["红外等",{"2":{"54":1}}],["启发",{"2":{"50":1}}],["加入高斯模糊或日光化处理时",{"2":{"49":1}}],["例如",{"2":{"49":1,"83":1,"96":2,"97":1}}],["例如两种vit变体在10个块时效果最佳",{"2":{"41":1}}],["两阶段先生成掩码提案再分类",{"2":{"70":1}}],["两阶段的框架存在不足",{"2":{"70":1}}],["两者结合时",{"2":{"67":1}}],["两者结合可进一步提高伪标签和预测标签的质量",{"2":{"50":1}}],["两种传统方法进行比较",{"2":{"49":1}}],["两个基本基线",{"2":{"27":1}}],["略高于多阶段的mctformer",{"2":{"48":1}}],["投影器",{"2":{"47":1}}],["聚合类感知表示并辅助建模公共统计信息",{"2":{"59":1}}],["聚合辅助模态的类感知表示",{"2":{"57":1}}],["聚合模块",{"2":{"47":1}}],["聚合多级别支持掩码等",{"2":{"20":1}}],["借助全局上下文理解",{"2":{"94":1}}],["借助语义约束分离图像",{"2":{"39":1}}],["借鉴",{"2":{"47":1}}],["防止过度平滑",{"2":{"47":1}}],["上采样实现方法",{"0":{"147":1}}],["上采样",{"0":{"130":1,"145":1},"1":{"146":1,"147":1,"148":1}}],["上查看",{"2":{"117":1}}],["上面代码串中记得进行",{"2":{"116":1}}],["上",{"2":{"111":1}}],["上的实验结果表明",{"2":{"96":1}}],["上的实验表明",{"2":{"70":1}}],["上分别取得",{"2":{"73":1}}],["上进行了广泛实验",{"2":{"57":1}}],["上预训练的",{"2":{"47":1}}],["上海人工智能实验室",{"0":{"27":1}}],["单阶段直接扩展视觉",{"2":{"70":1}}],["单阶段的框架存在不足",{"2":{"70":1}}],["单阶段方法将分类",{"2":{"46":1}}],["单样本设置下增益更大",{"2":{"66":1}}],["单样本设置下高出2",{"2":{"66":1}}],["单独使用dcam增强类别原型的判别能力可提升2",{"2":{"24":1}}],["单独使用tem增强查询前景特征可使性能提升0",{"2":{"24":1}}],["影响了对复杂场景的理解和处理能力",{"2":{"70":1}}],["影响分割效果",{"2":{"63":1}}],["影响分割精度",{"2":{"45":1}}],["影响模型对未见领域的泛化能力",{"2":{"33":1}}],["它并不是正向卷积的完全逆过程",{"2":{"130":1}}],["它将每个像素分类为属于对象类以及该类的实体",{"2":{"124":1}}],["它是将每个像素分类为属于对象类的过程",{"2":{"124":1}}],["它不仅能够对代码高亮",{"2":{"100":1}}],["它对于括号根本就没有高亮",{"2":{"100":1}}],["它主要关注对象最具判别性的区域",{"2":{"45":1}}],["它们的分割性能得到增强",{"2":{"29":1}}],["再进行普通的卷积",{"2":{"130":1}}],["再点击安装即可",{"2":{"101":1}}],["再点击安装",{"2":{"101":1}}],["再基于此和分层编码器的不同特征图",{"2":{"70":1}}],["再迁移到查询输入进行测试",{"2":{"63":1}}],["再输入到基础模型的下一层",{"2":{"57":1}}],["再在特定下游任务数据集上微调",{"2":{"56":1}}],["再训练分割模型评估性能",{"2":{"46":1}}],["再将其细化为伪分割标签来监督分割网络",{"2":{"45":1}}],["再以此为指导在整个查询特征图中寻找特征相似度高的点",{"2":{"21":1}}],["弱监督语义分割",{"2":{"50":1}}],["弱监督语义分割方法",{"2":{"46":1}}],["弱",{"2":{"45":1}}],["弱标签甚至零样本等更现实的场景时",{"2":{"41":1}}],["弱标签和零样本分割等场景",{"2":{"41":1}}],["弱标签fss和零",{"2":{"41":1}}],["涂鸦监督能提供更多关键语义信息",{"2":{"84":1}}],["涂鸦监督语义分割",{"2":{"82":1}}],["涂鸦标签属于弱监督学习",{"2":{"83":1}}],["涂鸦",{"2":{"45":1}}],["确定区域应当具备足够的鲁棒性以保持全局语义特征",{"2":{"44":1}}],["确保在自适应变换过程中前景对象的不变性",{"2":{"11":1}}],["来扩大输入图像的尺寸",{"2":{"130":1}}],["来自dino",{"2":{"96":1}}],["来自原始的",{"2":{"70":1}}],["来增强全局上下文信息的传播",{"2":{"96":1}}],["来实现这一目标",{"2":{"90":1}}],["来预测像素级的图像",{"2":{"70":1}}],["来恢复局部特征信息",{"2":{"44":1}}],["来进行超高分辨率图像的分割细化任务",{"2":{"29":1}}],["面临一个关键挑战",{"2":{"44":1}}],["面对显著的分布差异",{"2":{"32":1}}],["kerner",{"2":{"130":1}}],["kernel",{"2":{"130":1,"131":18}}],["kernelsize−stride",{"2":{"130":1}}],["kernelsize=",{"2":{"130":2}}],["kernelsize",{"2":{"130":1}}],["keybinding",{"2":{"107":3,"108":1,"119":1}}],["key",{"2":{"6":1,"39":1,"41":1,"70":1,"92":1,"131":2}}],["k百分比为8",{"2":{"87":1}}],["knowledge",{"2":{"43":1,"44":1}}],["zhihu",{"2":{"119":1}}],["zhuanlan",{"2":{"119":1}}],["zhejiang",{"2":{"42":2}}],["zihao",{"2":{"110":1}}],["zero",{"2":{"41":1,"72":1,"73":1,"90":3}}],["当同步到外部查看器时",{"2":{"119":1}}],["当编译成功后",{"2":{"111":1}}],["当编译失败时",{"2":{"108":1}}],["当发现页面下方出现",{"2":{"111":1}}],["当其余编译器引用时该",{"2":{"108":1}}],["当代码被保存时自动编译文件",{"2":{"108":1}}],["当出现下图所示弹窗时",{"2":{"101":1}}],["当上面标示的时间安装完之后",{"2":{"101":1}}],["当公式比较长的时候",{"2":{"100":1}}],["当每个类别的全局原型数量增加到约5时",{"2":{"87":1}}],["当前最先进的语义分割方法在预设的封闭数据集上表现出色",{"2":{"73":1}}],["当前主流方法主要通过对基于可见光",{"2":{"54":1}}],["当使用convnext",{"2":{"70":1}}],["当将其扩展到跨领域",{"2":{"41":1}}],["当部分掩码数量",{"2":{"41":1}}],["做法后",{"2":{"41":1}}],["盲目使用变压器提取特征可能无法带来预期的性能提升",{"2":{"41":1}}],["块的数量并非越多越好",{"2":{"41":1}}],["块数量的影响",{"2":{"41":1}}],["较小的vit",{"2":{"41":1}}],["较为合适",{"2":{"41":1}}],["说明",{"2":{"128":1}}],["说明编译失败",{"2":{"111":1}}],["说明编译成功",{"2":{"111":1}}],["说明安装完毕",{"2":{"101":1}}],["说明两者对于查询图像的分割都至关重要",{"2":{"41":1}}],["说明挖掘细粒度的部分语义能进一步发挥提示的作用",{"2":{"41":1}}],["产生冗余和噪声",{"2":{"41":1}}],["产生空间感知偏差",{"2":{"19":1}}],["继续增加数量",{"2":{"41":1}}],["语言模型进行分割",{"2":{"70":1}}],["语言模型",{"2":{"70":2}}],["语言信息比支持平均令牌更具类别代表性",{"2":{"41":1}}],["语言信息的优势",{"2":{"41":1}}],["语义提示转移",{"2":{"41":1}}],["语义提示迁移",{"2":{"41":1}}],["语义导向解耦",{"2":{"39":1}}],["语义分割常用优化目标",{"0":{"144":1}}],["语义分割评估指标",{"0":{"143":1}}],["语义分割的思路",{"0":{"127":1}}],["语义分割模型",{"2":{"126":1}}],["语义分割只需要对像素进行分类就行了",{"2":{"124":1}}],["语义分割概述",{"0":{"123":1},"1":{"124":1,"125":1,"126":1,"127":1,"128":1}}],["语义分割是计算机视觉的基础任务",{"2":{"93":1,"94":1}}],["语义分割是计算机视觉领域的基础且关键任务",{"2":{"63":1}}],["语义分割方法",{"2":{"70":1}}],["语义分割旨在为场景中每个像素分配语义类别",{"2":{"55":1}}],["语义分割",{"2":{"29":1,"39":1,"64":1,"124":1}}],["语义分割近年来依赖大规模标注数据集取得快速发展",{"2":{"9":1}}],["体现了背景语义对分割的重要性",{"2":{"41":1}}],["760",{"2":{"72":1}}],["768大小",{"2":{"70":1}}],["768",{"2":{"70":3}}],["7倍的加速",{"2":{"70":1}}],["7",{"0":{"109":1,"110":1,"111":1},"1":{"110":1,"111":1},"2":{"48":1,"59":2,"66":1,"70":1,"72":1,"73":1,"107":1,"131":1}}],["78",{"2":{"41":1}}],["746",{"2":{"72":1}}],["74",{"2":{"12":1}}],["挖掘细粒度语义提示",{"2":{"41":1}}],["挖掘目标对象自身的空间一致性",{"2":{"21":1}}],["精确转移语义",{"2":{"41":1}}],["精确的图像分割细化",{"2":{"29":1}}],["构建了包含多尺度编码器",{"2":{"99":1}}],["构建数据引擎收集sa",{"2":{"90":1}}],["构建不同粒度原型之间的交互",{"2":{"65":1}}],["构建部分掩码生成器",{"2":{"41":1}}],["构建三个关键增强点",{"2":{"41":1}}],["构建框架",{"2":{"27":1}}],["动态构建教师网络",{"2":{"46":1}}],["动态驱动编码器关注特定对象",{"2":{"41":1}}],["动态类别感知提示范式",{"2":{"41":1}}],["源于自然语言处理",{"2":{"41":1}}],["源数据集和目标数据集存在较大领域差距",{"2":{"9":1}}],["人类能够以独特的视觉感知模式选择性地关注视线中的关键对象",{"2":{"41":1}}],["人类视觉感知的启示",{"2":{"41":1}}],["人类可以轻松聚焦于视线中的特定物体",{"2":{"41":1}}],["会将所有的非",{"2":{"108":1}}],["会出现一些配置文件的安装运行写入",{"2":{"101":1}}],["会先出现下图",{"2":{"101":1}}],["会激活与目标类别无关的对象",{"2":{"41":1}}],["会导致注意力偏差",{"2":{"19":1}}],["半监督学习也难以很好地泛化到未见类别",{"2":{"41":1}}],["半监督和多标注场景下的分割研究也有开展",{"2":{"34":1}}],["遥感领域",{"2":{"41":1}}],["令人惊喜的是",{"2":{"41":1}}],["令人惊讶的是",{"2":{"41":1}}],["令牌长度和秩对模型性能的影响",{"2":{"27":1}}],["部分工作构建多分辨率特征图用于密集输出",{"2":{"95":1}}],["部分工作探索了与少样本学习的结合",{"2":{"41":1}}],["部分引入像素",{"2":{"95":1}}],["部分研究还改进了训练目标和架构",{"2":{"95":1}}],["部分研究开始探索适用于fss的特征编码器",{"2":{"41":1}}],["部分采用视觉变换器提升长程建模能力",{"2":{"46":1}}],["部分掩码数量影响",{"2":{"41":1}}],["部分掩码生成器",{"2":{"41":2}}],["部分方法在弱监督语义分割中探索了原型的使用",{"2":{"84":1}}],["部分方法引入文本信息用于分类",{"2":{"41":1}}],["部分方法开始应用于计算机视觉",{"2":{"27":1}}],["感兴趣的物体",{"2":{"41":1}}],["尤其是在当前大模型时代",{"2":{"41":1}}],["类目标",{"2":{"70":1}}],["类间相似性使像素级二分类困难",{"2":{"63":1}}],["类感知单模态提示器",{"2":{"57":1}}],["类别识别局限",{"2":{"70":1}}],["类别模板数量",{"2":{"70":1}}],["类别早期拒接",{"2":{"70":1}}],["类别",{"2":{"41":1}}],["类内差异会导致查询图像出现语义错误",{"2":{"63":1}}],["类内差异表示在三种不同的注意力机制中都有益",{"2":{"24":1}}],["类内差异表示利用一组可学习向量建模支持集和查询集之间的差异",{"2":{"21":1}}],["类内外观差异以及信息利用不充分等关键问题",{"2":{"9":1}}],["框架",{"2":{"39":1}}],["框架能够在不受光照影响的情况下提取反射成分",{"2":{"39":1}}],["框架能够以端到端方式无缝集成于transformer分割模型",{"2":{"32":1}}],["细节处理欠佳",{"2":{"90":1}}],["细节重建难",{"2":{"29":1}}],["细化预测结果",{"2":{"85":1}}],["细化nightcity数据集",{"2":{"39":1}}],["创新点",{"2":{"39":1,"90":1}}],["创新性地使用解耦的深度特征作为查询",{"2":{"38":1}}],["早期通过学习特征映射对齐视觉和文本特征",{"2":{"70":1}}],["早期基于预训练的视觉和语言模型开发",{"2":{"70":1}}],["早期因缺乏大规模标注数据集",{"2":{"39":1}}],["早期u",{"2":{"34":1}}],["夜间语义分割",{"2":{"39":1}}],["夜间场景光照强度低且人工光源复杂",{"2":{"39":1}}],["难以提取用于语义分割的判别特征",{"2":{"39":1}}],["难以对单张图像中不同实例的特征进行细化",{"2":{"27":1}}],["不然会报错",{"2":{"116":1}}],["不需要进行更改",{"2":{"108":1}}],["不包含外部",{"2":{"107":1}}],["不要对其进行修改",{"2":{"101":1}}],["不怎么好用",{"2":{"101":1}}],["不在本文探讨范围之内",{"2":{"100":1}}],["不够稳健",{"2":{"90":1}}],["不相连的虚假组件",{"2":{"90":1}}],["不过",{"2":{"70":1}}],["不能识别出来在训练集中没有的未知场景",{"2":{"70":1}}],["不同的是",{"2":{"105":1}}],["不同级别括号用不同颜色标注了",{"2":{"100":1}}],["不同标签体系的数据",{"2":{"73":1}}],["不同超参数",{"2":{"67":1}}],["不同类但纹理相似的对象同时出现时",{"2":{"63":1}}],["不同模态有效信息不同",{"2":{"55":1}}],["不同成像机制的模态存在异质差距",{"2":{"55":1}}],["不同骨干网络中",{"2":{"41":1}}],["不同骨干网络的性能",{"2":{"41":1}}],["不确定特征掩码在大多数情况下比随机特征掩码性能更高",{"2":{"49":1}}],["不确定特征选择分析",{"2":{"49":1}}],["不适合夜间复杂的光照条件",{"2":{"39":1}}],["不足",{"2":{"29":1,"70":1}}],["一个池化层到上一个池化层之间的部分认为一个卷积块",{"2":{"131":1}}],["一个全新的图像分割任务",{"2":{"90":1}}],["一般来说",{"2":{"128":1}}],["一定要选上",{"2":{"102":1}}],["一定程度上改善了夜间场景表现",{"2":{"39":1}}],["一些方法聚焦实例级",{"2":{"95":1}}],["一些专门的工具可能会比sam表现更好",{"2":{"90":1}}],["一致性监督",{"2":{"85":1}}],["一致性损失未在类别层面提供直接监督",{"2":{"83":1}}],["一致性损失",{"2":{"83":1,"84":1}}],["一致",{"2":{"70":1}}],["一方面",{"2":{"55":1}}],["一方面模型可迁移性较弱",{"2":{"54":1}}],["一是特征提取阶段",{"2":{"19":1}}],["虽有nightcity等大规模夜间数据集及相关方法提出",{"2":{"39":1}}],["虽能取得较好性能",{"2":{"29":1}}],["视觉transformer",{"2":{"95":1}}],["视觉编码器形式的预训练",{"2":{"70":1}}],["视觉",{"2":{"70":1}}],["视觉提示学习应用",{"2":{"56":1}}],["视觉变压器",{"2":{"47":1}}],["视觉系统近一半时间需在光照不足且复杂的夜间环境下工作",{"2":{"39":1}}],["视觉基础模型",{"2":{"27":2}}],["光照感知解析器",{"2":{"39":1}}],["展现出优异的跨领域泛化能力",{"2":{"73":1}}],["展现出高效性和优越性",{"2":{"57":1}}],["展现出显著优势",{"2":{"54":1}}],["展现出了卓越的领域泛化能力",{"2":{"38":1}}],["展示了fsr在不确定区域",{"2":{"49":1}}],["展示了该方法在图像分割细化上的高效性和快速性",{"2":{"29":1}}],["展示将crm应用于全景分割的可视化结果",{"2":{"29":1}}],["展示cascadepsp",{"2":{"29":1}}],["引用查看",{"2":{"116":1}}],["引导整个框架学习不同领域相似的通道特征模式",{"2":{"38":1}}],["引入辅助无标签分支作为有效的数据利用方法",{"2":{"65":1}}],["引入了对齐诱导的跨模态提示器",{"2":{"59":1}}],["引入显式语义分组机制到提示学习中",{"2":{"57":1}}],["引入额外的语言信息有助于生成更强大的提示",{"2":{"41":1}}],["引入额外类别语义",{"2":{"41":1}}],["引入背景提示",{"2":{"41":1}}],["引入前景提示",{"2":{"41":1}}],["引入跨模态语言信息初始化提示",{"2":{"41":1}}],["引入跨模态的语言信息来初始化每个任务的提示",{"2":{"41":1}}],["引入光照感知解析器",{"2":{"39":2}}],["引入隐函数",{"2":{"29":1}}],["引入深度卷积网络",{"2":{"29":1}}],["引入基于卷积transformer架构的多尺度局部感知调制transformer进行多尺度特征提取",{"2":{"21":1}}],["各组件有效性",{"2":{"87":1}}],["各组件实验",{"2":{"37":1}}],["各尺度实验",{"2":{"37":1}}],["眼底图像分割基准",{"2":{"36":1}}],["最突出的特点就是其强大的插件功能",{"2":{"100":1}}],["最让人头疼的是",{"2":{"100":1}}],["最高提升了2",{"2":{"36":1}}],["最后提出本文的不足",{"2":{"90":1}}],["最后将两个掩码相加得到最终的查询前景分割图",{"2":{"21":1}}],["最后",{"2":{"18":1}}],["四",{"2":{"36":1}}],["前端的选项",{"2":{"101":1}}],["前景和背景的局部特征易混淆",{"2":{"63":1}}],["前者通过融合深浅层特征",{"2":{"70":1}}],["前者通过条件损失对齐子网络嵌入",{"2":{"56":1}}],["前者将支持图像表示为类原型",{"2":{"10":1}}],["前列腺分割基准",{"2":{"36":1}}],["∑i=14lirdwt",{"2":{"35":1}}],["损失函数",{"2":{"35":1,"65":1}}],["损失函数进行监督",{"2":{"11":1}}],["然后按ctrl+alt+v",{"2":{"111":1}}],["然后按下该快捷键",{"2":{"111":1}}],["然后进行图示操作",{"2":{"111":1}}],["然后输入命令xelatex",{"2":{"101":1}}],["然后手动选择某一镜像网站进行下载",{"2":{"101":1}}],["然后与查询特征和先验掩码连接",{"2":{"65":1}}],["然后通过自适应融合这两部分信息来进行语义识别",{"2":{"39":1}}],["然后将结果输入到语义分割头进行最终预测",{"2":{"35":1}}],["然而在实际应用中",{"2":{"39":1}}],["然而",{"2":{"8":1,"27":1,"29":1,"32":1,"41":2,"45":1,"63":1,"73":1,"82":1,"93":1,"94":1}}],["解码器深度",{"2":{"98":1}}],["解码器由全局注意力阶段",{"2":{"96":1}}],["解码器视觉变压器",{"2":{"96":1}}],["解码器",{"2":{"47":1}}],["解码器架构mevt和自监督训练策略的框架",{"2":{"99":1}}],["解码器架构生成具有全局上下文传播能力的高分辨率特征",{"2":{"93":1}}],["解码器架构",{"2":{"70":1}}],["解码器架构被广泛应用",{"2":{"64":1}}],["解码器架构实现图像级监督的语义分割",{"2":{"47":1}}],["解码器架构的方法成为主流",{"2":{"39":1}}],["解码泛化表示",{"2":{"35":1}}],["解决固定编码器类别无关问题",{"2":{"41":1}}],["解决固有偏差",{"2":{"25":1}}],["解决方案",{"2":{"35":2}}],["解决该问题将是未来工作方向",{"2":{"29":1}}],["解决特征分类阶段的空间感知偏差问题",{"2":{"21":1}}],["解决特征匹配阶段的注意力偏差问题",{"2":{"21":1}}],["深层",{"2":{"49":1}}],["深层特征查询对不同领域浅层表示的一致性施加了隐式约束",{"2":{"35":1}}],["深度多模态融合展现出比单模态分割更显著的优势",{"2":{"55":1}}],["深度学习",{"2":{"122":1}}],["深度学习笔记",{"0":{"122":1}}],["深度学习技术推动了深度神经网络在图像分割的发展",{"2":{"83":1}}],["深度学习技术推动医学图像分割发展",{"2":{"34":1}}],["深度学习推动下",{"2":{"55":1}}],["深度学习在计算机视觉任务中取得显著进展",{"2":{"41":1}}],["深度表示解耦",{"2":{"39":1}}],["深度神经网络倾向于在多个通道中提取相似模式",{"2":{"35":1}}],["深度特征与浅层特征间的长程依赖关系可经由自注意力充分挖掘",{"2":{"32":1}}],["键和值基于浅层特征",{"2":{"35":1}}],["通常与之前的完全监督方法相当",{"2":{"90":1}}],["通道解耦特征虽增强了深度神经网络在跨领域场景中的表示能力",{"2":{"35":1}}],["通过调整其与",{"2":{"112":1}}],["通过网址",{"2":{"101":1}}],["通过实验和消融研究",{"2":{"96":1}}],["通过实验确定了tsf策略中微调编码器层能取得最佳性能",{"2":{"13":1}}],["通过最小化交叉熵损失将知识从教师网络蒸馏到学生网络",{"2":{"96":1}}],["通过",{"2":{"96":1}}],["通过预训练减少对大量标注样本的依赖显得尤为重要",{"2":{"93":1}}],["通过提示工程实现零样本迁移到下游分割任务",{"2":{"90":1}}],["通过提示工程可适应多种任务和数据分布",{"2":{"90":2}}],["通过使用我们高效的模型进行数据收集",{"2":{"90":1}}],["通过使用涂鸦生成的标注进行模型训练",{"2":{"82":1}}],["通过加权平均形成局部原型",{"2":{"85":1}}],["通过部分交叉熵损失",{"2":{"85":1}}],["通过挖掘特征原型",{"2":{"82":1}}],["通过整合包含",{"2":{"73":1}}],["通过对全局特征进行掩码平均池化",{"2":{"65":1}}],["通过对训练样本的子集进行重采样作为无标签数据",{"2":{"65":1}}],["通过对特定模态的类令牌进行分组",{"2":{"59":1}}],["通过指导模型提取对类内差异具有鲁棒性的本质特征",{"2":{"62":1}}],["通过在解码器的早期层次中剔除不存在的类别",{"2":{"70":1}}],["通过在提示学习中引入显式语义分组",{"2":{"59":1}}],["通过在模型执行的三个阶段进行策略性和高效交互",{"2":{"19":1}}],["通过引入特定于模态的类标记",{"2":{"57":1}}],["通过冻结预训练的基础模型",{"2":{"57":1}}],["通过共享提示参数聚合不同模态的类特征表示",{"2":{"54":1}}],["通过聚类同类模态特征",{"2":{"54":1}}],["通过语义分组机制学习模态专属提示",{"2":{"54":1}}],["通过分析注意力机制",{"2":{"49":1}}],["通过学生和教师管道实现自蒸馏",{"2":{"47":1}}],["通过自适应划分图像内容为确定区域和不确定区域并分别处理",{"2":{"45":1}}],["通过自蒸馏知识",{"2":{"44":1}}],["通过约束高置信区域与经过数据增强的同类别图像视图之间的语义一致性来强化模型",{"2":{"44":1}}],["通过精确地将图像中的类别特定语义迁移到提示中",{"2":{"41":1}}],["通过观察到光照成分可以作为一些语义模糊区域的线索",{"2":{"39":1}}],["通过t",{"2":{"36":1}}],["通过计算并可视化特征查询的协方差矩阵",{"2":{"36":1}}],["通过一个由权重w1和偏置b1参数化的线性层对学习到的泛化表示进行特征融合",{"2":{"35":1}}],["通过交叉注意力机制",{"2":{"32":1}}],["通过可学习令牌对特征图进行实例级细化",{"2":{"27":1}}],["通过消融实验",{"2":{"59":1}}],["通过消融实验分析rein各组件的有效性",{"2":{"27":1}}],["通过消融实验评估不同的dcm组件",{"2":{"24":1}}],["通过广泛的实验验证",{"2":{"27":1}}],["通过减少可训练的参数",{"2":{"27":1}}],["通过利用更强大的预训练模型和更少的可训练参数",{"2":{"27":1}}],["通过修改模型采用不同的注意力机制",{"2":{"24":1}}],["通过与其他方法在计算量和准确性方面进行对比实验",{"2":{"24":1}}],["通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别",{"2":{"21":1}}],["通过增强查询特征的语义和空间感知能力",{"2":{"18":1}}],["通过尝试预测支持图像的真实掩码来微调网络",{"2":{"11":1}}],["通过求解线性方程得到变换矩阵",{"2":{"11":1}}],["通过测量支持局部原型与查询全局特征之间的相似性",{"2":{"11":1}}],["通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配",{"2":{"11":1}}],["通过查询图像自身构建特定的转换矩阵",{"2":{"8":1}}],["通过非参数测量工具分割查询图像",{"2":{"10":1}}],["从输入通道到输出通道的阻塞连接数",{"2":{"130":1}}],["从使用的包中自动补全命令和环境",{"2":{"119":1}}],["从不自动编译",{"2":{"108":1}}],["从上文整个代码块儿可以看出此规则",{"2":{"107":1}}],["从一个块增加到两个块可提高线性探测和微调性能",{"2":{"98":1}}],["从涂鸦监督初始结果的置信部分提取原型",{"2":{"88":1}}],["从初始预测图的高置信区域中提取对应特征向量",{"2":{"85":1}}],["从查询分支中额外提取局部原型",{"2":{"65":1}}],["从查询分支中提取局部原型",{"2":{"65":1}}],["从支持特征中提取全局原型",{"2":{"65":2}}],["从1增加到8时",{"2":{"41":1}}],["从解耦特征查询中学习",{"2":{"35":1}}],["从而补全正在编写的代码",{"2":{"108":1}}],["从而可以对",{"2":{"107":1}}],["从而直接完成相应设置",{"2":{"105":1}}],["从而将之配置为高度个性化的编辑器",{"2":{"100":1}}],["从而推动图像分割进入基础模型时代",{"2":{"90":1}}],["从而完成整幅图像的语义分割",{"2":{"82":1}}],["从而在不牺牲精度的情况下",{"2":{"70":1}}],["从而影响",{"2":{"44":1}}],["从而提高分割精度",{"2":{"41":1}}],["从而提高预测的精度",{"2":{"39":1}}],["从而指导泛化表征的学习",{"2":{"32":1}}],["从而减轻固有的偏差",{"2":{"18":1}}],["从而为查询图像生成更准确的掩码",{"2":{"11":1}}],["从而增强分割结果",{"2":{"8":1}}],["问题",{"2":{"35":2}}],["后如clip从大规模图像",{"2":{"70":1}}],["后者将transformer用作骨干网络或分割解码器",{"2":{"70":1}}],["后者运用特定算子组合多模态子网络",{"2":{"56":1}}],["后者构建支持",{"2":{"10":1}}],["后deeplab及改进模型成为趋势",{"2":{"34":1}}],["网络为捕捉各领域模式会在多通道学习相似模式",{"2":{"33":1}}],["网络往往通过多个通道捕捉相似模式",{"2":{"32":1}}],["浅层特征的特征不对齐问题明显",{"2":{"33":1}}],["特别是公式比较多的数学专业",{"2":{"100":1}}],["特别是在元测试阶段",{"2":{"8":1}}],["特定领域表现不佳",{"2":{"90":1}}],["特定提示设计困难",{"2":{"90":1}}],["特定场景表现弱",{"2":{"90":1}}],["特征图填充宽度",{"2":{"130":1}}],["特征学习",{"2":{"96":1}}],["特征进行预训练",{"2":{"96":1}}],["特征增强",{"2":{"85":1}}],["特征提取",{"2":{"85":1}}],["特征原型在计算机视觉任务中用于增强模型识别能力",{"2":{"84":1}}],["特征激活",{"2":{"65":1}}],["特征激活模块",{"2":{"65":1}}],["特征融合",{"2":{"57":1}}],["特征融合和像素匹配三种方法",{"2":{"41":1}}],["特征自强化分析",{"2":{"49":1}}],["特征查询和松弛深度白化变换",{"2":{"37":1}}],["特征问题",{"2":{"33":1}}],["只不过相比较目标检测的矩形框",{"2":{"128":1}}],["只是提醒该插件已经更新到了8",{"2":{"104":1}}],["只需将此变量设置为true即可恢复菜单",{"2":{"108":1}}],["只需要更改下图框选出的部分即可",{"2":{"101":1}}],["只需按照图片中所指向图标进行配置即可",{"2":{"100":1}}],["只训练图像编码器和解码器",{"2":{"70":1}}],["只关注通道之间的相关性",{"2":{"35":1}}],["只能泛化到训练中见过的目标域",{"2":{"33":1}}],["只微调编码器的少数参数",{"2":{"11":1}}],["过多的部分掩码可能导致目标对象无法清晰划分",{"2":{"41":1}}],["过去医学图像分割的领域适应研究需目标域样本参与训练",{"2":{"33":1}}],["过滤无关像素",{"2":{"20":1}}],["医学影像",{"2":{"135":1}}],["医学",{"2":{"41":1}}],["医学图像分割",{"2":{"135":1}}],["医学图像分割领域泛化旨在从源域学习泛化到任意未见目标域的语义表示",{"2":{"34":1}}],["医学图像分割技术发展",{"2":{"34":1}}],["医学图像来自不同医院",{"2":{"33":1}}],["医学诊断等领域带来了新机遇",{"2":{"29":1}}],["指标分别以1",{"2":{"32":1}}],["所谓的分割",{"2":{"124":1}}],["所以得到的输出",{"2":{"125":1}}],["所以",{"2":{"125":1}}],["所以生成pdf时",{"2":{"108":1}}],["所以传统的语义分割取得了飞速进步",{"2":{"19":1}}],["所提方法显著优于其他单阶段方法",{"2":{"50":1}}],["所提出的方法显示出更精确和合理的预测",{"2":{"36":1}}],["所提出的解耦特征查询",{"2":{"32":1}}],["理想的泛化表征应确保跨域图像在相同特征通道上呈现相似的模式响应",{"2":{"32":1}}],["即要将输入扩大的倍数",{"2":{"130":1}}],["即要对所有目标都检测出来",{"2":{"124":1}}],["即某一个像素属于哪一类物体",{"2":{"124":1}}],["即从",{"2":{"111":1}}],["即从代码定位到",{"2":{"111":1}}],["即从代码定位到编译出来的",{"2":{"108":1}}],["即从编译出的",{"2":{"108":1}}],["即此命令设置是否将编译文档的选项出现在鼠标右键的菜单中",{"2":{"108":1}}],["即变量设置为false",{"2":{"108":1}}],["即变为",{"2":{"107":1}}],["即需编写者手动编译文档",{"2":{"108":1}}],["即当检测到代码被更改时就自动编译tex文件",{"2":{"108":1}}],["即什么时候自动进行代码的编译",{"2":{"108":1}}],["即可使用",{"2":{"117":1}}],["即可完成",{"2":{"107":1}}],["即可",{"2":{"104":1}}],["即使加入额外的网络来提供空间信息",{"2":{"70":1}}],["即使从低分辨率细化到高分辨率",{"2":{"29":1}}],["即l=lseg+λ",{"2":{"35":1}}],["即医学图像采集自不同医院和扫描设备",{"2":{"32":1}}],["域泛化医学图像分割任务要求模型能够从多个源域学习",{"2":{"32":1}}],["域随机化",{"2":{"10":1}}],["jsexport",{"2":{"157":1}}],["js",{"2":{"115":1,"116":1,"119":1}}],["json界面设置",{"2":{"118":1}}],["json",{"2":{"105":1,"107":2,"115":1,"117":1}}],["j",{"2":{"72":1}}],["jarvis",{"2":{"30":1}}],["just",{"2":{"27":1}}],["未来计划将此方法应用于其他任务",{"2":{"88":1}}],["未来将探索设计类别注意力策略或使用大规模细粒度数据集来解决该挑战",{"2":{"70":1}}],["未来工作待明确",{"2":{"29":1}}],["未来展望",{"2":{"29":1}}],["目前尚不清楚如何设计简单的提示来实现语义和全景分割",{"2":{"90":1}}],["目前采用",{"2":{"29":2}}],["目标增强模块",{"2":{"18":1,"21":1,"24":1}}],["可学习的上采样",{"2":{"130":1}}],["可直接完整复制文末笔者的个人配置到自己的编译器内",{"2":{"115":1}}],["可选",{"0":{"112":1},"1":{"113":1,"114":1,"115":1,"116":1}}],["可自行通过上文方式设置为您想要的快捷键",{"2":{"111":1}}],["可对其设置快捷键",{"2":{"111":1}}],["可能会导致",{"2":{"108":1}}],["可能无法有效消除通道相关性",{"2":{"35":1}}],["可根据其数字来判断安装所需时间",{"2":{"101":1}}],["可以看成是w×h个分类任务",{"2":{"125":1}}],["可以看到的是",{"2":{"101":1,"108":2,"117":1}}],["可以看到",{"2":{"100":1}}],["可以将较小的",{"2":{"130":1}}],["可以将",{"2":{"117":1}}],["可以实时进行跳转",{"2":{"112":1}}],["可以根据上文进行配置",{"2":{"111":1}}],["可以使用自己的tex文件进行测试",{"2":{"110":1}}],["可以更改的代码为",{"2":{"108":1}}],["可以跳过该小节",{"2":{"108":1}}],["可以直接复制上述代码至",{"2":{"107":1}}],["可以直接按ctrl",{"2":{"105":1}}],["可以点击",{"2":{"104":1}}],["可以返回前一页面",{"2":{"101":1}}],["可以查看此篇文章",{"2":{"101":1}}],["可以这么说",{"2":{"100":1}}],["可降低标注成本",{"2":{"45":1}}],["可分为优化和度量两类",{"2":{"41":1}}],["可提升现有日间方法在夜间的性能",{"2":{"39":1}}],["可视化的类到补丁注意力图显示类令牌可以自适应地学习关注目标区域",{"2":{"49":1}}],["可视化结果显示",{"2":{"41":1}}],["可视化分割结果",{"2":{"36":1}}],["可视化对比",{"2":{"29":1}}],["可处理低分辨率训练和超高清测试之间的分辨率差距",{"2":{"29":1}}],["泛化能力",{"2":{"29":1}}],["有三个参数变量",{"2":{"116":1}}],["有三个选项",{"2":{"108":1}}],["有三种变量参数",{"2":{"116":1}}],["有的时候",{"2":{"112":1}}],["有的地方需要更改路径",{"2":{"100":1}}],["有以下三种方法",{"2":{"111":1}}],["有两个变量",{"2":{"108":1}}],["有具体说明",{"2":{"100":1}}],["有时会生成小的",{"2":{"90":1}}],["有助于最小化领域差距",{"2":{"36":1}}],["有助于聚合特征以重建高分辨率掩码上的细节",{"2":{"29":1}}],["有助于提高性能直至收敛",{"2":{"29":1}}],["有效提升推理速度",{"2":{"70":1}}],["有效提升相似类别的可区分性",{"2":{"62":1}}],["有效捕捉多模态数据的共有统计规律",{"2":{"54":1}}],["有效降低计算成本",{"2":{"29":1}}],["有效减少了背景干扰",{"2":{"25":1}}],["更多详情可以访问",{"2":{"108":1}}],["更多的语义迁移次数通常能带来更高的分割精度",{"2":{"41":1}}],["更多的缩放比例意味着推理分辨率的连续性更好",{"2":{"29":1}}],["更新",{"2":{"96":1}}],["更有效地减少了特征冗余",{"2":{"35":1}}],["更重要的是",{"2":{"29":1}}],["推理连续性的影响",{"2":{"29":1}}],["二",{"2":{"36":1}}],["二者协同作用能提升性能",{"2":{"29":1}}],["二是特征匹配阶段",{"2":{"19":1}}],["验证cam和隐式函数都是crm不可或缺的部分",{"2":{"29":1}}],["验证了sed方法的有效性",{"2":{"70":1}}],["验证了smt",{"2":{"13":1}}],["验证了该方法的有效性",{"2":{"18":1}}],["消融实验显示",{"2":{"68":1}}],["消融实验表明",{"2":{"50":1}}],["消融实验验证",{"2":{"50":1}}],["消融实验",{"0":{"59":1},"2":{"29":1,"39":1}}],["掩码细节和整体分割效果都有显著改善",{"2":{"29":1}}],["掩码注意力",{"2":{"24":1}}],["应如下图页面所示",{"2":{"102":1}}],["应用前景",{"2":{"99":1}}],["应用unc",{"2":{"49":1}}],["应用示例",{"2":{"29":1}}],["应运而生",{"2":{"9":1,"41":1}}],["定义完成后",{"2":{"108":1}}],["定性结果也证明了其有效性",{"2":{"68":1}}],["定性结果也证明了rifenet的有效性",{"2":{"66":1}}],["定性结果展示",{"2":{"29":1}}],["定性评估",{"2":{"41":1}}],["定量结果评估",{"2":{"29":1}}],["8提高到71",{"2":{"98":1}}],["847",{"2":{"70":1}}],["82",{"2":{"70":1}}],["88",{"2":{"37":1}}],["80k",{"2":{"70":1}}],["80",{"2":{"36":1,"70":1}}],["8",{"0":{"112":1,"113":1,"114":1,"115":1,"116":1},"1":{"113":1,"114":1,"115":2,"116":2},"2":{"29":1,"59":1,"66":1,"98":1,"108":1,"131":1}}],["8×",{"2":{"29":1}}],["随机进入另一镜像网站进行下载尝试",{"2":{"101":1}}],["随机初始化提示",{"2":{"41":1}}],["随机iou阈值在0",{"2":{"29":1}}],["随着无标签图像数量增加",{"2":{"67":1}}],["随着卷积神经网络和基于transformer方法的发展",{"2":{"63":1}}],["随着传感器技术发展",{"2":{"55":1}}],["随着分辨率增加",{"2":{"29":1}}],["随着相机和显示设备的快速发展",{"2":{"29":1}}],["扰动掩码是在真实掩码上随机扰动得到",{"2":{"29":1}}],["总共有ncl个不同的分类",{"2":{"128":1}}],["总损失函数l是标准的二元交叉熵损失和dice损失",{"2":{"35":1}}],["总推理时间也不到",{"2":{"29":1}}],["总推理时间不到cascadepsp的一半",{"2":{"29":1}}],["总步数45",{"2":{"29":1}}],["学习率为2",{"2":{"29":1}}],["训练时剪裁图像",{"2":{"70":1}}],["训练集",{"2":{"70":1}}],["训练资源限制",{"2":{"29":1}}],["训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块",{"2":{"29":1}}],["训练设置",{"2":{"29":1}}],["训练数据集",{"2":{"29":1}}],["还有基于自注意力机制和transformer的网络被应用",{"2":{"39":1}}],["还能提升现有全景分割模型的性能",{"2":{"29":1}}],["还能提升现有全景分割模型性能",{"2":{"29":1}}],["还在重新标注的pascal",{"2":{"29":1}}],["进入设置页面",{"2":{"105":1}}],["进入代码设置页面",{"2":{"105":1}}],["进入镜像列表",{"2":{"101":1}}],["进入",{"2":{"101":1}}],["进一步增强了背景和前景的区分度",{"2":{"68":1}}],["进一步证明了该方法的优越性",{"2":{"48":1}}],["进一步提高分割性能",{"2":{"14":1}}],["进行分屏",{"2":{"117":1}}],["进行文件内容查看",{"2":{"111":1}}],["进行解压",{"2":{"110":1}}],["进行编译的速度比",{"2":{"108":1}}],["进行查看",{"2":{"104":1}}],["进行",{"2":{"103":1}}],["进行等待即可",{"2":{"101":1}}],["进行安装",{"2":{"101":1,"103":1,"104":1}}],["进行重新点击",{"2":{"101":1}}],["进行亲和性学习",{"2":{"47":1}}],["进行差异化处理",{"2":{"44":1}}],["进行超高清图像评估",{"2":{"29":1}}],["测试文件编译",{"0":{"111":1}}],["测试所用的",{"2":{"110":1}}],["测试时直接放缩图像到",{"2":{"70":1}}],["测试时自微调策略",{"2":{"11":1}}],["测试集",{"2":{"70":1}}],["测试数据集",{"2":{"29":1}}],["包含以下创新",{"2":{"93":1}}],["包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像",{"2":{"90":1}}],["包含",{"2":{"70":2}}],["包含大约",{"2":{"70":1}}],["包含2975张训练图像和500张验证图像",{"2":{"39":1}}],["包含36",{"2":{"29":1}}],["包括标准fss",{"2":{"41":1}}],["包括交并比",{"2":{"29":1}}],["包括采用自对齐模块",{"2":{"24":1}}],["遵循cascadepsp的设置",{"2":{"29":1}}],["无论何时",{"2":{"108":1}}],["无论是否编译成功",{"2":{"108":1}}],["无论是定性可视化结果还是定量评估指标",{"2":{"62":1}}],["无引导查询原型的无标签分支性能比基线更差",{"2":{"67":1}}],["无标签增强和多级原型策略共同作用时",{"2":{"68":1}}],["无标签图像数量设置为2时效果最佳",{"2":{"67":1}}],["无标签分支设计选择",{"2":{"67":1}}],["无标签分支的积极影响减小",{"2":{"66":1}}],["无标签分支与查询分支共享参数",{"2":{"65":1}}],["无标签数据与有标签图像的比例从2降至0",{"2":{"66":1}}],["无标签数据特征增强",{"2":{"65":1}}],["无需进行更改",{"2":{"108":1}}],["无需在意",{"2":{"104":1}}],["无需理会",{"2":{"101":1}}],["无需额外无标注数据支持且不增加计算负担",{"2":{"62":1}}],["无需微调",{"2":{"29":1}}],["无法充分利用预训练模型知识获得通用表示",{"2":{"55":1}}],["无法在输出掩码上构建细粒度细节",{"2":{"29":1}}],["利用大规模无标签数据进行预训练以减少对大量标注样本的依赖成为潜在解决方案",{"2":{"94":1}}],["利用这些原型增强初始特征",{"2":{"88":1}}],["利用涂鸦标签进行监督",{"2":{"85":1}}],["利用空间金字塔网络或注意力模块等提取上下文信息",{"2":{"70":1}}],["利用自监督学习进行密集预测预训练",{"2":{"95":1}}],["利用自蒸馏知识恢复局部信息",{"2":{"50":1}}],["利用自注意力机制中固有的长距离依赖关系",{"2":{"35":1}}],["利用光照中的语义线索实现更精确预测",{"2":{"39":1}}],["利用光照组件作为线索",{"2":{"39":1}}],["利用连续位置信息和特征对齐",{"2":{"29":1}}],["利用查询特征的内在引导",{"2":{"21":1}}],["利用查询特征的语义和空间感知",{"2":{"19":1}}],["隐式函数表示",{"2":{"29":1}}],["级联解码器方法能取得较好效果",{"2":{"29":1}}],["传播方法有计算和内存限制",{"2":{"29":1}}],["传统语义分割方法主要有基于fcn和基于transformer的方法",{"2":{"70":1}}],["传统的方法只能分割训练集的种类",{"2":{"70":1}}],["传统的深度白化变换",{"2":{"35":1}}],["传统方法存在语义模糊和类间相似性问题",{"2":{"68":1}}],["传统方法常受限于语义模糊性和类间相似性",{"2":{"62":1}}],["传统方法聚焦提升模型跨多未见领域的预测准确性",{"2":{"27":1}}],["传统dgss方法的局限",{"2":{"27":1}}],["针对这一局限性",{"2":{"82":1}}],["针对这一矛盾",{"2":{"32":1}}],["针对150个类别的miou得分为31",{"2":{"70":1}}],["针对不确定区域",{"2":{"44":1}}],["针对",{"2":{"29":1}}],["丢失了细节并破坏了全局上下文",{"2":{"29":1}}],["增加到三个块时性能下降",{"2":{"98":1}}],["增加基线的训练迭代次数对性能影响不大",{"2":{"67":1}}],["增加后续解码器分割新类别的负担",{"2":{"41":1}}],["增加了成本",{"2":{"29":1}}],["增强特征挖掘能力",{"2":{"65":1}}],["增强前景语义一致性",{"2":{"63":1}}],["增强前景特征",{"2":{"18":1,"21":1}}],["增强不确定特征和确定特征对语义分割都很重要",{"2":{"50":1}}],["增强了全局理解",{"2":{"49":1}}],["增强了通道表示能力并减少通道冗余",{"2":{"38":1}}],["增强模型的鲁棒性",{"2":{"19":1}}],["计算效率低",{"2":{"70":1}}],["计算每个注意力头在transformer层上的平均注意力熵",{"2":{"49":1}}],["计算机视觉领域也在探索基础模型",{"2":{"90":1}}],["计算机视觉领域虽也对基础模型有所探索",{"2":{"90":1}}],["计算机视觉领域尝试引入可学习参数激活语义知识",{"2":{"41":1}}],["计算机视觉中领域泛化分割多聚焦驾驶场景",{"2":{"34":1}}],["计算机视觉和机器学习领域对领域泛化广泛研究",{"2":{"34":1}}],["计算成本高",{"2":{"29":1}}],["计算查询特定的转换矩阵有助于防止过拟合",{"2":{"8":1}}],["超高清图像",{"2":{"29":1}}],["超高分辨率图像也给经典图像分割方法带来了挑战",{"2":{"29":1}}],["超越现有方法",{"2":{"27":1}}],["工业缺陷检测等众多视觉任务中应用广泛",{"2":{"63":1}}],["工业缺陷检测",{"2":{"29":1}}],["逐步聚合特征",{"2":{"29":1}}],["连续细化模型",{"2":{"29":1}}],["往往会错误地激活与目标类别无关的物体",{"2":{"41":1}}],["往往难以很好地平衡准确性和计算成本",{"2":{"29":1}}],["往往对背景噪声过于敏感",{"2":{"18":1}}],["等等",{"2":{"126":1}}],["等功能",{"2":{"110":1}}],["等待安装完成",{"2":{"103":1,"104":1}}],["等会儿会消失",{"2":{"101":1}}],["等数据集上",{"2":{"93":1}}],["等指标",{"2":{"29":1}}],["等",{"0":{"29":1},"2":{"29":1}}],["等方法缩小域差距",{"2":{"10":1}}],["香港中文大学",{"0":{"29":1}}],["quot",{"2":{"101":1,"102":1,"103":6,"104":6,"116":4}}],["queries",{"0":{"30":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"31":1}}],["query",{"2":{"7":6,"17":1,"31":1}}],["qualitative",{"2":{"61":1}}],["quality",{"0":{"28":1}}],["quantitative",{"2":{"29":1,"61":1}}],["写作启发",{"2":{"27":1,"90":1}}],["证明该方法的有效性源于学习到的判别性和语义特征",{"2":{"67":1}}],["证明其能够自适应地生成不同的部分掩码",{"2":{"41":1}}],["证明了mevt能够学习到具有细粒度和全局上下文感知能力的视觉表示",{"2":{"96":1}}],["证明了其在少样本语义分割任务中的有效性",{"2":{"65":1}}],["证明了该框架的优越性和泛化性",{"2":{"59":1}}],["证明了gopt已经达到了sota的标准",{"2":{"59":1}}],["证明了基于transformer的单阶段训练的有效性",{"2":{"50":1}}],["证明了强化不确定特征的有效性",{"2":{"49":1}}],["证明了动态类别感知提示范式对少样本分割",{"2":{"41":1}}],["证明了vfms在dgss领域的巨大潜力以及rein方法的有效性",{"2":{"27":1}}],["证明了优化类别原型和查询特征的必要性",{"2":{"24":1}}],["证实其强大泛化能力",{"2":{"27":1}}],["领域展现出强大的零样本和少样本泛化能力",{"2":{"90":1}}],["领域和未见过类别上的泛化能力仍然有限",{"2":{"73":1}}],["领域",{"2":{"41":1,"45":1}}],["领域泛化研究",{"2":{"34":1}}],["领域泛化语义分割",{"2":{"27":2}}],["领域偏移使深度学习模型同一通道中不同领域的医学图像激活模式差异大",{"2":{"33":1}}],["领域广义语义分割",{"2":{"27":1}}],["大特征图",{"2":{"130":1}}],["大语言模型在自然语言处理",{"2":{"90":1}}],["大小",{"2":{"70":1,"112":1}}],["大多数fss方法直接使用预训练编码器",{"2":{"41":1}}],["大多数少样本分割",{"2":{"41":1}}],["大多数现有的方法尝试使用预训练的视觉",{"2":{"70":1}}],["大多数现有的语义分割方法都是为白天场景设计的",{"2":{"39":1}}],["大多数现有方法通过插值对最终预测进行4到8倍上采样",{"2":{"29":1}}],["大多现存的语义分割方法都是基于白天场景开发的",{"2":{"39":1}}],["大模型易过拟合而浅细化网络细化能力有限等问题",{"2":{"29":1}}],["大量下游任务实验表明",{"2":{"59":1}}],["大量实验验证了该方案的先进性",{"2":{"32":1}}],["大量实验表明",{"2":{"14":1,"38":1,"39":1}}],["大量的输入像素在计算上代价高昂",{"2":{"29":1}}],["大幅超越现有方法",{"2":{"27":1}}],["冻结的vfms性能优于先前dgss方法",{"2":{"27":1}}],["得出以下结论",{"2":{"70":1,"88":1,"99":1}}],["得出结论",{"2":{"27":1}}],["得到最终的逐像素分割结果",{"2":{"65":1}}],["得到增强的查询特征",{"2":{"65":1}}],["得到基于空间分布概率的掩码用于精确的定位",{"2":{"21":1}}],["批量大小等进行训练",{"2":{"27":1}}],["迭代次数",{"2":{"27":1}}],["方法清晰",{"2":{"90":1}}],["方法构成",{"2":{"70":1}}],["方法设计有效",{"2":{"50":1}}],["方法在验证集和测试集上的miou分别达到75",{"2":{"48":1}}],["方法不仅超越了当前最先进的同类方案",{"2":{"44":1}}],["方法通常会直接使用预训练的编码器",{"2":{"41":1}}],["方法优势",{"2":{"39":1}}],["方法创新",{"2":{"38":1,"99":1}}],["方法的性能",{"2":{"27":1}}],["方法",{"2":{"27":1,"35":1,"41":1}}],["设计segment",{"2":{"90":1}}],["设计相应的模型架构",{"2":{"90":1}}],["设计了类感知单模态提示器",{"2":{"59":1}}],["设计了基于激活的掩码策略",{"2":{"50":1}}],["设计语义提示转移",{"2":{"41":1}}],["设计语义导向解纠缠框架",{"2":{"39":1}}],["设计优化策略",{"2":{"27":1}}],["设计目的",{"2":{"21":3}}],["设置vscode内部查看生成的pdf文件",{"2":{"119":1}}],["设置为onfaild",{"2":{"119":1}}],["设置为true时",{"2":{"108":1}}],["设置是否自动编译",{"2":{"119":1}}],["设置其位置参数",{"2":{"116":1}}],["设置外部查看器启动文件sumatrapdf",{"2":{"116":1}}],["设置快捷键步骤如下",{"2":{"111":1}}],["设置边距",{"2":{"110":1}}],["设置页面交互能力较强",{"2":{"105":1}}],["设置页面和代码设置页面均为设置页面",{"2":{"105":1}}],["设置页面",{"2":{"105":1}}],["设置特定学习率",{"2":{"27":1}}],["设置",{"2":{"27":1}}],["因缺乏大规模多模态训练集",{"2":{"56":1}}],["因为网络的设计未必那么完美",{"2":{"125":1}}],["因为如果涉及到",{"2":{"108":1}}],["因为毕竟涉及到代码",{"2":{"108":1}}],["因为这些错误和警告信息能够从终端中获取",{"2":{"108":1}}],["因为它可以通过新的",{"2":{"108":1}}],["因为sam更侧重于通用性和广泛适用性",{"2":{"90":1}}],["因为强增强可能会干扰分割目标",{"2":{"49":1}}],["因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战",{"2":{"24":1}}],["因此它可以零样本迁移到不同的图像类型和任务",{"2":{"90":1}}],["因此弱监督学习方法受关注",{"2":{"84":1}}],["因此备受研究人员推崇",{"2":{"82":1}}],["因此在夜间场景中往往表现不佳",{"2":{"39":1}}],["因此",{"2":{"27":1,"29":1,"41":2,"63":1,"83":1,"90":1,"94":1}}],["先按照一定的比例通过补",{"2":{"130":1}}],["先给出效果图",{"2":{"107":1}}],["先利用分层编码器生成像素级图像",{"2":{"70":1}}],["先分离再解析",{"2":{"39":1}}],["先解耦再解析",{"2":{"39":1}}],["先前dgss方法多采用传统骨干网络",{"2":{"27":1}}],["先以支持原型为参考在查询特征中选择匹配置信度高的点",{"2":{"21":1}}],["具有19个语义类别",{"2":{"39":1}}],["具有显著的泛化潜力",{"2":{"29":1}}],["具有显著的跨场景泛化能力",{"2":{"27":1}}],["具体的说",{"2":{"130":1}}],["具体的安装指标已在下图标明",{"2":{"101":1}}],["具体见下图",{"2":{"118":1}}],["具体看下图",{"2":{"108":1}}],["具体安装过程与常见的软件安装过程一致",{"2":{"102":1}}],["具体通过定义可提示的分割任务",{"2":{"90":1}}],["具体而言",{"2":{"59":1}}],["具体结论如下",{"2":{"50":1}}],["具体来说",{"2":{"39":1}}],["具体如下",{"2":{"27":1}}],["具体方法",{"2":{"21":3}}],["采用",{"2":{"93":1}}],["采用补丁令牌对比",{"2":{"47":1}}],["采用编码器",{"2":{"47":1}}],["采用无监督域适应技术将白天知识迁移到夜间",{"2":{"39":1}}],["采用层共享mlp权重和低秩token序列",{"2":{"27":1}}],["采用adamw优化器",{"2":{"27":1}}],["采用复杂数据增强和领域不变特征提取策略",{"2":{"27":1}}],["采用多尺度自适应局部注意力增强前景信息",{"2":{"21":1}}],["常用的上采样包括转置卷积",{"2":{"130":1}}],["常用数据集规模远小于imagenet",{"2":{"27":1}}],["常见的分割模型",{"0":{"126":1}}],["常见的策略",{"2":{"29":1}}],["常提供无效的跨模态融合",{"2":{"55":1}}],["常采用元学习",{"2":{"9":1}}],["其插入的",{"2":{"130":1}}],["其过程如下图所示",{"2":{"130":1}}],["其操作步骤与内嵌输出",{"2":{"117":1}}],["其安装很简单",{"2":{"113":1}}],["其变量有",{"2":{"108":1}}],["其生成的",{"2":{"108":1}}],["其内部编译命令来自上文latex",{"2":{"108":1}}],["其功能是一样的",{"2":{"105":1}}],["其余如图所示",{"2":{"102":1}}],["其余均为原创内容",{"2":{"100":1}}],["其重要性自不必多说",{"2":{"100":1}}],["其研究背景主要源于自然语言处理",{"2":{"90":1}}],["其研究背景主要源于当前fss方法存在的局限性以及人类视觉感知模式带来的启示",{"2":{"41":1}}],["其核心原理是通过将已标注像素的信息传递至相邻未标注区域",{"2":{"82":1}}],["其核心难点在于域偏移问题",{"2":{"32":1}}],["其前向传播过程包含三个主要模块",{"2":{"65":1}}],["其次是对齐引导的跨模态提示器",{"2":{"54":1}}],["其性能表现甚至优于那些通过增加模型复杂度来提升准确率的多阶段方法",{"2":{"44":1}}],["其他fss方法的性能有显著提升",{"2":{"41":1}}],["其互补数据集为bdd100k",{"2":{"39":1}}],["其dice相似系数",{"2":{"32":1}}],["其多视图一致性和光滑性有利于分割",{"2":{"29":1}}],["其在不同未知场景下展现出强大泛化能力",{"2":{"27":1}}],["其中侧边栏所展现的就是上文提及的新的",{"2":{"111":1}}],["其中name是标签",{"2":{"108":1}}],["其中的name为这些命令的标签",{"2":{"108":1}}],["其中多出来的第一个选项为进行tex文件的编译",{"2":{"108":1}}],["其中风格解耦查询的贡献更大",{"2":{"37":1}}],["其中λ设置为1×10−4",{"2":{"35":1}}],["其中",{"2":{"18":1,"45":1,"84":1,"128":1}}],["对这些小区域做分类",{"2":{"127":1}}],["对字体的支持更好",{"2":{"108":1}}],["对应的",{"2":{"108":1}}],["对全局平均池化",{"2":{"96":1}}],["对局部原型增强和全局原型增强方法进行消融实验",{"2":{"87":1}}],["对初始预测图和增强预测图进行约束",{"2":{"85":1}}],["对齐诱导跨模态提示器",{"2":{"57":1}}],["对辅助模态的视觉概念进行分层渐进分组",{"2":{"57":1}}],["对最大的夜间分割数据集nightcity进行细化",{"2":{"39":1}}],["对训练集和验证集中不合理的标注进行了仔细修改",{"2":{"39":1}}],["对dfq的理解",{"2":{"36":1}}],["对dcam中的关键组件进行了全面分析",{"2":{"24":1}}],["对比学习在下游视觉任务表现良好",{"2":{"95":1}}],["对比实验+消融实验",{"2":{"70":1}}],["对比实验",{"2":{"29":1}}],["对比方法",{"2":{"29":1}}],["对比rein与现有dgss和参数高效微调",{"2":{"27":1}}],["对于反卷积而言",{"2":{"130":1}}],["对于普通的",{"2":{"130":1}}],["对于理工科",{"2":{"100":1}}],["对于标注像素级别的样本需要大量的人力和财力",{"2":{"83":1}}],["对于低分辨率的输入",{"2":{"70":1}}],["对于前景对象",{"2":{"63":1}}],["对于少样本语义分割任务",{"2":{"62":1}}],["对于",{"2":{"29":1,"125":1}}],["对4k或6k超高分辨率图像进行分割时",{"2":{"29":1}}],["对vfms大量可训练参数进行微调会导致泛化能力受限",{"2":{"27":1}}],["对更强的vfms在dgss中的效能探索不足",{"2":{"27":1}}],["本文转载自https",{"2":{"119":1}}],["本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图",{"2":{"100":1}}],["本文作者探索一种适用于语义分割的自监督预训练方法",{"2":{"94":1}}],["本文旨在开发一个可提示的模型",{"2":{"90":1}}],["本文聚焦于构建图像分割基础模型",{"2":{"90":1}}],["本文聚焦于弱监督语义分割",{"2":{"45":1}}],["本文聚焦于",{"2":{"41":1}}],["本文聚焦于在领域泛化语义分割",{"2":{"27":1}}],["本文重点介绍了三项关键技术",{"2":{"41":1}}],["本文模仿人类的视觉感知方式",{"2":{"41":1}}],["本文的sed方法都表现出较好的效果",{"2":{"70":1}}],["本文的研究目标",{"2":{"45":1}}],["本文的研究背景",{"2":{"27":1}}],["本文的创新点",{"2":{"27":1}}],["本文提出基于原型",{"2":{"82":1}}],["本文提出相关内在特征增强网络",{"2":{"63":1}}],["本文提出了相关内在特征增强网络",{"2":{"65":1}}],["本文提出了分组提示调优框架",{"2":{"57":1}}],["本文提出了一种用于语义分割的自监督预训练框架",{"2":{"96":1}}],["本文提出了一种简单的编码器",{"2":{"70":1}}],["本文提出了一种全新的夜间语义分割方法",{"2":{"39":1}}],["本文提出了一种基于提示学习的",{"2":{"41":1}}],["本文提出了一种基于transformer的自适应原型匹配网络",{"2":{"21":1}}],["本文提出了一种基于双重匹配变换的网络",{"2":{"11":1}}],["本文提出了一种基于双重匹配转换的网络",{"2":{"8":1}}],["本文提出一种渐进式特征自我强化方法",{"2":{"45":1}}],["本文提出一种基于双重匹配变换的网络",{"2":{"9":1}}],["​",{"2":{"27":1,"39":1,"41":4}}],["代码解读",{"0":{"116":1}}],["代码展示",{"0":{"115":1}}],["代码进行编译",{"2":{"107":1}}],["代码不算哈哈哈",{"2":{"100":1}}],["代码已开源",{"2":{"73":1}}],["代码已发布",{"2":{"27":1}}],["代替多层感知器",{"2":{"24":1}}],["的测试函数",{"2":{"131":1}}],["的数量为stride−1",{"2":{"130":1}}],["的层数为",{"2":{"130":1}}],["的卷积",{"2":{"130":1}}],["的概念与目标检测中的",{"2":{"128":1}}],["的使用",{"0":{"117":1}}],["的窗口位置",{"2":{"112":1}}],["的内部查看器的快捷键绑定",{"2":{"108":1}}],["的标准字体",{"2":{"108":1}}],["的引用",{"2":{"108":1}}],["的",{"2":{"107":1,"108":1,"116":1,"131":1}}],["的快捷方式",{"2":{"102":1}}],["的编辑器",{"2":{"101":1}}],["的下载与安装说明",{"2":{"101":1}}],["的区别",{"2":{"101":1}}],["的性能提升",{"2":{"93":1}}],["的线性探测任务中实现了",{"2":{"93":1}}],["的统计特性来强化涂鸦监督效果",{"2":{"82":1}}],["的特征增强方法",{"2":{"82":1}}],["的训练集",{"2":{"70":1}}],["的模型参数",{"2":{"57":1,"59":1}}],["的参数",{"2":{"54":1}}],["的共性特征",{"2":{"54":1}}],["的熵更高且更集中",{"2":{"49":1}}],["的消融结果",{"2":{"49":1}}],["的整体准确性",{"2":{"44":1}}],["的有效性",{"2":{"41":1}}],["的平均交并比",{"2":{"41":1,"73":1}}],["的平均iou",{"2":{"12":2}}],["的夜间语义分割范式",{"2":{"39":1}}],["的优势显著超越次优方法",{"2":{"32":1}}],["的配置",{"2":{"29":2,"107":1}}],["的一半",{"2":{"29":1}}],["的miou提升",{"2":{"41":3}}],["的miou",{"2":{"27":1}}],["的可训练参数",{"2":{"27":1}}],["仅训练不到1",{"2":{"59":1}}],["仅更新分组提示器和分割头的梯度值",{"2":{"57":1}}],["仅微调少量视觉提示参数",{"2":{"57":1}}],["仅微调解码器",{"2":{"41":1}}],["仅需微调模型不足",{"2":{"54":1}}],["仅将支持图像或查询图像的目标语义转移到提示中会导致不同程度的性能下降",{"2":{"41":1}}],["仅使用图像级标签的弱监督语义分割",{"2":{"44":1}}],["仅使用语义分割标注训练就能生成类似抠图的结果",{"2":{"29":1}}],["仅使用基于空间分布概率的分割图时",{"2":{"24":1}}],["仅使用基于语义相似度的掩码可使模型性能提升1",{"2":{"24":1}}],["仅在冻结的骨干网络中增加1",{"2":{"27":1}}],["值得一提的是",{"2":{"27":1,"62":1}}],["能否编译参考文献",{"2":{"110":1}}],["能否编译目录",{"2":{"110":1}}],["能否进行引用",{"2":{"110":1}}],["能否插入图片",{"2":{"110":1}}],["能省很多麻烦",{"2":{"102":1}}],["能帮助网络学习通用视觉表示",{"2":{"94":1}}],["能作为预训练目标",{"2":{"90":1}}],["能以正确分类像素的原型引导错误分类像素的分类",{"2":{"88":1}}],["能显著减少标注工作量",{"2":{"83":1}}],["能提前拒绝不存在的类别",{"2":{"70":1}}],["能有效估计边界",{"2":{"50":1}}],["能连续对齐特征图与细化目标",{"2":{"29":1}}],["能生成更多细节",{"2":{"29":1}}],["能够在拥有这些优势的同时",{"2":{"112":1}}],["能够在",{"2":{"108":1}}],["能够针对不同任务激活相应的类别对象",{"2":{"41":1}}],["能够调整编码器专注于当前任务中的目标类别",{"2":{"41":1}}],["能够直接用于现有的白天分割方法",{"2":{"39":1}}],["能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距",{"2":{"29":1}}],["能够精确地细化并将特征图从每一层传递到骨干网络的下一层",{"2":{"27":1}}],["能应对类内差异的敏感性",{"2":{"24":1}}],["获得更好的泛化能力",{"2":{"27":1}}],["获取",{"2":{"8":1}}],["4和stage",{"2":{"96":1}}],["4和5",{"2":{"96":1}}],["4xa6000",{"2":{"70":1}}],["459",{"2":{"70":1}}],["4效果最佳",{"2":{"49":1}}],["44",{"2":{"41":1}}],["49",{"2":{"36":1}}],["4×或",{"2":{"29":1}}],["4k和6k分辨率变得常见",{"2":{"29":1}}],["4k",{"2":{"29":2}}],["4",{"0":{"104":1,"127":1,"137":1,"145":1,"146":1,"147":1,"148":1},"1":{"146":1,"147":1,"148":1},"2":{"27":2,"41":1,"59":2,"66":1,"70":1,"87":1,"97":2,"105":1,"131":1,"157":1}}],["缓解空间感知偏差",{"2":{"25":1}}],["缓解骨干网络的固有偏差",{"2":{"21":1}}],["中",{"2":{"108":1}}],["中的范围",{"2":{"131":1}}],["中的",{"2":{"108":1}}],["中的轻量级卷积解码器",{"2":{"47":1}}],["中文语言环境配置",{"0":{"103":1}}],["中进行泛化",{"2":{"41":1}}],["中科院",{"0":{"41":1}}],["中评估多种vfms",{"2":{"27":1}}],["中利用视觉基础模型",{"2":{"27":1}}],["中国科学技术大学",{"0":{"27":1}}],["中国科学院",{"0":{"16":1}}],["中固有偏差",{"2":{"25":1}}],["性能优越",{"2":{"99":1}}],["性能优势",{"2":{"29":1}}],["性能在预期范围内波动",{"2":{"49":1}}],["性能进一步提升",{"2":{"49":1}}],["性能验证",{"2":{"39":1}}],["性能表现优异",{"2":{"50":1}}],["性能表现",{"2":{"38":1,"96":1}}],["性能随着采样的缩放比例数量增加而提升",{"2":{"29":1}}],["性能下降2",{"2":{"24":1}}],["性能会下降",{"2":{"10":1}}],["且同样支持双向同步",{"2":{"117":1}}],["且根据需要关闭标签",{"2":{"117":1}}],["且根据笔者使用来看",{"2":{"112":1}}],["且侧面带有书签",{"2":{"117":1}}],["且支持双向同步功能",{"2":{"112":1}}],["且需要为英文路径",{"2":{"108":1}}],["且弹窗弹出比较烦人",{"2":{"108":1}}],["且代码设置可以直接克隆别人的代码到自己的编辑器中",{"2":{"105":1}}],["且所有引用在文中或文末注明了来源",{"2":{"100":1}}],["且",{"2":{"100":1}}],["且要通过密集的自监督信号实现上述两点",{"2":{"94":1}}],["且生成的边界不如一些计算密集型的",{"2":{"90":1}}],["且现有模型在泛化能力和处理模糊提示方面存在不足",{"2":{"90":1}}],["且现有方法难以实现强大的泛化能力",{"2":{"90":1}}],["且现有的参数高效微调策略大多不适用于dgss",{"2":{"27":1}}],["且性能随模型规模",{"2":{"90":1}}],["且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用",{"2":{"83":1}}],["且比点",{"2":{"83":1}}],["且因样本标注有限",{"2":{"55":1}}],["且当基于cam的选择不那么严格时",{"2":{"49":1}}],["且可扩展到跨领域",{"2":{"41":1}}],["且这一问题未得到实质性解决",{"2":{"41":1}}],["且弱监督",{"2":{"34":1}}],["且flops和参数更少",{"2":{"29":1}}],["且对gpu内存需求大",{"2":{"29":1}}],["且依赖复杂数据增强和领域不变特征提取策略",{"2":{"27":1}}],["且前馈网络在略微增加计算成本的情况下保留了更多特征细节",{"2":{"24":1}}],["且参数数量较少",{"2":{"21":1}}],["结合unc",{"2":{"49":1}}],["结合部分掩码生成器",{"2":{"41":1}}],["结合语义提示转移",{"2":{"41":1}}],["结合编码器",{"2":{"39":1}}],["结果略有下降",{"2":{"49":1}}],["结果显示",{"2":{"24":1,"37":1}}],["结果表明gopt仅训练不到1",{"2":{"57":1}}],["结果表明crm性能更好",{"2":{"29":1}}],["结果表明",{"2":{"24":1,"87":1}}],["结论与不足",{"2":{"70":1}}],["结论",{"0":{"14":1,"25":1,"38":1,"50":1,"68":1,"79":1,"88":1,"99":1},"2":{"29":1,"39":1,"59":1}}],["作为外部查看器",{"2":{"112":1}}],["作为",{"2":{"101":1}}],["作为一种强大的排版系统",{"2":{"100":1}}],["作为伪标签",{"2":{"46":1}}],["作为种子区域",{"2":{"45":1}}],["作为前馈网络",{"2":{"24":1}}],["作者希望该简单框架能推动无标签或少量标签语义分割的广泛应用",{"2":{"99":1}}],["作者希望这项工作能为小样本场景下的编码器设计提供新视角",{"2":{"41":1}}],["作者在摒弃以往冻结编码器以泛化到未见类别的小样本分割",{"2":{"41":1}}],["作者认为理想的fss特征编码器应具有类别感知能力",{"2":{"41":1}}],["作者引入rein微调方法",{"2":{"27":1}}],["作者评估并利用vfms进行dgss研究",{"2":{"27":1}}],["作者提出基于原型的特征增强方法",{"2":{"83":1}}],["作者提出了相关内在特征增强网络",{"2":{"68":1}}],["作者提出了用于开放词汇语义分割的sed方法",{"2":{"70":1}}],["作者提出了用于多模态图像分割的参数高效视觉调优框架gopt",{"2":{"59":1}}],["作者提出了用于超高清图像分割细化的连续细化模型",{"2":{"29":1}}],["作者提出了一种自监督预训练方法",{"2":{"99":1}}],["作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割",{"2":{"88":1}}],["作者提出了一种基于语义不确定性引导的弱监督语义分割方法",{"2":{"50":1}}],["作者提出了一种基于transformer的自适应原型匹配网络",{"2":{"25":1}}],["作者提出了一种新颖的夜间语义分割范式",{"2":{"39":1}}],["作者提出了nightcity",{"2":{"39":1}}],["作者提出了连续细化模型",{"2":{"29":1}}],["作者提出评估vfms在dgss中的性能以及如何有效利用vfms的问题",{"2":{"27":1}}],["作者提出一种基于transformer的自适应原型匹配网络",{"2":{"19":1}}],["作者提出用于跨域少样本语义分割的dmtnet",{"2":{"14":1}}],["+outputpadding",{"2":{"130":2}}],["+kernelsize",{"2":{"130":2}}],["+点击",{"2":{"119":1}}],["+2",{"2":{"98":1}}],["+4",{"2":{"98":1}}],["+1",{"2":{"92":1}}],["+",{"2":{"24":2,"48":1,"98":1,"101":1,"105":1,"131":1}}],["多尺度融合",{"2":{"98":1}}],["多尺度解码器",{"2":{"96":1}}],["多尺度自适应局部注意力",{"2":{"24":1}}],["多样化的分割数据集",{"2":{"90":1}}],["多级别原型设计选择",{"2":{"67":1}}],["多级原型交互",{"2":{"65":1}}],["多级原型交互模块",{"2":{"65":1}}],["多级原型处理",{"2":{"65":1}}],["多级原型生成模块",{"2":{"65":1}}],["多模态图像分割主流方法",{"2":{"56":1}}],["多模态图像分割技术是计算机视觉领域的关键挑战",{"2":{"54":1}}],["多模态方法常采用基于rgb的预训练分割器",{"2":{"55":1}}],["多模态融合用于分割成为图像解释核心问题",{"2":{"55":1}}],["多模态融合的重要性",{"2":{"55":1}}],["多阶段方法先通过分类模型生成类激活图",{"2":{"46":1}}],["多数自监督学习方法在语义分割任务中表现欠佳",{"2":{"94":1}}],["多数语义分割数据集规模远小于分类数据集",{"2":{"94":1}}],["多数采用预训练编码器并微调解码器",{"2":{"41":1}}],["多数方法遵循元学习范式",{"2":{"41":1}}],["多数现有医学图像分割方法假定训练和测试样本遵循相同统计分布",{"2":{"33":1}}],["多数现有cd",{"2":{"9":1}}],["多使用vggnet",{"2":{"27":1}}],["实例分割模型",{"2":{"126":1}}],["实例分割",{"2":{"124":1}}],["实例分割等",{"2":{"90":1}}],["实例分割等下游任务中带来显著性能提升",{"2":{"73":1}}],["实时计算且能处理歧义",{"2":{"90":1}}],["实现",{"2":{"130":1}}],["实现零样本泛化",{"2":{"90":1}}],["实现最多4",{"2":{"70":1}}],["实现更精确的分割",{"2":{"41":1}}],["实现更精准预测",{"2":{"39":1}}],["实现类别感知增强",{"2":{"41":1}}],["实现细节",{"2":{"29":1}}],["实现了精确分割",{"2":{"24":1,"25":1}}],["实验发现",{"2":{"87":1}}],["实验结果",{"2":{"70":1}}],["实验结果表明",{"2":{"21":1,"41":1,"65":1,"88":1}}],["实验设定",{"2":{"70":1}}],["实验证明了为无标签分支添加引导等操作的合理性和可靠性",{"2":{"67":1}}],["实验过程",{"0":{"58":1,"59":1,"86":1,"87":1}}],["实验验证",{"2":{"57":1}}],["实验数据显示",{"2":{"54":1}}],["实验步骤",{"2":{"29":1}}],["实验表明其在超高清图像上的分割效果最佳",{"2":{"29":1}}],["实验表明",{"2":{"25":1,"27":1,"29":1,"37":1,"62":1,"68":1,"82":1,"93":1}}],["实验",{"0":{"12":1,"13":1,"22":1,"36":1,"37":1,"48":1,"49":1,"66":1,"67":1,"77":1,"78":1,"97":1,"98":1},"1":{"23":1,"24":1},"2":{"39":1,"41":1}}],["减少对大量高质量标注数据的依赖",{"2":{"99":1}}],["减少通道冗余",{"2":{"35":1,"36":1}}],["减少参数冗余",{"2":{"27":1}}],["减少了背景干扰",{"2":{"24":1}}],["减轻背景干扰",{"2":{"21":1}}],["减轻fss中的背景干扰",{"2":{"19":1}}],["模仿人类视觉感知模式",{"2":{"41":1}}],["模型的输入是什么",{"2":{"125":1}}],["模型的输入和输出",{"0":{"125":1}}],["模型优势",{"2":{"96":1}}],["模型架构",{"2":{"96":1}}],["模型创新",{"2":{"90":1}}],["模型和数据集",{"2":{"90":1}}],["模型在",{"2":{"73":1}}],["模型在识别近义词类别时存在困难",{"2":{"70":1}}],["模型有时难以区分近义词类别",{"2":{"70":1}}],["模型设定",{"2":{"70":1}}],["模型分割效果提升",{"2":{"67":1}}],["模型训练方式",{"2":{"56":1}}],["模型实现",{"2":{"29":1}}],["模型易过拟合或细化能力有限等问题",{"2":{"29":1}}],["模型结构图",{"2":{"27":1}}],["模型整体比基线提升了3",{"2":{"24":1}}],["模块",{"2":{"8":1}}],["36",{"2":{"41":1}}],["38",{"2":{"36":1,"41":1}}],["31",{"2":{"31":1,"32":1,"38":1,"70":1,"131":1}}],["3",{"0":{"103":1,"126":1,"136":1,"141":1,"142":1,"143":1,"144":1,"148":1},"1":{"143":1,"144":1},"2":{"24":2,"41":3,"47":1,"50":1,"92":1,"93":1,"96":2,"98":1,"100":1,"130":4,"131":31}}],["64",{"2":{"131":1}}],["6的输出处分别应用自蒸馏损失",{"2":{"96":1}}],["65",{"2":{"72":1,"73":1}}],["60",{"2":{"72":1,"73":1}}],["61",{"2":{"41":1}}],["63",{"2":{"36":1}}],["6k",{"2":{"29":3}}],["6",{"0":{"106":1,"107":1,"108":1},"1":{"107":1,"108":1},"2":{"24":1,"59":1,"70":2,"86":1,"88":1,"96":1,"97":1,"107":1,"131":1}}],["68",{"2":{"12":1,"37":1}}],["24",{"2":{"131":2}}],["2编译链",{"2":{"111":1}}],["2和6",{"2":{"96":1}}],["2k",{"2":{"70":1}}],["2k分辨率图像的方法",{"2":{"29":1}}],["27",{"2":{"41":1}}],["26",{"2":{"36":1}}],["25×10⁻⁴",{"2":{"29":1}}],["2",{"0":{"102":1,"108":1,"111":1,"114":1,"115":1,"116":2,"125":1,"135":1,"138":1,"139":1,"140":1,"141":1,"144":1,"147":1},"1":{"115":1,"116":1,"139":1,"140":1,"141":1},"2":{"24":1,"39":2,"41":2,"48":1,"50":1,"72":1,"98":1,"100":1,"107":2,"108":2,"110":2,"119":2,"130":2,"131":1}}],["2020",{"2":{"110":1}}],["200",{"2":{"73":1}}],["20",{"2":{"70":1}}],["20k",{"2":{"70":1}}],["2015",{"2":{"72":1,"73":1}}],["2014基准上验证了有效性",{"2":{"50":1}}],["2014",{"2":{"43":1,"44":1,"48":2}}],["2012验证集上进行比较",{"2":{"86":1}}],["2012和ms",{"2":{"50":1}}],["2012数据集上达到了最先进的性能",{"2":{"88":1}}],["2012数据集上",{"2":{"29":1}}],["2012数据集上进行评估",{"2":{"29":1}}],["2012",{"2":{"12":1,"43":1,"44":1,"48":2,"72":1,"73":1,"81":1,"82":1}}],["20i数据集上分别实现了4",{"2":{"41":1}}],["20i数据集上分别实现了3",{"2":{"41":1}}],["20i数据集上分别实现了2",{"2":{"41":1}}],["20i数据集上分别实现了0",{"2":{"41":1}}],["20i数据集上以最少的参数达到了最先进的性能",{"2":{"25":1}}],["20i数据集上进行了大量实验",{"2":{"18":1}}],["20i两个基准数据集上取得了优于现有方法的性能",{"2":{"21":1}}],["20i",{"2":{"17":1,"23":1}}],["91",{"2":{"41":1}}],["96",{"2":{"41":1}}],["94",{"2":{"37":1}}],["98",{"2":{"31":1,"32":1,"38":1,"72":1}}],["9",{"0":{"117":1},"2":{"24":2,"98":2,"108":1,"131":1}}],["三通道的图像",{"2":{"125":1}}],["三种deit变体在11个块时效果最佳",{"2":{"41":1}}],["三个关键组件组成",{"2":{"37":1}}],["三个主要模块",{"2":{"24":1}}],["三是分类阶段",{"2":{"19":1}}],["该软件的优点在于在具有",{"2":{"112":1}}],["该快捷键为默认设置",{"2":{"111":1}}],["该框架在网络架构和自监督目标方面都有创新",{"2":{"96":1}}],["该框架包含两大创新模块",{"2":{"54":1}}],["该数据集包括10亿个掩码和1100万张图像",{"2":{"90":1}}],["该范式让模型利用少量标注数据学习分割",{"2":{"63":1}}],["该范式模仿人类视觉感知模式",{"2":{"41":1}}],["该无标注分支在测试阶段可完全移除",{"2":{"62":1}}],["该网络引入无标签分支",{"2":{"68":1}}],["该网络创新性地引入无标注分支训练策略",{"2":{"62":1}}],["该网络包含目标增强模块",{"2":{"25":1}}],["该方案构建了一种动态的类别感知提示机制",{"2":{"41":1}}],["该方法因能在显著降低标注成本的同时实现高性能表现",{"2":{"82":1}}],["该方法通过计算语义相似度实现未见过标签的分割",{"2":{"73":1}}],["该方法同样显著优于现有最优方法",{"2":{"36":1}}],["该方法能够以参数高效的方式利用vfm来解决dgss任务",{"2":{"27":1}}],["该方法在神经网络架构和自监督目标方面均有创新",{"2":{"99":1}}],["该方法在前列腺和眼底图像分割基准测试中显著优于现有最先进的方法",{"2":{"38":1}}],["该方法在pascal",{"2":{"25":1,"88":1}}],["该方法在降低计算复杂度的同时保持了较高的准确性",{"2":{"24":1}}],["该方法包含目标增强模块",{"2":{"24":1}}],["该子集包含314张夜间训练图像和31张验证图像",{"2":{"39":1}}],["该模块通过建立全局原型",{"2":{"62":1}}],["该模块能够学习语义与光照之间的关系",{"2":{"39":1}}],["该模型在pascal",{"2":{"21":1}}],["该模型主要包含以下三个模块",{"2":{"21":1}}],["该模型包含三个模块",{"2":{"18":1}}],["由内转外操作相同",{"2":{"118":1}}],["由编辑器根据情况自动设置",{"2":{"116":1}}],["由图像编码器",{"2":{"90":1}}],["由图像级类标签监督",{"2":{"47":1}}],["由于想要看到",{"2":{"112":1}}],["由于进行测试的文件中涉及参考文献的引用",{"2":{"111":1}}],["由于",{"2":{"101":1,"108":1}}],["由于大规模数据标注既困难又昂贵",{"2":{"73":1}}],["由于深度学习在计算机视觉领域的快速发展",{"2":{"19":1}}],["由",{"2":{"47":1,"131":1}}],["由类激活映射",{"2":{"47":1}}],["由类内差异表示和双语义感知注意力机制两个关键部分组成",{"2":{"21":1}}],["由不同水平的标注者标注",{"2":{"33":1}}],["导致路径已经是中文了",{"2":{"108":1}}],["导致特征模糊",{"2":{"67":1}}],["导致特征冗余",{"2":{"33":1,"35":1}}],["导致前景",{"2":{"62":1}}],["导致前景对象与背景的边界区域以及多语义不同对象内的误分类区域存在高度不确定性",{"2":{"45":1}}],["导致模态间知识共享和模态内信息处理失衡",{"2":{"55":1}}],["导致模型泛化能力要求高",{"2":{"33":1}}],["导致弱监督与全监督方法间存在显著性能差异",{"2":{"44":1}}],["导致物体外观随光照变化",{"2":{"39":1}}],["导致对未知类别的分割失败",{"2":{"24":1}}],["导致难以准确定位目标类别",{"2":{"21":1}}],["导致注意力偏差",{"2":{"21":1}}],["导致fss模型对未见领域的泛化能力较差",{"2":{"9":1}}],["这时候需要对边缘进行裁切或者补零",{"2":{"125":1}}],["这时可以使用外部查看器进行查看",{"2":{"112":1}}],["这要我们就可以把每一个像素的预测看成是一个分类任务",{"2":{"125":1}}],["这就意味着",{"2":{"108":1}}],["这里的快捷键为默认设置",{"2":{"111":1}}],["这里就不作赘述",{"2":{"102":1}}],["这里是重点compared",{"2":{"43":1}}],["这个时候可能就是由于辅助文件没有进行及时更新的缘故",{"2":{"108":1}}],["这个选项",{"2":{"102":1}}],["这个选项一定要选中",{"2":{"101":1}}],["这个模型被特别设计并训练为能够接受简单提示",{"2":{"90":1}}],["这项技术的核心难点在于如何有效融合不同模态",{"2":{"54":1}}],["这类方法通常会过度关注最具区分度的区域",{"2":{"44":1}}],["这是一个自动驾驶数据集",{"2":{"39":1}}],["这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域",{"2":{"24":1}}],["这些场景光照充足且均匀",{"2":{"39":1}}],["这些方法完全依赖于支持图像进行特征转换",{"2":{"8":1}}],["这种向量化的语义表示可以融合不同领域",{"2":{"73":1}}],["这种现象的典型表现是物体边界区域的识别精度下降",{"2":{"44":1}}],["这种固定参数的特征编码器往往对类别不敏感",{"2":{"41":1}}],["这种固定的编码器通常是类别无关的",{"2":{"41":1}}],["这种任务不仅在技术上具有挑战性",{"2":{"32":1}}],["这种关系不足以准确匹配",{"2":{"21":1}}],["这限制了模型在实际应用中对超高分辨率数据的处理能力",{"2":{"29":1}}],["这在人像照片后期处理",{"2":{"29":1}}],["这样的操作称为上采样",{"2":{"130":1}}],["这样可以帮助网络在复杂多变的光照条件下持续准确地识别语义",{"2":{"39":1}}],["这样",{"2":{"27":1}}],["用一句话来解释",{"2":{"130":1}}],["用作下文",{"2":{"108":1}}],["用文本嵌入",{"2":{"73":1}}],["用渐进式融合解码器生成高分辨率特征图进行分割",{"2":{"70":1}}],["用可逆神经网络",{"2":{"21":1}}],["用于自动驾驶场景",{"0":{"140":1}}],["用于反向同步的内部查看器的键绑定",{"2":{"119":1}}],["用于配置编译链",{"2":{"119":1}}],["用于促进下游语义分割任务",{"2":{"99":1}}],["用于开放词汇语义分割",{"2":{"70":1}}],["用于解决少样本语义分割任务中存在的语义模糊和类间相似性问题",{"2":{"65":1}}],["用于多模态图像分割任务",{"2":{"57":1}}],["用于调整编码器以聚焦不同fss任务中的特定对象",{"2":{"41":1}}],["用于更有效的训练和验证评估",{"2":{"39":1}}],["用于少样本语义分割",{"2":{"21":1}}],["用于跨域少样本语义分割",{"2":{"11":1}}],["倾向于提取与当前任务无关的特征",{"2":{"21":1}}],["存在显著的领域偏移问题",{"2":{"33":1}}],["存在图形模型依赖低级别颜色边界",{"2":{"29":1}}],["存在固有偏差",{"2":{"21":1}}],["存在三方面问题",{"2":{"19":1}}],["任务创新",{"2":{"90":1}}],["任务中",{"2":{"27":1}}],["任务",{"2":{"21":1}}],["如需写入到",{"2":{"115":1}}],["如上图所示",{"2":{"101":1}}],["如下图所示",{"2":{"103":1}}],["如下图",{"2":{"101":1,"111":1,"117":1}}],["如下采样",{"2":{"29":1}}],["如果bias=true",{"2":{"130":1}}],["如果出现",{"2":{"111":1}}],["如果使用onbuilt命令",{"2":{"108":1}}],["如果说论文中有很多图片或者其他元素没有嵌入字体的话",{"2":{"108":1}}],["如果您对此不感兴趣",{"2":{"108":1}}],["如果您日后需要在上述代码之后再添加其他代码",{"2":{"107":1}}],["如果您需要个性化程度高的话",{"2":{"101":1}}],["如果您想了解",{"2":{"101":1}}],["如果下载速度过慢",{"2":{"101":1}}],["如色彩化",{"2":{"95":1}}],["如交互式分割",{"2":{"90":1}}],["如使用涂鸦",{"2":{"84":1}}],["如使用预训练或低分辨率训练测试",{"2":{"29":1}}],["如ppnet和soopil的方法",{"2":{"64":1}}],["如pixda",{"2":{"10":1}}],["如sg",{"2":{"64":1}}],["如图像",{"2":{"54":1}}],["如第7",{"2":{"49":1}}],["如比beco分别高2",{"2":{"48":1}}],["如物体边界和易混淆类别",{"2":{"44":1}}],["如高置信度的前景和背景",{"2":{"44":1}}],["如计算机视觉",{"2":{"41":1}}],["如在gan框架中学习跨域不变表示",{"2":{"39":1}}],["如在眼底和前列腺基准测试中",{"2":{"38":1}}],["如egnet",{"2":{"39":1}}],["如deeplab系列引入空洞空间金字塔池化",{"2":{"39":1}}],["如",{"2":{"29":1}}],["如针对1k",{"2":{"29":1}}],["如clip和align利用对比学习训练文本和图像编码器",{"2":{"90":1}}],["如clip和align利用对比学习训练文本和图像编码器实现零样本泛化",{"2":{"90":1}}],["如clip",{"2":{"27":1}}],["如原始的普通注意力",{"2":{"24":1}}],["如动态调整分类器权重",{"2":{"20":1}}],["忽略了目标对象的空间一致性",{"2":{"21":1}}],["忽略空间信息",{"2":{"19":1}}],["忽略背景区域",{"2":{"9":1}}],["易优先提取无关特征",{"2":{"19":1}}],["易导致过拟合",{"2":{"9":1}}],["匹配和分类三个阶段",{"2":{"19":1}}],["执行过程分特征提取",{"2":{"19":1}}],["近期研究聚焦于多尺度特征融合",{"2":{"64":1}}],["近期也开始应用于视觉任务",{"2":{"56":1}}],["近期有基于nightcity数据集的方法",{"2":{"39":1}}],["近期vision",{"2":{"34":1}}],["近期提出了一些方法",{"2":{"10":1}}],["近年来",{"2":{"19":1,"27":1,"93":1,"94":1}}],["强化了约束效果",{"2":{"18":1}}],["双语义感知注意力",{"2":{"24":1}}],["双语义感知注意力机制通过两层约束",{"2":{"21":1}}],["双分类模块",{"2":{"21":1,"24":1}}],["双约束聚合模块",{"2":{"21":1,"24":2,"25":1}}],["双重约束聚合模块",{"2":{"18":1}}],["双超相关构建模块",{"2":{"11":1}}],["建立更为稳定的匹配关系",{"2":{"18":1}}],["512",{"2":{"131":2}}],["5k",{"2":{"70":2}}],["500步时将学习率降至十分之一",{"2":{"29":1}}],["500和37",{"2":{"29":1}}],["50作为编码器eθ",{"2":{"29":1}}],["572张具有超过1000个语义类别的图像",{"2":{"29":1}}],["5",{"0":{"105":1,"128":1},"2":{"24":1,"48":2,"65":1,"66":2,"87":1,"96":1,"98":1,"105":1,"131":3}}],["5i数据集",{"2":{"66":1}}],["5i和coco基准测试中超越了现有技术水平",{"2":{"68":1}}],["5i和coco数据集上的表现优于现有方法",{"2":{"65":1}}],["5i和coco",{"2":{"18":1,"21":1,"25":1,"41":4}}],["5i",{"2":{"17":1,"23":1,"62":1,"66":1}}],["59",{"2":{"12":1,"70":1}}],["💯",{"0":{"16":1},"2":{"122":1}}],["澳门大学",{"0":{"16":1}}],["青海师范大学",{"0":{"16":1}}],["南京信息工程大学",{"0":{"16":1}}],["提取初始特征图",{"2":{"85":1}}],["提供最终的分割结果",{"2":{"65":1}}],["提高涂鸦监督语义分割的性能",{"2":{"83":1}}],["提高效率",{"2":{"83":1}}],["提高了前景的类内泛化能力",{"2":{"68":1}}],["提高空间信息的适应性",{"2":{"57":1}}],["提高wsss性能",{"2":{"45":1}}],["提高分割精度",{"2":{"41":1}}],["提高训练效率",{"2":{"27":1}}],["提示词分割",{"2":{"90":1}}],["提示编码器和掩码解码器组成",{"2":{"90":1}}],["提示生成",{"2":{"57":1}}],["提示调优作为新范式",{"2":{"56":1}}],["提示增强次数",{"2":{"41":1}}],["提示增强消融实验",{"2":{"41":1}}],["提示初始化消融实验",{"2":{"41":1}}],["提示学习",{"2":{"41":1}}],["提示与迁移",{"2":{"41":1}}],["提升预测性能",{"2":{"88":1}}],["提升分割性能",{"2":{"45":1}}],["提升分割效果",{"2":{"14":1}}],["提升",{"2":{"41":1}}],["提升模型的识别能力",{"2":{"41":1}}],["提升其在夜间的分割效果",{"2":{"39":1}}],["提出多种预训练任务",{"2":{"95":1}}],["提出多级原型生成与交互模块",{"2":{"68":1}}],["提出可提示分割任务",{"2":{"90":1}}],["提出的特征自强化",{"2":{"48":1}}],["提出的模型",{"0":{"11":1,"21":1,"35":1,"47":1,"57":1,"65":1,"76":1,"85":1,"96":1},"2":{"41":1}}],["提出了一个系统的自监督预训练框架",{"2":{"94":1}}],["提出了一种经济高效的训练方法",{"2":{"82":1}}],["提出了一种新颖的动态类别感知提示范式",{"2":{"41":1}}],["提出了一种新颖且高效的基于提示的方案",{"2":{"41":1}}],["提出了放松的深度白化变换",{"2":{"38":1}}],["提出语义导向的解纠缠",{"2":{"39":1}}],["提出nightcity",{"2":{"39":2}}],["提出松弛深度白化变换",{"2":{"35":1}}],["提出连续细化模型",{"2":{"29":1}}],["提出问题",{"2":{"27":1}}],["提出",{"2":{"27":2,"39":1,"41":1}}],["避免过度依赖支持图像导致过拟合",{"2":{"14":1}}],["避免对支持图像过拟合",{"2":{"11":1}}],["移除任何一个模块都会导致平均性能下降",{"2":{"13":1}}],["使得字体变大",{"2":{"112":1}}],["使冻结的预训练基础模型适应各种下游多模态分割任务",{"2":{"59":1}}],["使冻结的预训练模型能灵活适配多种多模态分割任务",{"2":{"54":1}}],["使模型适应各种下游多模态分割任务",{"2":{"57":1}}],["使模型学习目标域风格信息",{"2":{"14":1}}],["使模型学习目标域的风格信息",{"2":{"11":1}}],["使特定区域的全局语义更好地聚合到提示中",{"2":{"41":1}}],["使其初步定位目标",{"2":{"41":1}}],["使编码器更精准地聚焦于目标对象",{"2":{"41":1}}],["使分割不受复杂光照干扰",{"2":{"39":1}}],["使网络在不同光照下提取一致特征",{"2":{"39":1}}],["使光不变反射率和光特定光照之间的纠缠加剧",{"2":{"39":1}}],["使用上次的recipe编译组合",{"2":{"119":1}}],["使用vscode内置pdf查看器或使用电脑默认浏览器进行pdf查看",{"2":{"116":1}}],["使用外部pdf查看器查看",{"2":{"116":1}}],["使用外部查看器时",{"2":{"119":1}}],["使用外部查看器时要执行的命令",{"2":{"119":1}}],["使用外部查看器",{"2":{"116":1}}],["使用外部",{"2":{"116":1}}],["使用电脑默认浏览器进行",{"2":{"116":1}}],["使用sumatrapdf查看的代码配置",{"0":{"114":1},"1":{"115":1,"116":1}}],["使用swin",{"2":{"96":1}}],["使用内置查看器已无法满足需求",{"2":{"112":1}}],["使用快捷键",{"2":{"111":1}}],["使用右键菜单",{"2":{"111":1}}],["使用侧边工具栏",{"2":{"111":1}}],["使用鼠标左键双击",{"2":{"108":1}}],["使用最近一次编译所用的编译链",{"2":{"108":1}}],["使用latex",{"2":{"108":1}}],["使用",{"2":{"108":1,"116":1,"119":1}}],["使用的是tex的标准字体",{"2":{"108":1}}],["使用的括号就比较多",{"2":{"100":1}}],["使用的数据集",{"2":{"39":1}}],["使用线性头时",{"2":{"97":1}}],["使用全局自注意力",{"2":{"96":1}}],["使用稍弱的骨干网络mit",{"2":{"88":1}}],["使用mit",{"2":{"87":1}}],["使用一致性损失",{"2":{"85":1}}],["使用一致编码器的比较",{"2":{"41":1}}],["使用局部和全局原型通过原型特征增强器对初始特征进行增强",{"2":{"85":1}}],["使用基于mix",{"2":{"85":1}}],["使用基于rgb的预训练基础模型的参数初始化多模态分割模型",{"2":{"57":1}}],["使用无标签分支或多级别原型交互均可使性能提升约2",{"2":{"67":1}}],["使用resnet101时",{"2":{"66":1}}],["使用resnet50骨干时",{"2":{"66":1}}],["使用transformer编码器对增强的查询特征进行自注意力和交叉注意力处理",{"2":{"65":1}}],["使用先验掩码和局部平均池化",{"2":{"65":1}}],["使用相互生成的伪标签进行训练",{"2":{"65":1}}],["使用n层transformer编码器进行特征激活",{"2":{"65":1}}],["使用强大的autoaugment时",{"2":{"49":1}}],["使用在",{"2":{"47":1}}],["使用更复杂的解码器不一定能优于简单的相似度计算",{"2":{"41":1}}],["使用deit",{"2":{"41":1}}],["使用dcm实现语义对齐和空间对齐可额外提升1",{"2":{"24":1}}],["使用ctrl",{"2":{"108":1}}],["使用clip提取的语言信息作为初始提示具有最优的分割结果",{"2":{"41":1}}],["使用cascadepsp提出的高分辨率图像分割数据集big",{"2":{"29":1}}],["使用随机初始化的提示来调整编码器不会带来性能提升",{"2":{"41":1}}],["使用其子集bdd100k",{"2":{"39":1}}],["使用风格解耦的键和值",{"2":{"37":1}}],["使用特征查询使dsc提高了0",{"2":{"37":1}}],["使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题",{"2":{"29":1}}],["使用超高清图像进行训练和测试仍耗资源",{"2":{"29":1}}],["使用adam优化器",{"2":{"29":1}}],["使用去掉conv5",{"2":{"29":1}}],["使用pytorch实现模型",{"2":{"29":1}}],["使用掩码注意力减轻背景噪声干扰对性能提升影响不大",{"2":{"24":1}}],["使用所有三个模块时模型性能最佳",{"2":{"13":1}}],["08",{"2":{"36":1,"110":1}}],["0",{"2":{"36":3,"48":3,"49":1,"70":1,"130":9,"131":2}}],["0之间",{"2":{"29":1}}],["000步",{"2":{"29":1}}],["02",{"2":{"12":1,"110":1}}],["01",{"2":{"12":1,"70":1}}],["和输入内部插",{"2":{"130":1}}],["和区域的交并比",{"2":{"128":1}}],["和微调miou",{"2":{"98":1}}],["和局部注意力阶段",{"2":{"96":1}}],["和计算机视觉领域的发展现状与需求",{"2":{"90":1}}],["和对应的数据集",{"2":{"90":1}}],["和全局最大池化",{"2":{"49":1}}],["和确定区域",{"2":{"49":1}}],["和75",{"2":{"48":1}}],["和5",{"2":{"41":1}}],["和3",{"2":{"41":1}}],["和0",{"2":{"36":1}}],["和1",{"2":{"32":1,"38":1,"41":1,"48":1}}],["和平均精度",{"2":{"29":1}}],["和",{"2":{"27":1,"44":1,"62":1,"70":4,"73":2,"93":1,"101":1,"108":1,"117":1}}],["和类内差异表示",{"2":{"24":1}}],["和双分类模块",{"2":{"24":1,"25":1}}],["和双重分类模块",{"2":{"18":1}}],["和4",{"2":{"12":1,"41":1}}],["和域泛化语义分割",{"2":{"10":1}}],["与上文相同",{"2":{"116":1}}],["与通用软件安装过程一致",{"2":{"113":1}}],["与当前最先进的方法tel相比",{"2":{"86":1}}],["与人工统一标签体系不同",{"2":{"73":1}}],["与普通的transformer相比",{"2":{"70":1}}],["与原始少样本任务设置不符",{"2":{"64":1}}],["与局部原型",{"2":{"62":1}}],["与其他数据增强方法的比较结果表明",{"2":{"49":1}}],["与全局平均池化",{"2":{"49":1}}],["与使用图像级标签和现成显著性图的多阶段方法相比",{"2":{"48":1}}],["与传统的像素级监督语义分割相比",{"2":{"45":1}}],["与不确定区域",{"2":{"44":1}}],["与sota方法的对比",{"0":{"58":1}}],["与sota方法的对比实验",{"2":{"39":1}}],["与spt协同工作",{"2":{"41":1}}],["与此不同",{"2":{"41":1}}],["与nightcity",{"2":{"39":1}}],["与以往方法将光照信息与特征混合的做法不同",{"2":{"39":1}}],["与现有方法在pascal",{"2":{"86":1}}],["与现有方法相比",{"2":{"36":1}}],["与现有最先进的dcama相比",{"2":{"66":1}}],["与现有最优方法对比",{"2":{"36":1}}],["与次优的ram",{"2":{"36":1}}],["与次优方法相比",{"2":{"36":1}}],["与每个特征的lirdwt的组合",{"2":{"35":1}}],["与cascadepsp的iou相当",{"2":{"29":1}}],["与基线相比",{"2":{"24":1,"41":1}}],["与最先进的patnet相比",{"2":{"12":1}}],["与迁移学习",{"2":{"12":1}}],["1cityscape",{"0":{"140":1}}],["17",{"2":{"131":2}}],["171",{"2":{"70":1}}],["1代码解读",{"2":{"116":1}}],["1使用外部查看器时要执行的命令",{"2":{"116":1}}],["1用于反向同步",{"2":{"108":1}}],["1该命令的作用为设置",{"2":{"108":1}}],["1这条命令是设置什么时候对上文设置的辅助文件进行清除",{"2":{"108":1}}],["1设置pdf查看器用于在",{"2":{"116":1}}],["1设置默认的pdf查看器",{"2":{"116":1}}],["1设置为true",{"2":{"108":1}}],["1设置何时使用默认的",{"2":{"108":1}}],["1启用上下文latex菜单",{"2":{"108":1}}],["1后添加上",{"2":{"107":1}}],["1版本",{"2":{"104":1}}],["12此命令是将生成的辅助文件",{"2":{"116":1}}],["1234567",{"2":{"157":1}}],["123456789output",{"2":{"157":1}}],["12345678910",{"2":{"131":1}}],["12345678910设置当",{"2":{"116":1}}],["12345678910111213141516171819output",{"2":{"158":1}}],["123456789101112131415161718192021这串命令则是设置编译完成后要清除掉的辅助文件类型",{"2":{"108":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657pythondef",{"2":{"131":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153写在最后",{"2":{"119":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116注",{"2":{"107":1}}],["1234567891011121314151617181920212223242526272829303132333435363738394041424344此串代码是对编译链进行定义",{"2":{"108":1}}],["1234567891011121314151617181920212223242526272829303132333435363738394041这些代码是定义在下文",{"2":{"108":1}}],["123456789101112131415161718此代码仅为展示所用",{"2":{"115":1}}],["12345678910111213141516",{"2":{"0":1,"110":1}}],["123456789代码解读",{"2":{"116":1}}],["1234pythonclass",{"2":{"131":1}}],["1234此代码是设置使用外部查看器时",{"2":{"116":1}}],["123代码解读",{"2":{"116":1}}],["12",{"2":{"116":1}}],["12这两个命令是设置当文档编译错误时是否弹出显示出错和警告的弹窗",{"2":{"108":1}}],["12其中的",{"2":{"107":1}}],["12656706",{"2":{"101":1}}],["1b数据集",{"2":{"90":1}}],["1b",{"2":{"90":4}}],["136",{"2":{"72":1}}],["150",{"2":{"70":2}}],["11m",{"2":{"90":2}}],["111",{"2":{"72":1}}],["118k",{"2":{"70":1}}],["11层",{"2":{"49":1}}],["11",{"0":{"119":1},"2":{"41":1,"104":1,"111":1}}],["166523064",{"2":{"119":1}}],["16虽然分割性能较差",{"2":{"41":1}}],["16或deit",{"2":{"41":1}}],["16次之",{"2":{"41":1}}],["16骨干网络在所有设置下具有最佳的分割精度",{"2":{"41":1}}],["16",{"2":{"37":1}}],["18",{"2":{"36":1}}],["14",{"2":{"36":1}}],["10mb",{"2":{"112":1}}],["10",{"0":{"118":1},"2":{"110":1,"131":2}}],["10k",{"2":{"29":1}}],["1000合并为训练数据集",{"2":{"29":1}}],["1000",{"2":{"12":1}}],["1k",{"2":{"29":1}}],["1",{"0":{"101":1,"107":1,"110":1,"113":1,"115":1,"124":1,"133":1,"134":2,"135":1,"136":1,"137":1,"139":1,"143":1,"146":1},"1":{"134":1,"135":1,"136":1,"137":1},"2":{"27":1,"29":1,"31":2,"39":2,"41":2,"50":1,"53":1,"54":1,"67":1,"70":2,"72":1,"90":1,"93":1,"96":1,"100":1,"105":1,"130":6,"131":57}}],["生成的",{"2":{"108":1}}],["生成的伪分割标签监督",{"2":{"47":1}}],["生成新的跨模态对齐诱导提示",{"2":{"57":1}}],["生成特定于模态的提示",{"2":{"57":1}}],["生成对应的rgb标记和辅助模态标记",{"2":{"57":1}}],["生成类激活图",{"2":{"45":1}}],["生成鲁棒的支持类别原型",{"2":{"21":1}}],["生成并监督前景和背景预测掩码",{"2":{"14":1}}],["生成预测的查询前景掩码和背景掩码",{"2":{"11":1}}],["生成更细粒度的预测查询掩码",{"2":{"11":1}}],["分类模型",{"2":{"125":1}}],["分类",{"2":{"124":1}}],["分层编码器",{"2":{"93":1}}],["分割的指标主要有两个",{"2":{"128":1}}],["分割",{"2":{"124":1}}],["分割种类的增加也会增加计算资源",{"2":{"70":1}}],["分割性能提升有限",{"2":{"39":1}}],["分割细化",{"2":{"29":1}}],["分析crm和隐式函数对不同分辨率下性能的影响",{"2":{"29":1}}],["分析数据",{"2":{"27":1}}],["分辨率图像的细化技术能提升分割质量",{"2":{"29":1}}],["分别基于支持前景特征和背景特征与查询特征构建4d相关张量",{"2":{"11":1}}],["分为域自适应语义分割",{"2":{"10":1}}],["探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性",{"2":{"11":1}}],["探索查询图像与支持图像前景和背景之间的超相关性",{"2":{"8":1}}],["自行选择",{"2":{"102":1}}],["自带的",{"2":{"101":1}}],["自监督预训练策略",{"2":{"96":1}}],["自监督学习",{"2":{"93":1,"94":1,"95":1}}],["自监督损失用于无标签分支的训练",{"2":{"65":1}}],["自全卷积网络",{"2":{"64":1}}],["自蒸馏",{"2":{"50":1}}],["自蒸馏机制",{"2":{"47":1}}],["自蒸馏方法",{"2":{"46":1}}],["自适应特征变换",{"2":{"11":1}}],["自匹配变换模块",{"2":{"11":1}}],["并不影响本文中所说的所有配置",{"2":{"100":1}}],["并对两个损失项进行等权重加权",{"2":{"96":1}}],["并对编码器和解码器进行联合预训练",{"2":{"96":1}}],["并学习细粒度的语义特征",{"2":{"94":1}}],["并在广泛的数据集上进行预训练",{"2":{"90":1}}],["并在11个基准测试中设立了新的技术标准",{"2":{"41":1}}],["并根据涂鸦监督的具体情况采用不同原型策略",{"2":{"88":1}}],["并通过1×1卷积和通道注意力机制进行细化",{"2":{"65":1}}],["并应用相同的分割损失",{"2":{"65":1}}],["并有效继承预训练基础模型的先验知识",{"2":{"57":1}}],["并有效泛化到任意未知目标域",{"2":{"32":1}}],["并引入自蒸馏方法增强语义一致性",{"2":{"50":1}}],["并将其集成到单阶段框架中",{"2":{"50":1}}],["并激发未来相关研究聚焦于此",{"2":{"41":1}}],["并且此文主要将",{"2":{"101":1}}],["并且页面不是很美观",{"2":{"100":1}}],["并且其标注过程也非常繁琐",{"2":{"83":1}}],["并且计算复杂度与输入的大小成线性关系",{"2":{"70":1}}],["并且具有类别早期拒绝的功能",{"2":{"70":1}}],["并且与基于解码器的方法具有良好的兼容性",{"2":{"41":1}}],["并且能更好地重建粗掩码中缺失的部分",{"2":{"29":1}}],["并结合spt",{"2":{"41":1}}],["并仅微调解码器",{"2":{"41":1}}],["并聚合光照特征",{"2":{"39":1}}],["并提出",{"2":{"27":1}}],["并得出以下结论",{"2":{"14":1,"29":1,"39":1}}],["并使用bce损失函数进行训练监督",{"2":{"11":1}}],["并使用二元交叉熵",{"2":{"11":1}}],["并忽视类内外观的差异",{"2":{"8":1}}],["将图像分割成一个个小的区域",{"2":{"127":1}}],["将图像的两个增强视图分别输入到教师网络和学生网络中",{"2":{"96":1}}],["将synctex转发到外部查看器时要执行的命令",{"2":{"119":1}}],["将完整代码复制到自己的",{"2":{"117":1}}],["将之下载后",{"2":{"110":1}}],["将之添加到环境变量",{"2":{"101":1}}],["将编译方式",{"2":{"108":1}}],["将在下文提及",{"2":{"108":1}}],["将解码器纳入预训练框架显著提高了线性探测miou",{"2":{"98":1}}],["将增强后的特征图再次通过解码器生成增强预测图",{"2":{"85":1}}],["将特征图输入解码器生成语义分割预测图",{"2":{"85":1}}],["将特定领域特征自适应转换为通用特征",{"2":{"14":1}}],["将成本图与不同层级骨干网络的特征图进行融合",{"2":{"70":1}}],["将生成的全局和局部原型扩展到特征图的大小",{"2":{"65":1}}],["将语义分割转化为像素级分类后",{"2":{"64":1}}],["将其他数据源的关键模式整合到rgb流中",{"2":{"57":1}}],["将学习到的提示作为残差添加到原始rgb流中",{"2":{"57":1}}],["将rgb图像和辅助模态图像输入到补丁嵌入层",{"2":{"57":1}}],["将确定特征的注意力聚合",{"2":{"49":1}}],["将所有令牌转换到合适的特征空间进行特征学习",{"2":{"47":1}}],["将patch",{"2":{"47":1}}],["将自监督学习与知识蒸馏结合",{"2":{"46":1}}],["将额外的类别语义作为初始提示可以调整编码器",{"2":{"41":1}}],["将crm添加到panopticfcn和entityseg后",{"2":{"29":1}}],["将crm作为全景分割和实体分割的扩展进行评估",{"2":{"29":1}}],["将msra",{"2":{"29":1}}],["将vfms用于dgss任务存在挑战",{"2":{"27":1}}],["将分割任务解耦为语义对齐和空间对齐两个子任务",{"2":{"21":1}}],["将得到的密集相关图输入到4d卷积金字塔编码器和2d卷积金字塔解码器中",{"2":{"11":1}}],["将支持特征划分为多个局部特征",{"2":{"11":1}}],["将领域特定的查询特征转换为领域无关的特征",{"2":{"8":1}}],["相反",{"2":{"111":1}}],["相比当前最优方法tel",{"2":{"88":1}}],["相比像素级标注",{"2":{"83":1}}],["相关方法在语义分割任务中表现良好",{"2":{"64":1}}],["相关代码已开源",{"2":{"82":1}}],["相关代码和数据集可以在https",{"2":{"39":1}}],["相关代码可以在https",{"2":{"29":1}}],["相关代码可在",{"2":{"8":1}}],["相较于需要像素级标注的传统语义分割方法",{"2":{"44":1}}],["相似性自匹配",{"2":{"11":1}}],["研究了不同骨干网络对方法的影响",{"2":{"87":1}}],["研究了解耦查询和风格不变的键与值的影响",{"2":{"37":1}}],["研究者越来越关注利用涂鸦标签进行监督学习的方法",{"2":{"83":1}}],["研究结论",{"2":{"41":1}}],["研究方法",{"2":{"29":1,"70":1}}],["研究思路清晰",{"2":{"27":1}}],["研究思路",{"2":{"27":1}}],["研究动机",{"2":{"27":1}}],["研究现状",{"0":{"10":1,"20":1,"34":1,"46":1,"56":1,"64":1,"75":1,"84":1,"95":1},"2":{"27":1,"29":1,"39":1,"41":1,"70":1,"90":1}}],["研究背景如下",{"2":{"45":1}}],["研究背景",{"0":{"9":1,"19":1,"33":1,"45":1,"55":1,"63":1,"74":1,"83":1,"94":1},"2":{"29":1,"39":1,"41":1,"70":1,"90":1}}],["被提出用于模拟有限数据和多类别的真实世界场景",{"2":{"19":1}}],["被提出",{"2":{"9":1}}],["跨数据集测试",{"2":{"70":1}}],["跨领域特征对齐",{"2":{"36":1}}],["跨领域少样本语义分割",{"2":{"9":1}}],["跨域fss",{"2":{"41":1}}],["跨域语义分割",{"2":{"10":1}}],["跨域少样本语义分割",{"2":{"8":1,"10":1}}],["为第二个卷积块",{"2":{"131":1}}],["为第一个卷积块",{"2":{"131":1}}],["为w×d×n",{"2":{"125":1}}],["为下文解读之用",{"2":{"115":1}}],["为默认选项",{"2":{"108":1}}],["为您添加的其余代码",{"2":{"107":1}}],["为其专属定制编辑器",{"2":{"100":1}}],["为模型训练提供强大支撑",{"2":{"90":1}}],["为二进制分类提供细粒度信息",{"2":{"65":1}}],["为更好的类间区分提供多粒度证据",{"2":{"65":1}}],["为增强类间区分度",{"2":{"62":1}}],["为此",{"2":{"62":1,"68":1,"73":1}}],["为此开发了互补式自蒸馏方法",{"2":{"44":1}}],["为不同个体生成不同但互补的部分提示",{"2":{"41":1}}],["为了出现和内嵌输出具有相同的效果",{"2":{"117":1}}],["为了测试",{"2":{"110":1}}],["为了后面不必要的麻烦",{"2":{"101":1}}],["为了让更多人能够有一个比较清晰的了解",{"2":{"100":1}}],["为了融合全局和局部上下文信息",{"2":{"96":1}}],["为了获得高分辨率的细粒度特征",{"2":{"96":1}}],["为了提高推理速度",{"2":{"70":1}}],["为了更方便进行编译",{"2":{"111":1}}],["为了更有效地泛化到未见类别",{"2":{"41":1}}],["为了更高效地在未知领域",{"2":{"41":1}}],["为了增强提示效果",{"2":{"41":1}}],["为了解决这个问题",{"2":{"39":1}}],["为了解决上述问题",{"2":{"8":1}}],["为夜间分割提供更可靠基准",{"2":{"39":1}}],["为应对不同领域间的分布差异",{"2":{"35":1}}],["为证明模型的通用性",{"2":{"29":1}}],["为解决这些问题",{"2":{"63":1}}],["为解决这一问题",{"2":{"44":1}}],["为解决上述问题",{"2":{"45":1}}],["为解决跨领域医学图像的特征不对齐问题",{"2":{"38":1}}],["为解决超高分辨率图像分割中精度与计算成本的平衡问题",{"2":{"29":1}}],["为解决fss模型在跨领域场景下性能显著下降的问题",{"2":{"9":1}}],["为该领域建立重要基准",{"2":{"27":1}}],["为支持和查询特征分别构建专门的变换矩阵",{"2":{"11":1}}],["为查询图像生成粗略的分割掩码",{"2":{"11":1}}],["但其只能测试一部分功能",{"2":{"110":1}}],["但其在零样本",{"2":{"73":1}}],["但笔者不建议这么做",{"2":{"108":1}}],["但却可以对自己想要的功能直接进行代码编写",{"2":{"105":1}}],["但一些设置需要去寻找",{"2":{"105":1}}],["但建议在不明白各个选项的作用时",{"2":{"101":1}}],["但texstudio的代码高亮功能实在是",{"2":{"100":1}}],["但标注成本高昂",{"2":{"94":1}}],["但由于语义分割标注的获取成本极高",{"2":{"93":1}}],["但缺乏大规模",{"2":{"90":1}}],["但计算机视觉问题广泛",{"2":{"90":1}}],["但计算复杂度高",{"2":{"64":1}}],["但该方法的miou仍提高了0",{"2":{"86":1}}],["但未充分发挥其特征增强和引导作用",{"2":{"84":1}}],["但是这个思想一个很重要的问题就是效率低",{"2":{"127":1}}],["但是其编译速度比较慢",{"2":{"100":1}}],["但是",{"2":{"83":1}}],["但都需额外无标签数据",{"2":{"64":1}}],["但获取像素级标注需大量人力和成本",{"2":{"63":1}}],["但效率低",{"2":{"55":1}}],["但面临诸多挑战",{"2":{"55":1}}],["但这种方法存在明显局限",{"2":{"54":1}}],["但这些方法存在一定局限性",{"2":{"84":1}}],["但这些方法存在一定缺陷",{"2":{"83":1}}],["但这些方法未明确估计光照对语义的影响",{"2":{"39":1}}],["但这些数据驱动的技术在标注数据不足时表现不佳",{"2":{"41":1}}],["但性能常不如多阶段方法",{"2":{"46":1}}],["但难以应对稀疏训练数据",{"2":{"64":1}}],["但难以应对不同成像条件下任意未见领域的特征分布变化",{"2":{"33":1}}],["但难以利用",{"2":{"45":1}}],["但推理速度具有竞争力",{"2":{"41":1}}],["但综合效率和性能考虑",{"2":{"41":1}}],["但存在标注错误",{"2":{"39":1}}],["但存在图形模型依赖低层次颜色边界",{"2":{"29":1}}],["但大多聚焦白天场景",{"2":{"39":1}}],["但大多不是为dgss设计",{"2":{"27":1}}],["但它们通常基于光照纠缠的表示进行场景解析",{"2":{"39":1}}],["但因夜间缺乏对应标签",{"2":{"39":1}}],["但松弛白化变换损失无法保证不同领域的医学图像在同一通道上显示相似的特征响应",{"2":{"35":1}}],["但实际中",{"2":{"33":1}}],["但实际场景中收集大量训练数据耗时且成本高",{"2":{"9":1}}],["但更注重细节",{"2":{"29":1}}],["但结构复杂",{"2":{"29":1}}],["但直接插值预测结果存在边缘锯齿和细节缺失问题",{"2":{"29":1}}],["但多采用vggnet",{"2":{"27":1}}],["但受背景干扰",{"2":{"19":1}}],["但在推理时需要下采样和裁剪补丁",{"2":{"29":1}}],["但在dgss中的性能及利用方式尚不明确",{"2":{"27":1}}],["但在dgss任务中的表现缺乏专门研究",{"2":{"27":1}}],["但在源域和目标域差距大时",{"2":{"10":1}}],["但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征",{"2":{"9":1}}],["但在实际应用中",{"2":{"9":1}}],["旨在生成具有全局上下文信息传播的高分辨率特征",{"2":{"94":1}}],["旨在推动计算机视觉领域的基础模型研究",{"2":{"90":1}}],["旨在提高少样本任务中前景分割性能",{"2":{"63":1}}],["旨在利用多数据源增强细粒度细节和像素级语义",{"2":{"56":1}}],["旨在利用少量标注样本快速泛化到未见领域",{"2":{"41":1}}],["旨在解决现有方法存在的问题",{"2":{"45":1}}],["旨在解决少样本和域差距问题",{"2":{"10":1}}],["旨在通过少量的标注样本为新的类别生成一个分割模型",{"2":{"18":1}}],["旨在用少量标注支持图像实现查询图像的准确分割",{"2":{"9":1}}],["旨在训练能够以少量标注图像对不同领域进行分割的通用模型",{"2":{"8":1}}],["在分割任务中",{"2":{"128":1}}],["在语义分割而言",{"2":{"125":1}}],["在构建失败后清除辅助文件",{"2":{"119":1}}],["在编译生成的",{"2":{"111":1}}],["在编译链中定义的命令出现在了vscode右侧的工具栏中",{"2":{"108":1}}],["在编辑器页面上端进行编译链选择",{"2":{"111":1}}],["在检测任何依赖项中的文件更改",{"2":{"108":1}}],["在打开方式中选择",{"2":{"101":1}}],["在线性探测和微调上分别高出2",{"2":{"98":1}}],["在不同比例标记的ade20k图像上",{"2":{"97":1}}],["在不增加额外数据的情况下",{"2":{"68":1}}],["在ade20k数据集上",{"2":{"97":1}}],["在具有挑战性的cityscapes数据集上",{"2":{"97":1}}],["在coco",{"2":{"97":1,"99":1}}],["在imagenet上进行300个epoch的预训练",{"2":{"97":1}}],["在定性结果中",{"2":{"96":1}}],["在stage",{"2":{"96":1}}],["在深层",{"2":{"96":1}}],["在浅层",{"2":{"96":1}}],["在自动驾驶",{"2":{"94":1}}],["在自然语言处理中表现出色",{"2":{"56":1}}],["在自然语言处理领域取得成功",{"2":{"27":1}}],["在特征提取方面展现出显著成效",{"2":{"93":1}}],["在特征提取阶段",{"2":{"21":1}}],["在特定领域",{"2":{"90":1}}],["在提供多个提示点时",{"2":{"90":1}}],["在提示调优过程中",{"2":{"57":1}}],["在图像分割方面",{"2":{"90":1}}],["在nlp中",{"2":{"90":1}}],["在原型提取时",{"2":{"87":1}}],["在训练迭代中",{"2":{"85":1}}],["在标准数据集微调后",{"2":{"73":1}}],["在对语义相近的类别进行分类和分割时存在困难",{"2":{"70":1}}],["在解码器中引入类别早期拒绝方案",{"2":{"70":1}}],["在解码高层特征时",{"2":{"35":1}}],["在零样本任务上表现出色",{"2":{"70":1}}],["在少样本分割任务中",{"2":{"68":1}}],["在单样本元训练过程中",{"2":{"67":1}}],["在单样本设置下",{"2":{"67":1}}],["在单样本设置下比cyctr高约3",{"2":{"66":1}}],["在处理前景与背景相似性问题上表现更好",{"2":{"66":1}}],["在支持集和查询集中前景对象姿态",{"2":{"66":1}}],["在该数据集的复杂场景下",{"2":{"66":1}}],["在传统的支持",{"2":{"65":1}}],["在区分前景和背景方面",{"2":{"63":1}}],["在医疗图像理解",{"2":{"63":1}}],["在保留模态内独特空间信息的同时",{"2":{"54":1}}],["在两个基准测试中",{"2":{"50":1}}],["在验证集上miou达到45",{"2":{"48":1}}],["在",{"2":{"44":1,"62":1,"93":2}}],["在使用一致的特征编码器设置下",{"2":{"41":1}}],["在变压器编码器的最后l个块中进行提示增强",{"2":{"41":1}}],["在没有spt和pmg持续增强其类别感知能力的情况下",{"2":{"41":1}}],["在pascal",{"2":{"41":4,"50":1,"96":1}}],["在白天场景下从50个不同城市采集",{"2":{"39":1}}],["在各种夜间分割任务的实验中",{"2":{"39":1}}],["在多个语义分割数据集",{"2":{"96":1}}],["在多个数据集",{"2":{"70":1}}],["在多个数据集和三种泛化设置下进行实验",{"2":{"27":1}}],["在多个下游多模态图像分割任务",{"2":{"57":1}}],["在多个任务和基准上取得新的最优性能",{"2":{"41":1}}],["在多个指标上取得了最佳性能",{"2":{"38":1}}],["在第一",{"2":{"36":1}}],["在学习解相关表示时存在问题",{"2":{"35":1}}],["在眼底和前列腺的泛化性基准测试中",{"2":{"32":1}}],["在性能和速度方面表现出色",{"2":{"29":1}}],["在重新标注的pascal",{"2":{"29":1}}],["在全景分割和实体分割实验中",{"2":{"29":1}}],["在高分辨率图像上运行速度更快",{"2":{"29":1}}],["在big数据集上对比crm",{"2":{"29":1}}],["在实验中从连续范围中选择4个缩放比例进行细化",{"2":{"29":1}}],["在22",{"2":{"29":1}}],["在神经网络中用于表示对象或场景",{"2":{"29":1}}],["在骨干网络层间嵌入该机制",{"2":{"27":1}}],["在本文中",{"2":{"27":1}}],["在本研究中",{"2":{"18":1}}],["在目标类别存在显著类内差异时",{"2":{"21":1}}],["在前馈过程中保留更细粒度的特征",{"2":{"21":1}}],["在fss中得到应用",{"2":{"20":1}}],["在这种情况下",{"2":{"19":1}}],["在5",{"2":{"12":1}}],["在1",{"2":{"12":2}}],["在元测试阶段",{"2":{"11":1,"14":1}}],["在四个流行数据集上的大量实验表明",{"2":{"8":1}}],["在获得领域无关特征后",{"2":{"8":1}}],["以池化层为区分",{"2":{"131":1}}],["以下展示由外部查看转为内部查看的操作",{"2":{"118":1}}],["以下为具体操作",{"2":{"117":1}}],["以下是详细介绍",{"2":{"96":1}}],["以下是该模型的详细介绍",{"2":{"57":1,"65":1}}],["以此来进行更多更好的文字编写",{"2":{"119":1}}],["以此来选中",{"2":{"111":1}}],["以此测试其是否支持中英文",{"2":{"110":1}}],["以此可以随时对自己的配置代码进行更改",{"2":{"100":1}}],["以免后期使用产生奇怪的问题",{"2":{"101":1}}],["以确保编码器和解码器网络得到充分的预训练",{"2":{"96":1}}],["以提升语义分割任务的性能",{"2":{"94":1}}],["以挖掘其巨大潜力和应用价值",{"2":{"88":1}}],["以仅使用部分交叉熵损失作为基线",{"2":{"87":1}}],["以捕获高维类别级别的类别信息",{"2":{"65":1}}],["以进行准确的识别",{"2":{"65":1}}],["以少量提示参数促进模型快速收敛",{"2":{"57":1}}],["以深度多模态融合为主",{"2":{"56":1}}],["以继承基础模型的强大特征提取能力",{"2":{"54":1}}],["以改进模型训练",{"2":{"47":1}}],["以约束最后一层补丁令牌之间的亲和性",{"2":{"47":1}}],["以明确不确定区域的视觉语义",{"2":{"45":1}}],["以生成类别感知特征",{"2":{"41":1}}],["以动态驱动编码器关注特定对象",{"2":{"41":1}}],["以极少额外参数助力现有方法提升性能",{"2":{"39":1}}],["以更灵活的方式实现通道解耦特征学习",{"2":{"32":1}}],["以更准确地在未知领域自我调整查询预测",{"2":{"8":1}}],["以实现高效",{"2":{"29":1}}],["以较少可训练参数有效利用vfms",{"2":{"27":1}}],["以及专门设计的自监督训练策略用于学习细粒度语义特征",{"2":{"93":1}}],["以及构建数据引擎收集大规模数据集",{"2":{"90":1}}],["以及风格解耦的查询",{"2":{"37":1}}],["以及训练速度",{"2":{"27":1}}],["以及用可逆神经网络",{"2":{"24":1}}],["以有效利用vfms的强大能力",{"2":{"27":1}}],["以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机",{"2":{"27":1}}],["以高效利用vfms解决dgss问题",{"2":{"27":1}}],["以应对少样本语义分割",{"2":{"25":1}}],["以往少样本语义分割方法存在语义模糊和类间相似性问题",{"2":{"63":1}}],["以往dgss方法着重提升模型在多未见领域的预测准确性",{"2":{"27":1}}],["以往工作依赖预训练骨干网络直接提取的特征",{"2":{"21":1}}],["以往的分割细化方法",{"2":{"29":1}}],["以往的fss方法由于固有偏差",{"2":{"18":1}}],["以往的研究已经证明了特征转换在解决cd",{"2":{"8":1}}],["以解决新数据分布下的一系列下游分割问题",{"2":{"90":1}}],["以解决现有方法的不足",{"2":{"83":1}}],["以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战",{"2":{"57":1}}],["以解决现有fss模型存在的固有偏差",{"2":{"21":1}}],["以解决现有fss模型存在的问题",{"2":{"19":1}}],["以解决特征变换过度依赖支持图像",{"2":{"9":1}}],["策略",{"2":{"8":1}}],["此功能不受官方支持",{"2":{"119":3}}],["此属性必须是字符串数组",{"2":{"119":1}}],["此路径为",{"2":{"116":1}}],["此命令作用于",{"2":{"116":1}}],["此参数为下文进行pdf内部查看和外部查看进行切换的关键参数",{"2":{"116":1}}],["此处就不再赘述",{"2":{"117":1}}],["此处需要您根据自身情况进行路径更改",{"2":{"116":1}}],["此处设置为auto",{"2":{"116":1}}],["此处选择",{"2":{"116":1}}],["此处快捷键的选择为上文设置",{"2":{"111":1}}],["此处笔者使用的为double",{"2":{"108":1}}],["此处为默认配置",{"2":{"108":1}}],["此菜单默认状态下停用",{"2":{"108":1}}],["此项笔者设置为never",{"2":{"108":1}}],["此框架简单且强大",{"2":{"99":1}}],["此前探索了多种图像表示解纠缠方法",{"2":{"39":1}}],["此前采用无监督域适应技术将白天知识迁移到夜间",{"2":{"39":1}}],["此外",{"2":{"8":1,"9":1,"32":1}}],["此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割",{"2":{"8":1}}],["我们可以发现",{"2":{"130":1}}],["我们假设模型的分类数量为",{"2":{"125":1}}],["我们先要对模型有一个大的了解",{"2":{"125":1}}],["我们开发了系统的自监督预训练框架",{"2":{"93":1}}],["我们开发了一种自适应深度白化变换",{"2":{"32":1}}],["我们将在segment",{"2":{"90":1}}],["我们成功构建了迄今为止最大的图像分割数据集",{"2":{"90":1}}],["我们推出了segment",{"2":{"90":1}}],["我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性",{"2":{"82":1}}],["我们通过语言模型生成短文本描述来表征类别语义",{"2":{"73":1}}],["我们通过定量的性能评估和可视化结果",{"2":{"29":1}}],["我们训练的模型在",{"2":{"73":1}}],["我们的框架相比其他主流自监督预训练方法展现出竞争优势",{"2":{"93":1}}],["我们的方法在",{"2":{"82":1}}],["我们的源代码可以通过https",{"2":{"70":1}}],["我们的逐渐融合解码器采用自上而下的结构",{"2":{"70":1}}],["我们在多个任务中测试了该模型",{"2":{"90":1}}],["我们在多个开放词汇语义分割数据集上进行了实验",{"2":{"70":1}}],["我们在解码器中引入了类别早期拒绝方案",{"2":{"70":1}}],["我们在pascal",{"2":{"18":1}}],["我们设计了多层次原型生成与交互模块",{"2":{"62":1}}],["我们设计了一种自适应掩码策略",{"2":{"44":1}}],["我们研发了分组提示调优框架",{"2":{"54":1}}],["我们引入了",{"2":{"39":1}}],["我们提出相关本质特征增强网络",{"2":{"62":1}}],["我们提出将图像内容自适应划分为确定区域",{"2":{"44":1}}],["我们提出的单阶段",{"2":{"44":1}}],["我们提出的",{"2":{"39":1}}],["我们提出的动机是",{"2":{"27":1}}],["我们提出基于通道解耦深度特征的查询机制",{"2":{"32":1}}],["我们提出了",{"2":{"29":1}}],["我们提出了一种高效扩展多领域语义分割数据集的方法",{"2":{"73":1}}],["我们提出了一种高效的微调方法",{"2":{"27":1}}],["我们提出了一种基于transformer的自适应原型匹配网络",{"2":{"18":1}}],["我们首先在领域泛化语义分割",{"2":{"27":1}}],["我们还提出了一种测试时自我微调",{"2":{"8":1}}],["我们利用双重超相关构建",{"2":{"8":1}}],["我们并不完全依赖支持图像",{"2":{"8":1}}],["而对于输入的内部插",{"2":{"130":1}}],["而对于我们大部分人来说",{"2":{"101":1}}],["而实例分割不仅要对像素进行分类",{"2":{"124":1}}],["而never命令做不到这一点",{"2":{"108":1}}],["而有时候将",{"2":{"108":1}}],["而编译链就解决了这个问题",{"2":{"108":1}}],["而使用",{"2":{"108":1}}],["而",{"2":{"108":1}}],["而command为在该拓展中的编译方式",{"2":{"108":1}}],["而第二个选项为进行正向同步",{"2":{"108":1}}],["而每个代码块儿的最后一句是不需要加上",{"2":{"107":1}}],["而言不那么直观",{"2":{"105":1}}],["而代码设置页面虽然相对",{"2":{"105":1}}],["而visual",{"2":{"100":1}}],["而选择一个比较好的编译器是很重要的",{"2":{"100":1}}],["而非高iou的交互式分割",{"2":{"90":1}}],["而非数据的多次采样",{"2":{"67":1}}],["而不是",{"2":{"116":1}}],["而不是传统的transformer",{"2":{"70":1}}],["而不同跨域模式又可能共存于同一通道",{"2":{"32":1}}],["而是学习内容和光照的纠缠表示",{"2":{"39":1}}],["而是提出了自匹配转换",{"2":{"8":1}}],["而且对非拉丁字体的支持更好",{"2":{"108":1}}],["而且",{"2":{"39":1}}],["而且无需使用任何真实的城市场景数据集",{"2":{"27":1}}],["而rdwt通过在计算协方差矩阵之前对特征进行归一化",{"2":{"35":1}}],["而处理超高分辨率细化的级联解码器方法",{"2":{"29":1}}],["而大规模vfms虽在计算机视觉挑战中表现出色",{"2":{"27":1}}],["而双语义感知注意力机制通过可学习的方式减轻背景干扰",{"2":{"24":1}}],["而重复利用少量支持图像来处理每个类别",{"2":{"8":1}}],["容易导致过拟合",{"2":{"8":1}}],["翻译",{"0":{"8":1,"18":1,"32":1,"44":1,"54":1,"62":1,"73":1,"82":1,"93":1},"2":{"27":1,"29":1,"39":1,"41":1,"70":1,"90":1}}],["gz",{"2":{"116":2}}],["geometry",{"2":{"110":1}}],["generating",{"2":{"92":1}}],["generation",{"2":{"61":1,"70":2}}],["generator",{"2":{"41":1}}],["generate",{"2":{"17":1,"41":1}}],["generated",{"2":{"7":1,"81":1}}],["generalize",{"2":{"31":1}}],["generalized",{"0":{"26":1,"30":1},"1":{"27":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"7":1,"27":1,"31":4}}],["generalization",{"2":{"29":1,"41":1,"72":2,"90":1}}],["generaliz",{"2":{"27":1}}],["gls",{"2":{"107":1,"108":1,"119":1}}],["glo",{"2":{"107":1,"108":1,"119":1}}],["global",{"2":{"43":1,"61":1,"92":2}}],["glg",{"2":{"107":1,"108":1,"119":1}}],["gpu",{"2":{"70":1}}],["gpu内存使用和模型存储要求",{"2":{"27":1}}],["gt",{"2":{"70":3,"111":2,"130":1}}],["gradual",{"2":{"70":3}}],["grained",{"2":{"61":1,"92":2}}],["groups=1",{"2":{"130":1}}],["groups",{"2":{"70":1,"130":1}}],["group",{"2":{"60":1}}],["grouping",{"0":{"51":1},"1":{"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1},"2":{"53":3,"57":1,"59":1}}],["gopt在准确性和效率上达到了最佳平衡",{"2":{"59":1}}],["gopt",{"2":{"53":2,"54":2,"57":1}}],["gmp",{"2":{"49":1}}],["g",{"2":{"41":1,"43":2,"92":1,"115":1,"116":1,"119":1}}],["guides",{"2":{"31":1}}],["guangxi",{"2":{"30":1}}],["gap性能优于gmp",{"2":{"49":1}}],["gap",{"2":{"29":1,"49":1,"96":1}}],["git下载",{"2":{"39":1}}],["git访问",{"2":{"27":1}}],["git",{"2":{"27":2,"39":1}}],["github",{"2":{"7":1,"8":1,"27":3,"29":2,"31":1,"32":1,"39":2,"70":2,"72":1,"73":1,"81":1,"82":1,"108":1,"110":1}}],["hello",{"2":{"110":1}}],["heiti",{"2":{"110":1}}],["here",{"2":{"102":1,"108":1,"110":1,"113":1}}],["http",{"2":{"101":1}}],["https",{"2":{"7":1,"8":1,"27":2,"29":1,"31":1,"32":1,"39":1,"70":1,"72":1,"73":1,"81":1,"82":1,"90":1,"101":1}}],["html",{"2":{"101":2}}],["hin−1",{"2":{"130":1}}],["hinder",{"2":{"72":1}}],["hierarchical",{"2":{"70":5,"92":1}}],["highlighted",{"2":{"157":2}}],["highlighting",{"0":{"157":1},"2":{"157":2}}],["highly",{"2":{"81":1}}],["high",{"0":{"28":2},"2":{"29":3,"81":1,"92":1}}],["hout=",{"2":{"130":1}}],["hohai",{"2":{"80":1}}],["how",{"2":{"61":1}}],["however",{"2":{"7":1,"31":1,"41":1,"61":1,"72":1,"81":1,"92":1}}],["hospitals",{"2":{"31":1}}],["human",{"2":{"72":1}}],["humans",{"2":{"29":1,"41":1}}],["hundreds",{"2":{"7":1}}],["has",{"2":{"70":1,"72":1,"81":1,"92":2}}],["hard",{"2":{"59":1}}],["harness",{"2":{"27":2}}],["harnessing",{"0":{"26":1},"1":{"27":1}}],["have",{"2":{"7":1,"17":1,"39":1,"72":1}}],["hypercorrelations",{"2":{"7":1}}],["hypercorrelation",{"2":{"7":1,"11":1}}],["error",{"2":{"107":4,"108":4,"119":4}}],["era",{"2":{"41":1}}],["even",{"2":{"90":1}}],["everingham",{"2":{"72":1,"73":1}}],["evaluate",{"2":{"90":1}}],["evaluation",{"2":{"29":1}}],["eva02和dinov2等五种不同训练策略和数据集的vfms进行评估",{"2":{"27":1}}],["european",{"2":{"72":1}}],["et",{"2":{"72":2,"73":2}}],["equal",{"2":{"72":1}}],["ema",{"2":{"96":1}}],["embedding",{"2":{"72":3,"73":1}}],["embeddings",{"0":{"71":1},"1":{"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1},"2":{"72":1}}],["employs",{"2":{"70":2}}],["elaborated",{"2":{"41":1}}],["either",{"2":{"31":1}}],["e",{"2":{"31":1,"39":1,"41":1,"43":2,"92":1}}],["ec",{"2":{"29":1}}],["enumerate",{"2":{"131":1}}],["engine",{"2":{"90":1}}],["enabled",{"2":{"107":1,"108":1,"119":1}}],["enable",{"2":{"90":1}}],["enables",{"2":{"39":1,"72":3}}],["enabling",{"2":{"81":1}}],["enough",{"2":{"43":1}}],["encoder",{"0":{"69":1},"1":{"70":1},"2":{"41":1,"70":4,"92":1,"96":1}}],["encoders",{"2":{"41":2}}],["entropy",{"2":{"85":1}}],["entire",{"2":{"81":1}}],["entity",{"2":{"29":2}}],["entangled",{"2":{"39":1}}],["end",{"2":{"31":2,"110":2,"131":2}}],["enhancement",{"0":{"40":1,"60":1},"1":{"41":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"17":1,"21":1,"61":1,"62":1,"65":1}}],["enhance",{"2":{"7":1,"17":1,"41":1}}],["effort",{"2":{"72":1}}],["effortlessly",{"2":{"41":1}}],["efficacy",{"2":{"70":1}}],["efficient",{"2":{"41":1,"61":1,"90":1}}],["efficiently",{"2":{"27":2}}],["effective",{"2":{"29":1,"53":1,"81":1}}],["effectiveness",{"2":{"7":1,"17":1,"59":1,"92":1}}],["estimation",{"2":{"72":1}}],["establish",{"2":{"17":1}}],["especially",{"2":{"7":1,"41":1}}],["exe文件所在位置",{"2":{"116":1}}],["exe",{"2":{"115":2,"116":3,"119":2}}],["excluded",{"2":{"61":1}}],["existing",{"2":{"31":1,"39":1,"53":1,"70":2,"81":1}}],["external",{"2":{"115":5,"116":10,"119":7}}],["extensions",{"2":{"156":1,"159":1}}],["extension",{"0":{"156":1},"1":{"157":1,"158":1,"159":1}}],["extensive",{"2":{"7":1,"17":1,"27":1,"31":1,"39":1,"43":1,"53":1}}],["extend",{"2":{"61":1}}],["extremely",{"2":{"92":1}}],["extracting",{"2":{"92":1}}],["extraction",{"2":{"39":1}}],["extract",{"2":{"61":2}}],["extra",{"2":{"27":1,"29":1,"61":1}}],["export",{"2":{"157":1}}],["experimental",{"2":{"81":1}}],["experiments",{"0":{"22":1,"24":1,"37":1,"49":1,"67":1,"78":1,"87":1,"98":1},"1":{"23":1,"24":1},"2":{"7":1,"17":1,"27":1,"31":1,"39":1,"41":2,"43":1,"53":1,"70":1,"90":1}}],["expensive",{"2":{"72":1,"92":1}}],["explicit",{"2":{"53":1}}],["explicitly",{"2":{"39":2}}],["exploring",{"2":{"17":1}}],["explore",{"2":{"7":1,"70":1}}],["exploit",{"2":{"7":1,"41":1}}],["examples",{"0":{"0":1,"156":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"157":1,"158":1,"159":1}}],["early",{"2":{"70":4}}],["easily",{"2":{"7":1}}],["each",{"2":{"7":1,"27":2,"41":1,"53":1,"72":2}}],["warning",{"2":{"107":1,"108":1,"119":1,"158":6}}],["way",{"2":{"31":1}}],["www",{"2":{"101":1}}],["w",{"2":{"98":1}}],["wsss使用如边界框",{"2":{"45":1}}],["wsss的优势与挑战",{"2":{"45":1}}],["wsss",{"2":{"43":3,"44":3,"45":1}}],["wout=",{"2":{"130":1}}],["would",{"2":{"41":1}}],["world",{"2":{"110":1}}],["work",{"2":{"39":1}}],["workshop插件",{"2":{"104":1}}],["workshop",{"2":{"104":1,"107":13,"108":14,"115":6,"116":8,"119":19}}],["workshop安装",{"0":{"104":1}}],["works",{"2":{"0":1,"7":1,"39":1,"92":1}}],["win−1",{"2":{"130":1}}],["windows",{"2":{"101":2}}],["wiki",{"2":{"108":1}}],["widely",{"2":{"92":1}}],["will",{"2":{"90":1}}],["wise",{"2":{"31":2}}],["withspt",{"2":{"41":1}}],["without",{"2":{"27":1,"39":1,"61":1,"70":1}}],["within",{"2":{"27":3,"31":1,"41":1}}],["with",{"0":{"12":1,"22":1,"23":1,"36":1,"39":1,"48":1,"51":1,"66":1,"71":1,"77":1,"80":1,"86":1,"97":1},"1":{"23":1,"24":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"7":2,"27":2,"29":1,"31":3,"39":2,"41":2,"43":4,"70":3,"72":4,"90":2,"92":4,"157":1}}],["wuhan",{"2":{"30":1}}],["w1oves",{"2":{"27":3,"39":2}}],["what",{"2":{"90":3}}],["when",{"2":{"70":1}}],["where",{"2":{"7":1}}],["whu",{"2":{"58":1}}],["whitening",{"2":{"31":1}}],["while",{"2":{"31":1,"39":1,"53":1,"81":1}}],["which",{"2":{"7":1,"41":1,"53":2,"61":1,"70":3,"72":2,"81":1,"92":1}}],["weakly",{"0":{"42":1},"1":{"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1},"2":{"43":2,"44":1}}],["weak",{"2":{"41":1,"53":1,"72":1}}],["well",{"2":{"29":1,"31":1}}],["we",{"2":{"7":4,"17":1,"27":2,"29":2,"31":1,"39":2,"43":3,"53":1,"61":3,"70":2,"72":5,"90":4,"92":1}}],["摘要",{"0":{"7":1,"17":1,"31":1,"43":1,"53":1,"61":1,"72":1,"81":1,"92":1},"2":{"27":1,"29":1,"39":1,"41":1,"70":1,"90":1}}],["xetex",{"2":{"108":1}}],["xelatex",{"2":{"107":9,"108":14,"111":1,"119":9}}],["xb534",{"2":{"70":2}}],["x的resnet",{"2":{"29":1}}],["x",{"2":{"12":1,"131":3}}],["xidian",{"2":{"6":1,"42":1}}],["x3c",{"2":{"0":8}}],["nn",{"2":{"130":1,"131":36}}],["nij表示像素点属于分类i被预测为分类j的像素点数量",{"2":{"128":1}}],["night进行补充实验",{"2":{"39":1}}],["nightcity",{"2":{"39":3}}],["nightlab等",{"2":{"39":1}}],["night",{"0":{"39":1},"2":{"39":6}}],["n",{"2":{"125":4}}],["nlp",{"2":{"90":2}}],["numpy",{"2":{"131":1}}],["numerous",{"2":{"90":1}}],["number",{"2":{"17":1,"59":1,"92":1}}],["nyud",{"2":{"72":1,"73":1}}],["nyudv2",{"2":{"58":1}}],["np",{"2":{"41":1,"131":1}}],["never",{"2":{"107":1,"108":3,"119":1}}],["nevertheless",{"2":{"92":1}}],["need",{"2":{"92":2}}],["needs",{"2":{"29":1}}],["new",{"2":{"41":1,"90":2}}],["negligible",{"2":{"39":1}}],["net及其变体占主导",{"2":{"34":1}}],["network",{"0":{"15":1,"60":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"7":1,"11":1,"17":1,"21":1,"31":1,"39":1,"61":1,"62":1,"65":1}}],["networks",{"2":{"6":1}}],["nerf",{"2":{"29":1}}],["next",{"2":{"27":1}}],["naive",{"2":{"72":1}}],["name",{"2":{"107":10,"108":10,"119":10}}],["named",{"2":{"70":1}}],["namely",{"2":{"27":1}}],["nanjing",{"2":{"6":1,"80":1}}],["now",{"2":{"103":1}}],["noisy",{"2":{"72":1}}],["noise",{"2":{"17":1}}],["no",{"2":{"70":1}}],["not",{"2":{"43":1,"53":1,"72":1}}],["notably",{"2":{"31":1,"61":1}}],["novel",{"2":{"17":1,"39":1,"41":1,"61":1}}],["nw",{"2":{"10":1}}],["lt",{"2":{"53":1}}],["l",{"2":{"41":1,"70":1,"115":1,"116":1,"119":1}}],["loner",{"2":{"110":1}}],["long",{"2":{"31":1}}],["lot",{"2":{"107":1,"108":1,"119":1}}],["lof",{"2":{"107":1,"108":1,"119":1}}],["log",{"2":{"104":1,"107":1,"108":1,"119":1}}],["loop",{"2":{"90":1}}],["loss",{"2":{"85":2}}],["low",{"2":{"29":1}}],["local",{"2":{"17":1,"43":1,"61":1,"70":1}}],["lastused",{"2":{"107":1,"108":2,"119":1}}],["latex配置代码解读",{"0":{"108":1}}],["latex配置代码展示",{"0":{"107":1}}],["latexmk",{"2":{"107":5,"108":6,"119":5}}],["latex环境的代码配置",{"0":{"106":1},"1":{"107":1,"108":1}}],["latex的支持插件",{"0":{"104":1}}],["latex",{"0":{"104":1},"2":{"100":1,"101":1,"104":1,"107":22,"108":23,"110":1,"111":1,"115":6,"116":8,"119":26}}],["language",{"2":{"53":1,"70":1,"72":1,"103":1}}],["largest",{"2":{"90":1}}],["large",{"2":{"41":1,"72":1,"92":1}}],["layer",{"2":{"27":2,"70":1,"131":2}}],["lab",{"2":{"42":1}}],["labeling",{"2":{"72":1}}],["labels",{"2":{"43":2,"72":3}}],["label",{"2":{"41":1,"72":2}}],["labeled",{"2":{"7":1,"61":1,"81":1,"92":1}}],["laboratory",{"2":{"6":1,"39":1}}],["less",{"2":{"72":1}}],["learn",{"2":{"31":2,"39":1,"53":1}}],["learning",{"0":{"30":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"31":2,"53":1,"92":2}}],["leading",{"2":{"43":1}}],["lead",{"2":{"7":1}}],["level",{"2":{"43":2,"61":2,"70":3,"92":1}}],["levels",{"2":{"29":1,"70":1}}],["leverages",{"2":{"81":1}}],["leverage",{"2":{"31":1}}],["leveraging",{"2":{"27":1}}],["like",{"2":{"157":1}}],["lipsum",{"2":{"110":2}}],["liuliang1999",{"2":{"101":1}}],["live",{"0":{"101":1},"2":{"101":5}}],["licensed",{"2":{"90":1}}],["limitations",{"2":{"81":1}}],["limit",{"2":{"61":1}}],["limited",{"2":{"17":1,"61":1,"72":1}}],["lies",{"2":{"53":1}}],["linguistic",{"2":{"41":1}}],["linear",{"2":{"70":1,"92":1}}],["line",{"2":{"41":1,"107":3,"108":3,"115":1,"116":2,"119":5,"157":1}}],["linked",{"2":{"27":1}}],["light",{"2":{"39":2}}],["lighting",{"2":{"39":5}}],["list",{"2":{"5":1,"101":1,"159":1}}],["ui界面设置",{"2":{"118":1}}],["ui",{"2":{"105":4}}],["utilizes",{"2":{"81":1}}],["utilized",{"2":{"72":1}}],["utilization",{"2":{"61":1}}],["utilizing",{"2":{"7":1,"70":1}}],["up",{"0":{"71":1},"1":{"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1},"2":{"31":1,"72":1}}],["upon",{"2":{"27":1}}],["uous",{"2":{"29":1}}],["ultra",{"0":{"28":1},"2":{"29":3}}],["urban",{"2":{"27":1}}],["unet",{"0":{"155":1}}],["unmerging",{"2":{"96":1}}],["unlabeled",{"2":{"61":3,"81":1}}],["uni",{"2":{"53":1,"57":1}}],["university",{"0":{"52":1},"2":{"6":2,"30":2,"39":1,"42":3,"60":1,"80":2}}],["unc",{"2":{"49":3}}],["uncertain",{"2":{"43":2}}],["understanding",{"2":{"92":2}}],["under",{"2":{"39":1}}],["underperforming",{"2":{"39":1}}],["unseen",{"2":{"7":1,"31":1,"41":1,"72":3}}],["usepackage",{"2":{"110":3}}],["used",{"2":{"0":1,"7":1,"39":1}}],["usedata",{"2":{"0":3}}],["using",{"2":{"17":1,"70":1,"72":1,"90":1,"92":1}}],["usage",{"2":{"0":1}}],["🥇",{"0":{"6":1,"37":1,"78":1,"97":1,"98":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1}}],["other",{"2":{"92":2}}],["optional",{"2":{"130":6}}],["optimally",{"2":{"92":1}}],["optimal",{"2":{"53":1}}],["open",{"0":{"69":1},"1":{"70":1},"2":{"70":4}}],["os",{"2":{"58":1}}],["omron",{"2":{"29":1}}],["object",{"2":{"41":1,"43":2}}],["objects",{"2":{"29":1,"41":2}}],["observation",{"2":{"39":1}}],["obtaining",{"2":{"7":1}}],["ously",{"2":{"29":1}}],["our",{"2":{"17":1,"29":2,"39":1,"43":1,"53":1,"70":3,"72":1,"81":1,"90":1,"92":2}}],["output",{"2":{"130":2,"131":6}}],["outperforming",{"2":{"31":1}}],["outperforms",{"2":{"27":1,"39":1,"43":1}}],["outdir",{"2":{"107":1,"108":1,"119":1}}],["outdir=",{"2":{"107":1,"108":1,"119":1}}],["out",{"2":{"5":1,"107":1,"108":1,"115":1,"116":1,"119":2,"130":2,"131":13,"159":1}}],["org",{"2":{"101":1}}],["oriented",{"2":{"39":1}}],["or",{"2":{"7":2,"29":1,"31":1,"90":1,"130":5}}],["over",{"2":{"7":1,"72":2,"90":1}}],["overlooking",{"2":{"7":1}}],["overfitting",{"2":{"7":2}}],["onbuilt",{"2":{"108":1}}],["onsave",{"2":{"108":1}}],["onfilechange",{"2":{"108":1}}],["onfailed",{"2":{"107":1,"108":2,"119":1}}],["one",{"2":{"7":1,"53":1,"64":1}}],["ones",{"2":{"7":1,"29":1}}],["only",{"2":{"7":1,"41":1,"43":1,"53":1}}],["on",{"2":{"7":5,"17":1,"27":1,"29":1,"31":2,"39":3,"41":4,"43":3,"53":2,"61":1,"70":3,"72":7,"81":1,"90":2,"92":3,"116":1,"119":1}}],["often",{"2":{"81":1,"90":1}}],["ofour",{"2":{"70":1}}],["ofdecoder",{"2":{"70":1}}],["ofplain",{"2":{"70":1}}],["ofhumanbeings",{"2":{"41":1}}],["of78",{"2":{"27":1}}],["of",{"0":{"22":1,"23":1,"52":2},"1":{"23":1,"24":1},"2":{"0":2,"5":1,"6":2,"7":5,"17":4,"27":4,"31":3,"39":6,"41":5,"42":1,"43":2,"53":5,"59":3,"60":3,"61":5,"70":3,"72":10,"81":2,"90":1,"92":8,"156":1,"159":1}}],["rcnn",{"2":{"126":1}}],["r",{"2":{"101":1,"115":1,"116":1,"119":1}}],["rmit",{"2":{"80":1}}],["rifenet是一种有效的少样本语义分割模型",{"2":{"68":1}}],["rifenet性能提升显著",{"2":{"68":1}}],["rifenet在基线基础上提高3",{"2":{"67":1}}],["rifenet在保持前景语义一致性方面有显著改进",{"2":{"66":1}}],["rifenet在单样本设置的几乎所有分割中仍比当前最佳的dcama高出0",{"2":{"66":1}}],["rifenet在大多数实验场景下优于最佳方法",{"2":{"66":1}}],["rifenet在pascal",{"2":{"65":1,"68":1}}],["rifenet的损失函数是主损失和自监督损失的加权和",{"2":{"65":1}}],["rifenet由三个共享主干网络的分支组成",{"2":{"65":1}}],["rifenet",{"2":{"61":2,"62":2,"63":1,"65":1,"68":1}}],["row",{"2":{"59":2}}],["robust",{"2":{"17":1,"27":1,"43":1,"61":1,"72":1}}],["rgb",{"2":{"53":1,"54":1,"57":3,"58":1,"125":1}}],["rdwt进一步使dsc提高了1",{"2":{"37":1}}],["rdwt",{"2":{"35":1,"37":1,"38":1}}],["run",{"2":{"107":2,"108":2,"119":2}}],["runner",{"2":{"31":1}}],["runtime",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1,"5":1}}],["randn",{"2":{"131":1}}],["range",{"2":{"31":1,"131":3}}],["ray",{"2":{"12":1}}],["rtd",{"2":{"10":1}}],["return",{"2":{"131":1,"157":2}}],["retinanet",{"2":{"126":1}}],["retaining",{"2":{"53":1}}],["reuse",{"2":{"115":1,"116":1,"119":1}}],["regarded",{"2":{"81":1}}],["regions",{"2":{"39":1,"43":5}}],["rejects",{"2":{"70":1}}],["rejection",{"2":{"70":3}}],["recipe",{"2":{"107":1,"108":1,"119":1}}],["recipes中的第一条编译链",{"2":{"108":1}}],["recipes中内容",{"2":{"108":1}}],["recipes",{"2":{"107":1,"108":3,"119":1}}],["recently",{"2":{"92":1}}],["recent",{"2":{"53":1}}],["recover",{"2":{"43":1}}],["recognize",{"2":{"39":1}}],["recognizes",{"2":{"39":1}}],["reconstruct",{"2":{"29":1}}],["remote",{"2":{"41":1}}],["remarkably",{"2":{"27":1}}],["re",{"2":{"39":1}}],["ref上的",{"2":{"119":1}}],["ref",{"2":{"115":1,"116":3,"119":1}}],["reflectance",{"2":{"39":1}}],["refinenet",{"2":{"126":1}}],["refinement",{"2":{"29":4}}],["refinements",{"2":{"27":1}}],["refines",{"2":{"27":1}}],["reaching",{"2":{"31":1}}],["real",{"2":{"27":1}}],["replace",{"2":{"72":1}}],["representations",{"2":{"53":1,"92":1}}],["representation",{"2":{"31":3,"53":1}}],["repeatedly",{"2":{"7":1}}],["requires",{"2":{"31":1}}],["reinforcement",{"0":{"42":1},"1":{"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1}}],["rein以更少可训练参数显著增强vfms的泛化能力",{"2":{"27":1}}],["rein便在cityscapes数据集上达到了78",{"2":{"27":1}}],["rein显著超越了现有的最先进方法",{"2":{"27":1}}],["rein在微调vfm时",{"2":{"27":1}}],["rein能够在单张图像中为不同的类别生成多样化的细化结果",{"2":{"27":1}}],["rein方法依赖于一组可训练的标记",{"2":{"27":1}}],["rein",{"2":{"27":12}}],["relu",{"2":{"131":13}}],["release",{"2":{"90":1}}],["releasing",{"2":{"90":1}}],["relevant",{"0":{"60":1},"1":{"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"61":1,"62":1,"65":1}}],["relevance",{"2":{"17":1}}],["related",{"2":{"53":1}}],["relationships",{"2":{"17":1}}],["relaxed",{"2":{"31":1}}],["relying",{"2":{"7":1}}],["rely",{"2":{"7":1}}],["resources",{"2":{"115":1,"116":1,"119":1}}],["resolu",{"2":{"29":1}}],["resolution",{"0":{"28":1},"2":{"29":4,"92":1}}],["restart",{"2":{"103":1}}],["rest",{"2":{"31":1}}],["responsible",{"2":{"90":1}}],["responses",{"2":{"31":1}}],["responding",{"2":{"90":1}}],["respecting",{"2":{"90":1}}],["respectively",{"2":{"7":1,"31":1,"72":1}}],["respect",{"2":{"70":1}}],["researchers",{"2":{"81":1}}],["research",{"2":{"29":2,"30":1,"90":1}}],["resulting",{"2":{"43":1,"70":1}}],["result",{"2":{"7":1}}],["results",{"0":{"1":1},"1":{"2":1,"3":1,"4":1},"2":{"0":1,"81":1,"90":1}}],["=",{"2":{"0":1,"41":1,"49":1,"70":3,"131":8}}],["f",{"2":{"115":4,"116":4,"119":4,"131":1}}],["fdb",{"2":{"107":1,"108":1,"119":1}}],["fls",{"2":{"107":1,"108":1,"119":1}}],["flectance",{"2":{"39":1}}],["fg",{"2":{"41":1}}],["fsr通过提高深层的上下文程度有利于语义分割",{"2":{"49":1}}],["fsr时",{"2":{"49":1}}],["fsr分析",{"2":{"49":2}}],["fsr互补",{"2":{"49":1}}],["fsr可以进一步提高伪标签和预测标签的质量",{"2":{"49":1}}],["fsr和cer",{"2":{"49":1}}],["fsr在伪标签和预测标签上都有显著提升",{"2":{"49":1}}],["fsr",{"2":{"48":1,"49":2}}],["fsl",{"2":{"41":2}}],["fss旨在用少量标注样本为新类别生成分割模型",{"2":{"20":1}}],["fss遵循元学习框架",{"2":{"19":1}}],["fss方法在分割过程中只关注前景目标区域",{"2":{"9":1}}],["fss方法patnet通过将特定领域特征转换为领域无关特征来消除领域差距",{"2":{"9":1}}],["fss问题中的有效性",{"2":{"8":1}}],["fss",{"2":{"7":2,"8":1,"9":2,"10":2,"11":1,"12":1,"17":2,"18":1,"19":1,"20":1,"21":1,"25":1,"41":9}}],["f4",{"2":{"37":1}}],["f3",{"2":{"37":1}}],["f2",{"2":{"37":1}}],["f1",{"2":{"37":1}}],["functional",{"2":{"131":1}}],["fundamental",{"2":{"92":1}}],["fundus",{"2":{"31":1}}],["furthermore",{"2":{"39":1,"53":1,"61":1}}],["further",{"2":{"39":1,"43":1}}],["fusion",{"2":{"39":1,"70":3}}],["fully",{"2":{"31":1,"43":1,"90":1}}],["full",{"2":{"5":1,"27":2,"53":1,"159":1}}],["fcn结果",{"0":{"152":1}}],["fcn细节",{"0":{"151":1}}],["fcn基本原理",{"0":{"150":1}}],["fcn模型讲解",{"0":{"131":1}}],["fcn",{"0":{"149":1},"1":{"150":1,"151":1,"152":1},"2":{"29":1,"39":1,"64":1,"126":1,"131":4}}],["false",{"2":{"107":2,"108":2,"119":2}}],["far",{"2":{"90":1}}],["fast",{"2":{"29":1}}],["factors",{"2":{"92":1}}],["fact",{"2":{"29":1}}],["framework",{"2":{"39":1,"53":1,"57":1,"92":2}}],["freeze",{"2":{"27":1}}],["frozen",{"2":{"27":1,"53":1}}],["frontmatter",{"0":{"4":1},"2":{"0":3,"4":1}}],["from",{"0":{"30":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"0":1,"7":1,"27":1,"29":1,"31":2,"61":1,"70":1,"72":1,"81":1}}],["foster",{"2":{"90":1}}],["focuses",{"2":{"43":1}}],["focused",{"2":{"31":1}}],["focusing",{"2":{"41":1}}],["focus",{"2":{"41":1}}],["foundation",{"0":{"26":1},"1":{"27":1},"2":{"27":1,"53":2,"90":2}}],["four",{"2":{"7":1}}],["forward",{"2":{"115":1,"116":1,"119":1,"131":1}}],["forwards",{"2":{"27":1}}],["foraccuracy",{"2":{"43":1}}],["formance",{"2":{"29":1}}],["foreground",{"2":{"7":2,"17":1,"43":1,"61":3}}],["for",{"0":{"15":1,"26":1,"28":1,"40":1,"42":1,"60":1,"69":1,"91":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"27":1,"41":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"70":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"0":1,"5":1,"7":3,"17":1,"27":4,"29":1,"31":1,"39":3,"41":4,"43":3,"53":1,"61":2,"70":3,"81":1,"90":2,"92":5,"103":1,"108":1,"110":2,"131":3,"159":1}}],["figure",{"2":{"59":2}}],["fixed",{"2":{"41":1}}],["filetypes",{"2":{"107":1,"108":1,"119":1}}],["file",{"2":{"107":3,"108":3,"119":3}}],["files",{"2":{"0":1}}],["fill",{"2":{"29":1}}],["first",{"2":{"27":1,"108":1}}],["find",{"2":{"90":1}}],["finely",{"2":{"72":1}}],["fine一起为夜间分割提供了更优的基准",{"2":{"39":1}}],["fine",{"2":{"27":3,"39":5,"41":1,"53":1,"72":1,"92":2}}],["finetuning",{"2":{"7":1,"11":1}}],["finally",{"2":{"17":1}}],["feasible",{"2":{"31":1}}],["fea",{"2":{"29":1}}],["features",{"2":{"7":2,"17":2,"31":3,"39":2,"61":1,"81":1,"92":2,"131":3,"157":1}}],["feature",{"0":{"30":1,"42":1,"60":1,"80":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"7":2,"27":1,"29":1,"31":1,"41":1,"61":1,"62":1,"65":1,"70":1,"81":3,"82":1,"130":2}}],["fewer",{"0":{"26":1},"1":{"27":1},"2":{"27":2}}],["few",{"0":{"6":1,"15":1,"40":1,"60":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"41":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"7":3,"11":1,"17":1,"18":1,"19":1,"20":1,"21":1,"41":2,"61":1}}],["vgg",{"2":{"131":4}}],["vgg16",{"2":{"131":4}}],["v",{"2":{"101":1}}],["v2",{"2":{"72":1,"73":1}}],["vector",{"2":{"72":2}}],["vscode的中文环境需要下载插件来进行支持",{"2":{"103":1}}],["vscode下载与安装",{"0":{"102":1}}],["vscode",{"2":{"100":3,"101":1,"102":2,"103":1,"104":1,"108":2,"110":3,"112":1,"116":1,"117":1}}],["vscode配置latex环境",{"0":{"100":1},"1":{"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1}}],["vs",{"2":{"59":1,"115":1,"116":2,"119":1}}],["vfm",{"2":{"27":1}}],["vfms的潜力与挑战",{"2":{"27":1}}],["vfms",{"2":{"27":7}}],["valued",{"2":{"72":2,"81":1}}],["validation",{"2":{"70":2}}],["variety",{"2":{"92":1}}],["variability",{"2":{"61":1}}],["various",{"2":{"27":2,"39":1,"53":2}}],["varying",{"2":{"39":1,"72":1}}],["va",{"2":{"24":1}}],["voc上",{"2":{"96":1}}],["voc和cityscapes四个常用数据集的多种语义分割和低样本评估指标上达到了最优性能",{"2":{"99":1}}],["voc和cityscapes",{"2":{"96":1}}],["vocabulary",{"0":{"69":1},"1":{"70":1},"2":{"70":3}}],["voc",{"2":{"12":1,"29":2,"43":1,"44":1,"48":2,"50":1,"70":3,"81":1,"82":1,"86":1,"88":1,"92":2,"93":2}}],["vit块",{"2":{"96":1}}],["vit将transformer应用于图像识别",{"2":{"95":1}}],["vit",{"2":{"47":1}}],["vitepress",{"2":{"0":2,"156":1,"157":1}}],["viewer",{"2":{"115":4,"116":4,"119":4}}],["view",{"2":{"43":1,"107":3,"108":1,"115":6,"116":10,"119":11}}],["visi",{"2":{"72":1}}],["vision",{"0":{"26":1,"91":1},"1":{"27":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"27":1,"53":1,"70":1,"72":1,"90":1,"92":2,"96":1}}],["visual",{"2":{"41":1,"102":1,"103":1}}],["visualization",{"2":{"29":1}}],["via",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1},"2":{"31":1}}],["vue",{"2":{"0":1}}],["much",{"2":{"72":1}}],["mul",{"2":{"31":1}}],["multiple",{"2":{"31":1,"70":1,"72":1}}],["multi",{"0":{"51":1,"71":1},"1":{"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1},"2":{"17":1,"43":1,"53":3,"59":1,"61":1,"72":2,"96":1}}],["mfnet",{"2":{"58":1}}],["mca大幅优于gap",{"2":{"49":1}}],["mca",{"2":{"49":1}}],["msg",{"2":{"157":2}}],["ms",{"2":{"43":1,"44":1,"48":2,"70":2}}],["msla",{"2":{"24":2}}],["mba提高",{"2":{"29":1}}],["mba",{"2":{"29":1}}],["microsoft",{"2":{"115":1,"116":2,"119":1}}],["mirror",{"2":{"101":1}}],["miktex",{"2":{"101":1}}],["minimizing",{"2":{"81":1}}],["mined",{"2":{"31":1}}],["million",{"2":{"72":1}}],["millisecond",{"2":{"70":1}}],["misclassified",{"2":{"43":1}}],["mimics",{"2":{"41":1}}],["miou=1ncl∑inii",{"2":{"128":1}}],["miou和2",{"2":{"98":1}}],["miou达到81",{"2":{"87":1}}],["miou的增加趋于饱和",{"2":{"87":1}}],["miou比基线提高了10",{"2":{"87":1}}],["miou精度反而下降",{"2":{"41":1}}],["miou精度随之增加",{"2":{"41":1}}],["miou",{"2":{"27":1,"41":1,"70":1,"72":1,"73":1,"92":1,"93":1,"96":1,"97":3,"98":1}}],["mitigates",{"2":{"17":1}}],["mlp",{"2":{"21":1,"24":2}}],["mean",{"2":{"128":2}}],["message",{"2":{"104":1,"107":2,"108":2,"119":2}}],["mevt比之前最好的方法ibot高出2",{"2":{"97":1}}],["mevt比基于transformer的dino方法高出8",{"2":{"97":1}}],["mevt在低分辨率阶段使用全局自注意力进行多尺度信息融合",{"2":{"98":1}}],["mevt在各种监督水平下均优于现有方法",{"2":{"97":1}}],["mevt在各种设置",{"2":{"96":1}}],["mevt在大多数数据集上优于所有基线方法",{"2":{"97":1}}],["mevt在复杂环境中识别小物体的能力优于其他基线方法",{"2":{"96":1}}],["mevt在线性探测中比其他方法提高了+1",{"2":{"96":1}}],["mevt在预训练架构中引入了多尺度解码器",{"2":{"96":1}}],["mevt使用图像级自蒸馏损失",{"2":{"96":1}}],["mevt采用了一种简单而有效的混合注意力策略",{"2":{"96":1}}],["mevt",{"2":{"92":2,"93":2,"96":1}}],["melbourne",{"2":{"80":1}}],["merged",{"2":{"72":1}}],["merging",{"2":{"72":2}}],["medical",{"0":{"30":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"30":1,"31":1,"41":1}}],["mechanism",{"2":{"17":1,"31":1}}],["metric",{"2":{"31":1}}],["method",{"2":{"29":1,"43":1,"61":1,"70":2,"72":1,"81":2}}],["methods",{"2":{"17":1,"27":1,"31":1,"39":3,"43":1,"53":1,"61":2,"70":1,"72":2,"81":1,"92":1}}],["meta",{"0":{"90":1},"2":{"7":1}}],["most",{"2":{"39":1,"41":1,"43":1,"70":2,"72":1,"92":1}}],["motivated",{"2":{"29":1}}],["motivation",{"2":{"27":1}}],["mobilenetv2等旧骨干网络",{"2":{"27":1}}],["mobilenetv2和resnet等经典骨干网络",{"2":{"27":1}}],["mode=false",{"2":{"131":5}}],["modeling",{"2":{"53":1}}],["model",{"2":{"17":2,"29":2,"31":1,"53":3,"61":1,"70":1,"72":3,"90":10}}],["models",{"0":{"22":1,"26":1},"1":{"23":1,"24":1,"27":1},"2":{"7":1,"27":2,"31":1,"41":2,"53":1,"70":1,"90":2}}],["modality",{"2":{"53":2}}],["modalities",{"2":{"53":2}}],["modal",{"0":{"51":1},"1":{"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1},"2":{"41":1,"53":7,"57":2,"59":1}}],["modules",{"2":{"17":1}}],["module",{"2":{"7":1,"17":4,"21":3,"61":1,"131":1}}],["more",{"0":{"5":1,"159":1},"2":{"7":1,"39":1,"41":1,"108":1}}],["markdown",{"0":{"156":1},"1":{"157":1,"158":1,"159":1},"2":{"156":1,"159":1}}],["margin=1in",{"2":{"110":1}}],["maxpool2d",{"2":{"131":5}}],["macc=1nc1∑iniiti计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均",{"2":{"128":1}}],["maketitle",{"2":{"110":1}}],["made",{"2":{"81":1}}],["manually",{"2":{"72":1}}],["many",{"2":{"70":1,"92":1}}],["manifestation",{"2":{"43":1}}],["manchester",{"2":{"42":1}}],["manner",{"2":{"31":1}}],["masks",{"2":{"90":2}}],["masking",{"2":{"43":1}}],["mask",{"2":{"41":1,"126":1}}],["map",{"2":{"29":1,"70":5,"130":1}}],["maps",{"2":{"7":1,"27":1,"70":1}}],["mae",{"2":{"27":3}}],["ma",{"2":{"24":1}}],["matrices",{"2":{"7":1}}],["matri",{"2":{"7":1}}],["matching",{"0":{"6":1,"15":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1},"2":{"7":2,"11":2,"17":2,"21":1}}],["may",{"2":{"7":1}}],["main",{"2":{"0":1,"29":2,"53":1,"92":1}}],["md```js",{"2":{"157":1}}],["md",{"2":{"0":2,"158":1}}],["iii",{"2":{"92":1,"93":1}}],["ii",{"2":{"92":1,"93":1}}],["ignore",{"2":{"81":1,"92":1}}],["irrelevant",{"2":{"41":1}}],["iaparser",{"2":{"39":4}}],["illumination",{"0":{"39":1},"2":{"39":4}}],["id",{"2":{"124":1}}],["idx",{"2":{"107":1,"108":1,"119":1,"131":2}}],["ideal",{"2":{"31":1}}],["idr",{"2":{"24":1}}],["i",{"2":{"31":1,"39":1,"92":1,"93":1}}],["iou",{"2":{"29":1,"128":4}}],["ist",{"2":{"107":1,"108":1,"119":1}}],["iso",{"2":{"101":2}}],["isic2018",{"2":{"12":1}}],["is",{"2":{"7":1,"27":1,"29":2,"31":4,"41":1,"43":1,"53":4,"61":2,"70":2,"72":4,"81":2,"90":3,"92":3,"110":2,"158":10}}],["issues",{"2":{"53":1,"61":1}}],["issue",{"2":{"7":1,"29":1,"31":1,"43":1}}],["impressive",{"2":{"72":2,"90":1}}],["improvements",{"2":{"72":1}}],["improvement",{"2":{"72":1}}],["improve",{"2":{"61":1}}],["improving",{"2":{"17":1,"53":1}}],["impact",{"2":{"59":1}}],["impeded",{"2":{"39":1}}],["import",{"2":{"0":1,"131":3}}],["imagenet",{"2":{"47":1}}],["image",{"0":{"30":1,"51":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1},"2":{"7":1,"27":1,"29":3,"31":1,"43":2,"53":2,"70":3,"72":1,"81":1,"90":2}}],["images",{"0":{"28":1},"2":{"7":8,"29":2,"31":2,"39":1,"41":1,"72":2,"90":2}}],["im",{"2":{"7":1}}],["inplace=true",{"2":{"131":13}}],["input",{"2":{"70":1,"131":6,"157":1,"158":1}}],["init",{"2":{"131":2}}],["initialize",{"2":{"41":1}}],["inverse",{"2":{"115":1,"116":1,"119":1}}],["invariant",{"2":{"31":1,"39":1}}],["inconsistent",{"2":{"72":1}}],["including",{"2":{"41":1}}],["includes",{"2":{"17":1}}],["info",{"2":{"158":4}}],["information",{"2":{"41":1,"43":1,"53":2,"59":1,"61":1,"70":1,"81":1,"92":1}}],["inference",{"2":{"70":1}}],["ind",{"2":{"107":1,"108":1,"119":1}}],["individual",{"2":{"72":1}}],["individuals",{"2":{"41":1}}],["induced",{"2":{"53":1,"57":1}}],["inherit",{"2":{"53":1}}],["inherent",{"2":{"17":2}}],["inevitably",{"2":{"41":1}}],["inspired",{"2":{"53":1}}],["insufficient",{"2":{"39":1}}],["install",{"2":{"101":1,"103":1,"104":1}}],["instance",{"2":{"72":1,"115":1,"116":1,"119":1}}],["instances",{"2":{"27":1,"61":1}}],["instead",{"2":{"7":1,"39":1,"70":1,"72":1}}],["inn",{"2":{"21":1,"24":2}}],["int",{"2":{"72":1,"130":8}}],["intellisense",{"2":{"107":1,"108":1,"119":1}}],["internal",{"2":{"107":3,"108":1,"119":1}}],["interaction=nonstopmode",{"2":{"107":3,"108":3,"119":3}}],["interaction",{"2":{"61":1}}],["inter",{"2":{"53":1,"61":2}}],["interested",{"2":{"41":1}}],["integrating",{"2":{"53":1}}],["integrate",{"2":{"31":1}}],["integrated",{"2":{"6":1}}],["intrinsic",{"0":{"60":1},"1":{"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"61":3,"62":1,"65":1}}],["introduces",{"2":{"53":1}}],["introduced",{"2":{"41":1,"53":1}}],["introduce",{"2":{"27":1,"39":1,"43":1,"70":1,"90":1,"92":1}}],["intra",{"2":{"7":1,"53":1,"61":1}}],["into",{"2":{"7":1,"17":1,"31":1,"39":1,"43":1,"70":1,"90":1}}],["in",{"2":{"0":1,"7":4,"17":2,"27":2,"29":1,"31":3,"39":3,"41":5,"43":1,"53":4,"70":4,"72":3,"81":2,"90":1,"92":5,"130":2,"131":16,"156":1}}],["its",{"2":{"29":1,"31":1,"90":2}}],["it",{"2":{"0":1,"43":1,"72":1,"81":2,"90":1,"92":1}}],["cmd",{"2":{"119":1}}],["cmd+鼠标左键单击",{"2":{"108":1}}],["c",{"2":{"111":1}}],["ct照片等",{"2":{"135":1}}],["ctex",{"2":{"110":1}}],["ctrl",{"2":{"108":1,"119":1}}],["cnblogs",{"2":{"101":1}}],["cv",{"2":{"41":1}}],["custom",{"0":{"158":1}}],["cup",{"2":{"57":1,"59":1}}],["cues",{"2":{"43":1}}],["cue",{"2":{"39":1}}],["current",{"2":{"0":1,"41":2,"72":1}}],["chinese",{"0":{"52":1},"2":{"60":2,"103":1}}],["china",{"2":{"39":1,"80":1}}],["change",{"2":{"104":1}}],["channels=512",{"2":{"131":11}}],["channels=256",{"2":{"131":6}}],["channels=128",{"2":{"131":4}}],["channels=64",{"2":{"131":4}}],["channels=3",{"2":{"131":1}}],["channels",{"2":{"31":1,"130":4}}],["channel",{"2":{"31":4}}],["challenge",{"2":{"39":1,"43":1,"53":1}}],["challenging",{"2":{"31":1,"72":1}}],["chest",{"2":{"12":1}}],["chenjiayi68",{"2":{"7":1,"8":1}}],["check",{"2":{"5":1,"159":1}}],["ceil",{"2":{"131":5}}],["cer",{"2":{"49":2}}],["certain",{"2":{"43":1}}],["center",{"2":{"30":1}}],["ces",{"2":{"7":1}}],["crm在低分辨率下能细化出较好的通用掩码",{"2":{"29":1}}],["crm在超高分辨率图像上取得最佳分割结果",{"2":{"29":1}}],["crm和推理分辨率",{"2":{"29":1}}],["crm的细化结果包含更多细节",{"2":{"29":1}}],["crm的总推理时间仍不到cascadepsp的一半",{"2":{"29":1}}],["crm比segfix表现更好",{"2":{"29":1}}],["crm采用多分辨率推理策略",{"2":{"29":1}}],["crm找到",{"2":{"29":1}}],["crm展现出了很强的泛化能力",{"2":{"29":1}}],["crm通过不断地对齐特征图和细化目标",{"2":{"29":1}}],["crm",{"2":{"29":10}}],["crop",{"2":{"29":1}}],["cross",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1},"2":{"7":1,"11":1,"31":3,"41":2,"53":1,"57":1,"85":1}}],["clean",{"2":{"107":1,"108":1,"119":1}}],["closeness",{"2":{"72":1}}],["close",{"2":{"72":1}}],["cli",{"2":{"115":1,"116":1,"119":1}}],["click",{"2":{"102":1,"107":3,"108":5,"110":1,"113":1,"119":1}}],["clinically",{"2":{"31":1}}],["clip成功后",{"2":{"70":1}}],["clip",{"2":{"27":1,"70":1}}],["classified",{"2":{"81":1}}],["classification",{"2":{"17":1,"21":1,"61":1}}],["class",{"0":{"40":1},"1":{"41":1},"2":{"7":2,"41":5,"43":1,"53":3,"57":1,"61":4,"72":3}}],["classes",{"2":{"7":1,"17":1,"41":1,"72":1}}],["cityscapes和pascal等数据集上",{"2":{"97":1}}],["cityscapes",{"2":{"27":1,"39":1,"92":1,"93":1}}],["capabilities",{"2":{"90":1}}],["captures",{"2":{"70":1}}],["capture",{"2":{"31":1}}],["cat",{"2":{"70":1}}],["category",{"2":{"70":3}}],["categories",{"2":{"27":1,"43":1,"61":1,"70":3,"72":1}}],["cams本质上存在缺陷",{"2":{"45":1}}],["cams",{"2":{"45":1,"46":1,"47":1}}],["cam和隐式函数",{"2":{"29":1}}],["called",{"2":{"41":1}}],["calculating",{"2":{"7":1}}],["cascadepsp",{"2":{"29":2}}],["cascade",{"2":{"29":1}}],["cannot",{"2":{"29":1}}],["can",{"2":{"0":1,"7":2,"31":2,"39":2,"41":1,"72":1,"90":2}}],["collect",{"2":{"92":1}}],["collection",{"2":{"90":1}}],["collected",{"2":{"31":1}}],["corresponding",{"2":{"90":1}}],["correlation",{"2":{"39":1}}],["cor",{"2":{"90":1}}],["core",{"2":{"53":1}}],["counterparts",{"2":{"43":1}}],["cover",{"2":{"39":1}}],["coarse",{"2":{"29":1}}],["costs",{"2":{"81":1}}],["cost",{"2":{"29":1,"70":5,"81":1}}],["coco数据集",{"0":{"141":1},"2":{"66":1}}],["coco",{"2":{"17":1,"23":1,"43":1,"44":1,"48":2,"50":1,"61":1,"62":1,"66":1,"70":1,"92":1,"93":1,"96":1}}],["connections",{"2":{"96":1}}],["conv2d",{"2":{"131":13}}],["convtranspose2d",{"2":{"130":1}}],["convnext",{"2":{"70":2}}],["conventional",{"2":{"43":1}}],["conducted",{"2":{"43":1}}],["conditions",{"2":{"39":2}}],["conjunction",{"2":{"41":1}}],["conference",{"2":{"72":1}}],["confident",{"2":{"43":3}}],["confirm",{"2":{"17":1}}],["confused",{"2":{"39":1}}],["concretely",{"2":{"39":1}}],["containers",{"0":{"158":1}}],["content",{"2":{"43":1}}],["contextual",{"2":{"92":1}}],["context",{"2":{"17":1,"27":1,"70":2,"72":1,"73":1,"92":1}}],["contrast",{"2":{"41":1}}],["contin",{"2":{"29":1}}],["continuously",{"2":{"29":1}}],["continu",{"2":{"29":1}}],["consists",{"2":{"92":1}}],["consistent",{"2":{"72":1}}],["consistently",{"2":{"39":1}}],["consistency",{"2":{"43":1,"61":1,"85":1}}],["consideration",{"2":{"29":1}}],["consensus",{"2":{"31":1}}],["constrains",{"2":{"43":1}}],["constraints",{"2":{"17":1}}],["constraint",{"2":{"17":1,"21":1}}],["constructs",{"2":{"41":1}}],["construction",{"2":{"7":1,"11":1}}],["construct",{"2":{"7":1}}],["const",{"2":{"0":1}}],["command的参数",{"2":{"116":1,"119":1}}],["command",{"2":{"107":4,"108":4,"115":2,"116":2,"119":6}}],["commonly",{"2":{"43":1}}],["common",{"2":{"29":1,"53":2}}],["com网站发布segment",{"2":{"90":1}}],["comput",{"2":{"72":1}}],["computer",{"2":{"53":1,"72":1,"90":1,"92":1}}],["computational",{"2":{"70":1}}],["computation",{"2":{"29":2,"61":1}}],["competitive",{"2":{"41":1,"90":1,"92":1}}],["component",{"2":{"39":2}}],["components",{"2":{"39":2}}],["comprises",{"2":{"39":1,"70":1}}],["complementarity",{"2":{"61":1}}],["complementary",{"2":{"41":1,"43":1}}],["complexity",{"2":{"43":1,"70":1}}],["completely",{"2":{"7":2}}],["complicated",{"2":{"39":2}}],["comparison",{"0":{"23":1},"2":{"41":1}}],["compared",{"0":{"12":1,"22":1,"36":1,"48":1,"66":1,"77":1,"86":1,"97":1},"1":{"23":1,"24":1},"2":{"41":1,"70":1,"92":1}}],["com",{"2":{"7":1,"8":1,"27":3,"29":2,"31":1,"32":1,"39":2,"70":3,"72":1,"73":1,"81":1,"82":1,"90":2,"101":1,"119":1}}],["code插件",{"2":{"103":1}}],["code呢",{"2":{"100":1}}],["code",{"2":{"7":1,"27":1,"29":1,"31":1,"39":1,"70":1,"72":1,"81":1,"102":1,"115":2,"116":3,"119":2}}],["cd",{"2":{"7":2,"8":1,"9":1,"10":1,"11":1}}],["d",{"2":{"57":1,"58":1,"131":1}}],["dynamic",{"0":{"40":1},"1":{"41":1},"2":{"41":1}}],["dtp显著优于现有技术",{"2":{"39":1}}],["dtp显著超越了现有的先进方法",{"2":{"39":1}}],["dtp可作为即插即用范式",{"2":{"39":1}}],["dtp几乎不增加额外的参数",{"2":{"39":1}}],["dtp包含两个关键技术",{"2":{"39":1}}],["dtp方法首先将夜间图像分解为不受光照影响的反射成分和与光照相关的光照成分",{"2":{"39":1}}],["dtp",{"2":{"39":10}}],["dwt",{"2":{"35":1}}],["dsir方法相比",{"2":{"36":1}}],["dsc指标分别至少高出1",{"2":{"38":1}}],["dsc指标在六个领域中的五个领域超过了所有现有方法",{"2":{"36":1}}],["dsc",{"2":{"31":1,"32":1}}],["dsaa",{"2":{"24":1}}],["dfq框架由分割骨干网络",{"2":{"37":1}}],["dfq",{"2":{"31":2,"32":2}}],["dvlab",{"2":{"29":2}}],["dilation",{"2":{"130":1}}],["dilation=1",{"2":{"130":1,"131":5}}],["diminished",{"2":{"43":1}}],["directly",{"2":{"39":1,"41":1}}],["disable",{"2":{"104":1}}],["disparity",{"2":{"43":1}}],["discriminative",{"2":{"43":1}}],["discrepancy",{"2":{"31":1}}],["disentangles",{"2":{"39":1}}],["disentanglement",{"0":{"39":1},"2":{"39":1}}],["disentangle",{"0":{"39":1},"2":{"39":2}}],["distillation",{"2":{"43":1}}],["distilled",{"2":{"43":1,"44":1}}],["distinguish",{"2":{"29":1,"70":1}}],["distinction",{"2":{"61":1}}],["distinct",{"2":{"27":1}}],["distributions",{"2":{"90":1}}],["distribution",{"2":{"31":1}}],["diverse",{"2":{"27":1}}],["differences",{"2":{"7":1,"61":1}}],["different",{"2":{"7":1,"27":1,"31":2,"41":3,"53":1,"61":1,"70":2,"72":1}}],["dcm将分割任务解耦为语义对齐和空间对齐",{"2":{"25":1}}],["dcm模块将分割任务拆解为语义对齐和空间对齐",{"2":{"18":1}}],["dcm",{"2":{"17":2,"18":1,"21":1,"24":2,"25":1}}],["dcam利用双语义感知注意力机制加强约束",{"2":{"25":1}}],["dcam通过双重语义感知注意力机制解决了注意力偏差问题",{"2":{"18":1}}],["dcam",{"2":{"17":2,"18":1,"21":1,"24":2,"25":1}}],["during",{"2":{"61":1,"81":1}}],["dut",{"2":{"29":1}}],["due",{"2":{"17":1,"31":1,"39":1,"53":1}}],["dual",{"2":{"7":1,"11":1,"17":3,"21":2}}],["def",{"2":{"131":2}}],["default",{"2":{"107":1,"108":1,"119":1,"157":2}}],["dec",{"2":{"98":1}}],["decoder",{"0":{"69":1},"1":{"70":1},"2":{"41":1,"70":5,"92":1,"96":1}}],["decoupled",{"0":{"30":1},"1":{"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1},"2":{"31":3}}],["decouples",{"2":{"17":1}}],["dense",{"2":{"92":1}}],["depth",{"2":{"72":1}}],["dependency",{"2":{"31":1}}],["describe",{"2":{"72":1}}],["describing",{"2":{"72":1}}],["despite",{"2":{"72":1}}],["designed",{"2":{"53":1,"90":1}}],["degradation",{"2":{"70":1}}],["deteriorated",{"2":{"43":1}}],["details",{"2":{"29":1,"108":1,"158":4}}],["deit",{"2":{"41":1}}],["developed",{"2":{"39":1}}],["deal",{"2":{"31":1}}],["deepmask等等",{"2":{"126":1}}],["deep",{"2":{"31":3}}],["deeplab",{"2":{"29":1,"47":1,"126":1}}],["deepglobe",{"2":{"12":1}}],["demonstrate",{"2":{"27":1,"39":1,"43":1,"81":1}}],["demonstrates",{"2":{"0":1,"70":1,"156":1}}],["drawn",{"2":{"72":1}}],["driven",{"2":{"27":1,"41":1}}],["dr",{"2":{"10":1}}],["dressing",{"2":{"7":1}}],["dgss通过归一化和白化",{"2":{"10":1}}],["dgss",{"0":{"26":1},"1":{"27":1},"2":{"10":1,"27":8}}],["dangerous",{"2":{"158":2}}],["danger",{"2":{"158":2}}],["date",{"2":{"90":1,"110":1}}],["dataset",{"2":{"39":1,"72":2,"81":1,"90":4}}],["datasets",{"2":{"7":1,"27":1,"70":1,"72":4,"92":1}}],["data",{"0":{"2":1,"3":1},"2":{"0":3,"53":1,"61":3,"72":1,"90":4,"157":2}}],["day",{"2":{"39":3}}],["dass通过联合使用源域和目标域数据训练模型",{"2":{"10":1}}],["dass",{"2":{"10":1}}],["dhc模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性",{"2":{"14":1}}],["dhc和tsf三个关键模块的有效性",{"2":{"13":1}}],["dhc",{"2":{"7":1,"8":1,"11":1}}],["dmtnet利用smt模块基于自身原型为支持和查询图像计算变换矩阵",{"2":{"14":1}}],["dmtnet在四个具有不同领域差距的数据集上有效",{"2":{"14":1}}],["dmtnet在四个数据集的平均结果上表现优异",{"2":{"12":1}}],["dmtnet在性能上优于现有的最先进方法",{"2":{"8":1}}],["dmtnet",{"2":{"7":3,"8":2,"9":1,"11":1}}],["double",{"2":{"107":3,"108":2,"119":1}}],["doubly",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1},"2":{"7":1,"11":1}}],["document",{"2":{"110":2}}],["documentclass",{"2":{"110":1}}],["documentation",{"2":{"5":1,"159":1}}],["doc表明编译器访问的是没有扩展名的根文件完整路径",{"2":{"108":1}}],["doc",{"2":{"108":1}}],["docfile仅是因为之前使用",{"2":{"108":1}}],["docfile可以将文件所在路径设置为中文",{"2":{"108":1}}],["docfile表明编译器访问没有扩展名的根文件名",{"2":{"108":1}}],["docfile更改为",{"2":{"108":1}}],["docfile",{"2":{"107":4,"108":4,"119":4}}],["download",{"2":{"102":1,"110":1,"113":1}}],["downstream",{"2":{"53":3,"92":1}}],["down",{"2":{"29":1,"70":1}}],["domains",{"2":{"7":2,"31":2,"41":2,"72":4}}],["domain",{"0":{"6":1,"26":1,"71":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"27":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1},"2":{"7":4,"11":1,"27":1,"31":5,"41":1,"72":1}}],["syntax",{"0":{"157":1},"2":{"157":1}}],["synctex的参数",{"2":{"116":1,"119":1}}],["synctex",{"2":{"107":3,"108":1,"115":2,"116":4,"119":3}}],["synctex=1",{"2":{"107":3,"108":3,"119":3}}],["systematic",{"2":{"92":1}}],["skip",{"2":{"96":1}}],["swin",{"2":{"95":1}}],["ssl",{"2":{"92":1,"93":1,"94":1}}],["ssiw",{"2":{"72":1,"73":1}}],["ssd和fss",{"2":{"29":1}}],["s",{"2":{"41":2}}],["sne可视化特征空间",{"2":{"36":1}}],["shiki",{"2":{"157":1}}],["shift",{"2":{"31":1}}],["share",{"2":{"53":1}}],["shanghai",{"2":{"39":1}}],["shallow",{"2":{"31":1}}],["shape",{"2":{"31":1,"125":1}}],["short",{"2":{"72":1}}],["should",{"2":{"43":1}}],["showcontextmenu",{"2":{"107":1,"108":1,"119":1}}],["showing",{"2":{"72":1}}],["shows",{"2":{"29":1,"92":1}}],["shown",{"2":{"17":1,"92":1}}],["show",{"2":{"7":1,"29":1,"31":2,"53":1,"107":2,"108":2,"119":2}}],["shot分割",{"2":{"41":1}}],["shot语义分割",{"2":{"18":1}}],["shot设置下分别提高了3",{"2":{"12":1}}],["shot设置下达到66",{"2":{"12":1}}],["shot设置下达到",{"2":{"12":1}}],["shot和5",{"2":{"12":1}}],["shot",{"0":{"6":1,"15":1,"40":1,"60":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"41":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1},"2":{"7":1,"11":1,"17":1,"19":1,"20":1,"21":1,"41":3,"61":1,"72":1,"73":1,"90":3}}],["sumatrapdf作为自己的",{"2":{"117":1}}],["sumatrapdf下载与安装",{"0":{"113":1}}],["sumatrapdf",{"0":{"112":1,"117":1},"1":{"113":1,"114":1,"115":1,"116":1},"2":{"112":1,"113":1,"115":4,"116":5,"117":2,"119":5}}],["sub",{"2":{"92":1}}],["surrounding",{"2":{"81":1}}],["surpasses",{"2":{"43":1,"61":1}}],["surpassing",{"2":{"27":1}}],["surprisingly",{"2":{"27":1,"41":1}}],["sun",{"2":{"58":1}}],["success",{"2":{"53":1}}],["such",{"2":{"29":1,"31":1,"41":1,"53":1,"72":1}}],["supposed",{"2":{"31":1}}],["support",{"2":{"7":5}}],["super",{"2":{"131":1}}],["supervisory",{"2":{"92":1}}],["supervision",{"2":{"43":2,"81":2}}],["supervised",{"0":{"42":1,"80":1,"91":1},"1":{"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"7":1,"43":1,"44":1,"72":2,"81":2,"82":1,"90":1,"92":4}}],["superiority",{"2":{"53":1}}],["superior",{"0":{"26":1},"1":{"27":1},"2":{"7":1,"27":1,"90":1}}],["silberman",{"2":{"72":1,"73":1}}],["size=2",{"2":{"131":5}}],["size=",{"2":{"131":13}}],["size",{"2":{"70":1,"130":2,"131":1}}],["simplified",{"2":{"103":1}}],["simple",{"0":{"69":1},"1":{"70":1},"2":{"70":1,"72":1}}],["similarity",{"2":{"61":1}}],["similar",{"2":{"31":2,"61":1}}],["signal",{"2":{"92":1}}],["significant",{"2":{"29":1,"31":1,"72":1}}],["significantly",{"2":{"27":1,"39":1}}],["sight",{"2":{"41":1}}],["single",{"2":{"27":1,"43":1,"70":1}}],["site",{"2":{"0":1}}],["sar分割",{"2":{"57":1}}],["sam可能会遗漏图像中的精细结构",{"2":{"90":1}}],["same",{"2":{"31":2,"43":1}}],["sampling",{"2":{"29":1}}],["samples",{"2":{"17":1,"92":1}}],["sam等在计算机视觉挑战中表现出色",{"2":{"27":1}}],["sam等大规模vfms显著提升了计算机视觉任务的性能",{"2":{"27":1}}],["sam",{"2":{"24":1,"27":1,"90":4}}],["sa",{"2":{"24":1,"90":5}}],["scribbling",{"2":{"81":1}}],["scribble",{"0":{"80":1},"1":{"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"81":4,"82":1,"86":1}}],["script>",{"2":{"0":1}}],["script",{"2":{"0":1}}],["score",{"2":{"70":1}}],["scenarios",{"2":{"43":1}}],["scenes",{"2":{"39":2}}],["scene",{"2":{"27":1}}],["sciences",{"0":{"52":1},"2":{"60":2}}],["science",{"2":{"39":1}}],["scheme",{"2":{"31":1,"41":1,"70":1}}],["scaling",{"0":{"71":1},"1":{"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1}}],["scale",{"2":{"17":1,"72":2,"96":1}}],["scarce",{"2":{"53":1}}],["scanners",{"2":{"31":1}}],["springer",{"2":{"72":1}}],["space",{"2":{"72":1}}],["spatial",{"2":{"17":4,"53":1,"70":1}}],["speed",{"2":{"70":1}}],["specifically",{"2":{"53":1}}],["specific",{"2":{"7":3,"39":1,"41":2,"53":2,"61":1,"92":1}}],["spt中的高斯抑制通过调整注意力分布",{"2":{"41":1}}],["spt",{"2":{"41":5}}],["studio",{"2":{"100":1,"102":1,"103":1}}],["study",{"0":{"13":1},"2":{"17":1,"92":1}}],["stuff",{"2":{"70":1,"92":1,"93":1,"96":1,"97":1,"99":1}}],["stride=2",{"2":{"131":5}}],["stride=",{"2":{"131":13}}],["stride=1",{"2":{"130":1}}],["strides=",{"2":{"130":2}}],["stride",{"2":{"130":2}}],["strives",{"2":{"70":1}}],["strong",{"2":{"72":1,"92":1}}],["stronger",{"0":{"26":1},"1":{"27":1},"2":{"27":1}}],["structure",{"2":{"59":1,"70":1}}],["strategies",{"2":{"29":1}}],["strategy",{"2":{"7":1,"43":1,"92":1}}],["strengthen",{"2":{"17":1}}],["start",{"2":{"131":2}}],["statistics",{"2":{"53":1}}],["state",{"0":{"22":1,"23":1},"1":{"23":1,"24":1},"2":{"6":1,"7":1,"27":1,"31":1,"39":1,"41":3,"43":1,"61":1,"72":3,"81":1}}],["standard",{"2":{"41":1,"72":1}}],["stage",{"2":{"7":1,"43":2,"96":4}}],["smt",{"2":{"7":1,"8":1,"11":1}}],["songti",{"2":{"110":1}}],["so",{"2":{"90":1,"92":1}}],["solution",{"2":{"72":1}}],["solve",{"2":{"7":1}}],["soft",{"2":{"59":1}}],["sod",{"2":{"39":4}}],["source",{"2":{"31":3,"70":1}}],["sota",{"0":{"12":1,"36":1,"48":1,"66":1,"77":1,"86":1,"97":1},"2":{"53":1}}],["some",{"2":{"0":1,"39":1,"72":1,"156":1}}],["sequential",{"2":{"131":1}}],["section",{"2":{"110":2}}],["search",{"2":{"115":2,"116":2,"119":2}}],["searchpath",{"2":{"101":1}}],["seamlessly",{"2":{"31":1}}],["sentence",{"0":{"71":1},"1":{"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1},"2":{"72":2}}],["sensing",{"2":{"41":1}}],["sensitivity",{"2":{"17":1}}],["sed在准确性和速度方面均有效",{"2":{"70":1}}],["sed由基于分层编码器的代价图生成和带有类别早期拒绝的渐进式融合解码器组成",{"2":{"70":1}}],["sed下载",{"2":{"70":1}}],["sed方法在ade20k数据集上",{"2":{"70":1}}],["sed包含两部分",{"2":{"70":1}}],["sed",{"0":{"69":1},"1":{"70":1},"2":{"70":4}}],["seek",{"2":{"43":1}}],["separate",{"2":{"43":1}}],["serve",{"2":{"39":1}}],["services",{"2":{"6":1}}],["segnet的基本原理",{"0":{"154":1}}],["segnet",{"0":{"153":1},"1":{"154":1}}],["segformer中的mit",{"2":{"85":1}}],["segfix和crm的细化结果对比",{"2":{"29":1}}],["segfix和mgmatting的性能",{"2":{"29":1}}],["segfix作为高分辨率分割细化方法",{"2":{"29":1}}],["seg",{"2":{"70":1}}],["segmentor作为全景和实体分割的基准方法",{"2":{"29":1}}],["segmenting",{"2":{"17":1}}],["segment",{"0":{"89":1},"1":{"90":1},"2":{"7":2,"29":1,"72":1,"90":5}}],["segmentation",{"0":{"6":1,"15":1,"26":1,"28":1,"30":1,"39":1,"40":1,"42":1,"51":1,"60":1,"69":1,"71":1,"80":1,"91":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"27":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"41":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"70":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"7":2,"11":1,"17":2,"19":1,"20":1,"21":1,"27":1,"29":3,"31":2,"39":4,"41":3,"43":2,"44":1,"53":3,"61":1,"70":5,"72":6,"81":3,"82":1,"90":2,"92":6}}],["setting",{"2":{"41":1}}],["settings",{"2":{"27":1,"39":1}}],["set",{"2":{"27":1,"70":1,"72":1}}],["setup>",{"2":{"0":1}}],["several",{"2":{"7":1}}],["self",{"0":{"42":1,"91":1},"1":{"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"7":3,"11":2,"31":1,"43":2,"44":1,"92":5,"131":7}}],["semantically",{"2":{"39":1}}],["semantics",{"2":{"39":3,"41":1,"43":1,"72":1}}],["semantic",{"0":{"6":1,"15":1,"26":1,"39":1,"42":1,"51":1,"60":1,"69":1,"71":1,"80":1,"91":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"27":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"70":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"7":1,"11":1,"17":4,"20":1,"21":1,"27":1,"39":3,"41":1,"43":3,"44":1,"53":2,"61":3,"70":4,"72":4,"81":3,"82":1,"92":6}}],["a4paper",{"2":{"110":1}}],["author",{"2":{"110":1}}],["auto",{"2":{"115":1,"116":2,"119":1}}],["autoclean",{"2":{"107":1,"108":1,"119":1}}],["autobuild",{"2":{"107":1,"108":1,"119":1}}],["aux",{"2":{"107":1,"108":1,"119":1}}],["australia",{"2":{"80":1}}],["augment",{"2":{"81":1}}],["augmentation",{"0":{"80":1},"1":{"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"81":1}}],["augmented",{"2":{"43":1}}],["a6000",{"2":{"70":1}}],["ai",{"0":{"90":1},"2":{"39":1,"90":1}}],["aid",{"2":{"31":1}}],["aims",{"2":{"7":1,"17":1}}],["ambiguity",{"2":{"61":1}}],["among",{"2":{"29":1,"31":1,"53":1,"81":1}}],["amp",{"0":{"26":1},"1":{"27":1}}],["alg",{"2":{"107":1,"108":1,"119":1}}],["al",{"2":{"72":2,"73":2}}],["ali",{"2":{"110":1}}],["alibaba",{"2":{"60":1}}],["align从噪声图像",{"2":{"70":1}}],["aligns",{"2":{"29":1}}],["alignment",{"2":{"17":2,"53":1,"57":1}}],["although",{"2":{"53":1}}],["alabtion",{"0":{"49":1}}],["allows",{"2":{"61":1}}],["allowing",{"2":{"39":1}}],["allowed",{"2":{"31":1}}],["alleviate",{"2":{"17":1,"43":1,"61":1,"92":1}}],["also",{"2":{"31":1,"43":1,"72":1}}],["aware",{"0":{"40":1},"1":{"41":1},"2":{"17":3,"39":1,"41":1,"53":2,"57":1}}],["annotation",{"2":{"81":1}}],["annotations",{"2":{"72":2,"81":1,"92":1}}],["annotated",{"2":{"17":1,"72":1}}],["anything",{"0":{"89":1},"1":{"90":1},"2":{"90":8}}],["any",{"2":{"27":1,"72":1}}],["an",{"2":{"27":1,"31":2,"39":1,"43":2,"53":1,"61":2,"70":1,"158":2}}],["and",{"0":{"22":1,"40":1},"1":{"23":1,"24":1,"41":1},"2":{"0":2,"6":1,"7":5,"12":1,"17":5,"27":3,"29":6,"31":7,"39":9,"41":9,"43":8,"53":3,"59":1,"61":7,"70":3,"72":8,"90":8,"92":4}}],["abstract",{"2":{"110":2}}],["about",{"2":{"108":1}}],["above",{"2":{"7":1,"92":1}}],["ability",{"2":{"27":1,"29":1}}],["ablation",{"0":{"13":1,"22":1,"24":1,"37":1,"67":1,"78":1,"87":1,"98":1},"1":{"23":1,"24":1},"2":{"41":2}}],["attaining",{"2":{"81":1}}],["attention",{"2":{"17":3,"31":2,"72":1}}],["at",{"2":{"7":1,"27":1,"29":1,"31":1,"39":1,"70":4,"72":2,"81":1,"90":1}}],["available",{"2":{"7":1,"27":1,"29":1,"31":1,"39":1,"70":1,"72":3,"81":1}}],["args",{"2":{"107":4,"108":4,"115":2,"116":2,"119":6}}],["architecture",{"2":{"90":1,"92":1}}],["arbitrary",{"2":{"31":1}}],["article",{"2":{"110":1}}],["arts",{"0":{"23":1},"2":{"41":2}}],["art",{"0":{"22":1},"1":{"23":1,"24":1},"2":{"7":1,"27":1,"31":1,"39":1,"41":1,"43":1,"61":1,"72":3,"81":1}}],["are",{"2":{"7":2,"31":2,"39":1,"41":2,"70":1,"90":1,"92":1}}],["acr",{"2":{"107":1,"108":1,"119":1}}],["across",{"2":{"27":1}}],["acn",{"2":{"107":1,"108":1,"119":1}}],["acquire",{"2":{"101":1}}],["achieve",{"2":{"72":1,"92":1}}],["achieved",{"2":{"72":1}}],["achieves",{"2":{"7":1,"27":1,"41":1,"53":1,"70":1,"72":1,"81":1,"92":1}}],["acp",{"2":{"57":1,"59":1}}],["academy",{"0":{"52":1},"2":{"60":2}}],["activating",{"2":{"41":1}}],["acceleration",{"2":{"70":1}}],["accelerate",{"2":{"70":1}}],["accessing",{"2":{"27":1}}],["access",{"2":{"0":1}}],["accurarcy",{"2":{"128":1}}],["accuracy",{"2":{"29":1,"43":1,"61":1,"70":1,"128":2}}],["accurately",{"2":{"7":1}}],["aggregate",{"2":{"39":1,"53":1}}],["aggregates",{"2":{"29":1}}],["aggregation",{"2":{"17":1,"21":1}}],["age",{"2":{"7":1}}],["agnostic",{"2":{"7":2,"41":1}}],["after",{"2":{"7":1}}],["assignment",{"2":{"59":1}}],["assistance",{"2":{"92":1}}],["assist",{"2":{"53":1}}],["assume",{"2":{"43":1}}],["assess",{"2":{"27":1}}],["aspp",{"2":{"39":1}}],["asd提高了0",{"2":{"37":2}}],["asd改善了0",{"2":{"36":1}}],["as",{"2":{"7":1,"29":1,"31":1,"39":2,"61":1,"72":1,"131":3}}],["astronautics",{"2":{"6":1}}],["ap",{"2":{"29":1}}],["app",{"2":{"115":1,"116":1,"119":1}}],["applied",{"2":{"92":1}}],["applications",{"2":{"72":1,"92":1}}],["appealing",{"2":{"92":1}}],["appearance",{"2":{"7":1}}],["approach",{"2":{"17":1,"27":1,"43":1,"72":1,"81":1}}],["approaches",{"2":{"7":1}}],["apis",{"2":{"0":1,"5":1}}],["api",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1}}],["adjust",{"2":{"101":1}}],["advancde",{"2":{"101":1}}],["advantage",{"2":{"92":1}}],["ade20k",{"2":{"70":3,"92":1,"93":1,"96":1,"97":1,"99":1}}],["adaptation",{"2":{"72":1}}],["adaptability",{"2":{"53":1}}],["adapt",{"2":{"70":1}}],["adapting",{"2":{"53":1}}],["adaptively",{"2":{"41":1,"43":1}}],["adaptive",{"0":{"15":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1},"2":{"17":1,"21":1,"39":1,"43":1}}],["adobe",{"0":{"29":1}}],["address",{"2":{"29":1,"31":1,"72":1,"81":1}}],["addresses",{"2":{"17":1}}],["additionally",{"2":{"72":1}}],["additional",{"2":{"39":1,"157":1}}],["addition",{"2":{"7":1}}],["ad",{"2":{"7":1}}],["a",{"0":{"15":1,"69":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"70":1},"2":{"7":5,"17":3,"27":4,"31":3,"39":2,"41":2,"43":3,"53":1,"61":1,"70":8,"72":10,"81":2,"90":2,"92":7,"98":1,"110":2,"111":2,"158":8}}],["aeronautics",{"2":{"6":1}}],["txt",{"2":{"107":3,"108":10,"110":1,"115":1,"116":6,"119":1}}],["tl",{"2":{"101":1}}],["t的moby方法高出2",{"2":{"97":1}}],["typical",{"2":{"43":1}}],["typically",{"2":{"39":1,"53":1}}],["t",{"2":{"41":1,"57":1,"98":1}}],["two",{"2":{"39":1}}],["true",{"2":{"107":2,"108":2,"119":2}}],["tree",{"2":{"29":2}}],["tranbilchan",{"2":{"82":1}}],["tranquilchan",{"2":{"81":1}}],["transferability",{"2":{"53":1}}],["transfers",{"2":{"41":1}}],["transfer",{"0":{"40":1},"1":{"41":1},"2":{"41":4,"90":1}}],["transformer的窗口注意力块来处理局部信息",{"2":{"96":1}}],["transformer的编码器",{"2":{"85":1}}],["transformer引入卷积风格窗口计算",{"2":{"95":1}}],["transformers",{"0":{"91":1},"1":{"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1}}],["transformer因强大特征表示能力受关注",{"2":{"34":1}}],["transformer因能捕捉长距离相关性",{"2":{"20":1}}],["transformer应用",{"2":{"20":1}}],["transformer",{"0":{"15":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1},"2":{"17":1,"21":1,"31":1,"70":2,"96":1}}],["transform",{"2":{"7":1}}],["transformation",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1},"2":{"7":6,"11":2,"31":1}}],["trade",{"2":{"43":1}}],["training",{"2":{"29":1,"53":1,"70":2,"72":2,"81":1,"92":4}}],["trainable",{"2":{"27":4}}],["trained",{"2":{"27":1,"53":2,"70":1,"90":1}}],["train",{"2":{"7":1}}],["tip",{"2":{"158":4}}],["tiple",{"2":{"31":1}}],["ti+∑jnji−nii",{"2":{"128":1}}],["ti=∑jnij表示标签中所有分类为i的像素点数量",{"2":{"128":1}}],["title",{"2":{"110":1}}],["tion",{"2":{"29":1,"53":1}}],["times",{"2":{"70":1}}],["time",{"0":{"39":1},"2":{"7":1,"11":1,"39":7}}],["tuple",{"2":{"130":5}}],["tug",{"2":{"101":1}}],["tures",{"2":{"29":1}}],["tuning",{"2":{"27":2,"53":2,"57":1,"72":2}}],["tunes",{"2":{"27":1}}],["tune",{"2":{"7":1,"41":2}}],["taborbrowser",{"2":{"116":1}}],["tab",{"2":{"116":1}}],["tableofcontents",{"2":{"110":1}}],["table",{"2":{"59":2}}],["taxonomies",{"2":{"72":1}}],["tackle",{"2":{"39":1}}],["tasks",{"2":{"27":1,"41":1,"53":2,"81":1,"90":2,"92":1}}],["task",{"2":{"17":1,"29":1,"31":1,"39":1,"41":2,"61":1,"70":1,"90":5,"92":1}}],["target",{"2":{"17":1,"21":1,"29":1,"31":1,"41":2}}],["tex的latex文件路径",{"2":{"119":1}}],["tex测试文件下载",{"0":{"110":1}}],["tex文件编译",{"0":{"109":1},"1":{"110":1,"111":1}}],["texstudio",{"2":{"108":1}}],["texworks",{"2":{"101":2}}],["texlive",{"2":{"101":1}}],["tex",{"0":{"101":1,"111":1},"2":{"101":6,"108":7,"110":2,"111":4,"115":1,"116":3,"117":1,"119":2}}],["text",{"2":{"70":1,"73":1}}],["teaches",{"2":{"61":1}}],["tend",{"2":{"41":1}}],["tends",{"2":{"31":1}}],["technology",{"2":{"39":1}}],["technically",{"2":{"31":1}}],["tem通过多尺度局部上下文相关性增强前景特征",{"2":{"25":1}}],["tem通过探索多尺度局部上下文的相关性",{"2":{"18":1}}],["tem旨在减轻骨干网络的固有偏差并增强查询前景区域",{"2":{"24":1}}],["tem和dcam协同作用可提升2",{"2":{"24":1}}],["tem",{"2":{"17":2,"18":1,"21":1,"24":2,"25":1}}],["testfile",{"2":{"110":1}}],["test",{"2":{"7":1,"11":1,"110":1,"131":2}}],["testing",{"2":{"7":1,"29":1,"61":1}}],["tsf策略微调少量参数",{"2":{"14":1}}],["tsf",{"2":{"7":1,"8":1,"11":1}}],["torch",{"2":{"130":1,"131":4}}],["tool是name标签所对应的编译顺序",{"2":{"108":1}}],["tools",{"2":{"107":7,"108":7,"119":7}}],["toc",{"2":{"107":1,"108":1,"119":1}}],["top",{"2":{"70":1}}],["token汇总为一个类token",{"2":{"47":1}}],["tokens",{"2":{"27":1,"53":1}}],["to",{"2":{"0":1,"7":9,"17":7,"27":3,"29":5,"31":11,"39":5,"41":8,"43":6,"53":7,"61":5,"70":7,"72":6,"81":3,"90":5,"92":3,"102":1,"110":1,"113":1}}],["things和stuff",{"2":{"136":1}}],["this",{"2":{"0":1,"7":1,"17":1,"27":2,"31":1,"39":2,"41":1,"43":1,"70":1,"72":2,"81":1,"90":1,"104":1,"108":1,"110":2,"156":1,"158":10}}],["through",{"2":{"17":1,"81":1}}],["three",{"2":{"17":1,"41":1}}],["thousands",{"2":{"7":1}}],["thatworks",{"2":{"41":1}}],["that",{"2":{"7":2,"27":2,"29":2,"39":2,"41":2,"43":5,"70":1,"72":2,"81":3,"90":1}}],["therefrom",{"2":{"72":1}}],["thereby",{"2":{"53":1}}],["their",{"2":{"39":1,"72":1}}],["these",{"2":{"29":1,"61":1,"81":1,"92":1}}],["then",{"0":{"39":1},"2":{"17":1,"31":1,"39":3}}],["themselves",{"2":{"7":1}}],["theme",{"0":{"2":1},"2":{"0":4,"2":1}}],["they",{"2":{"7":1,"92":1}}],["the",{"0":{"22":2,"23":2},"1":{"23":2,"24":2},"2":{"0":3,"5":2,"7":10,"17":7,"27":8,"29":7,"31":13,"39":10,"41":16,"43":8,"53":9,"61":12,"70":7,"72":13,"81":6,"90":5,"92":7,"110":1,"156":1,"159":2}}],["browser",{"2":{"116":1}}],["branch",{"2":{"61":2}}],["blg",{"2":{"107":1,"108":1,"119":1}}],["block",{"2":{"92":1,"158":2}}],["bbl",{"2":{"107":1,"108":1,"119":1}}],["b5时",{"2":{"87":1}}],["b1仍使miou提高了0",{"2":{"88":1}}],["b1骨干网络在全监督数据集上的性能稍弱",{"2":{"86":1}}],["b1作为骨干网络",{"2":{"86":1}}],["b1",{"2":{"85":1}}],["b时",{"2":{"70":1}}],["box",{"2":{"158":2}}],["bool",{"2":{"130":1}}],["boundaries",{"2":{"43":2}}],["both",{"2":{"0":1,"31":1,"92":1}}],["b",{"2":{"41":1,"47":1,"70":2,"111":2}}],["bg",{"2":{"41":1}}],["building",{"2":{"92":1}}],["built",{"2":{"27":1,"90":1,"156":1}}],["but",{"2":{"41":1,"43":1,"72":1}}],["bdd100k",{"2":{"39":2}}],["bib的编译",{"2":{"111":1}}],["bib",{"2":{"108":1}}],["bibtex",{"2":{"107":8,"108":8,"111":1,"119":8}}],["billion",{"2":{"90":1}}],["bine",{"2":{"70":1}}],["biqiwhu",{"2":{"31":1,"32":1}}],["bias=true",{"2":{"130":1}}],["bias",{"2":{"17":6,"130":1}}],["bce",{"2":{"11":1}}],["balance",{"2":{"29":1,"53":1}}],["backbone",{"2":{"27":2,"70":3,"131":2}}],["background",{"2":{"7":2,"17":1,"43":1,"61":2}}],["based",{"0":{"15":1,"80":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"7":3,"11":1,"17":1,"21":1,"39":2,"53":1,"70":3,"72":1,"81":1,"92":1}}],["begin",{"2":{"110":2}}],["because",{"2":{"92":1}}],["better",{"2":{"61":1,"70":1}}],["between",{"2":{"7":1,"29":2,"31":1,"39":1,"43":2,"53":1,"61":3}}],["benchmark",{"2":{"72":1}}],["benchmarks",{"2":{"31":1,"41":1,"61":1}}],["benefit",{"2":{"39":1}}],["being",{"2":{"39":1}}],["been",{"2":{"39":1,"92":1}}],["besides",{"2":{"29":1,"31":1}}],["be",{"2":{"0":1,"31":2,"39":1,"41":1,"43":1,"90":1}}],["by",{"2":{"0":1,"17":2,"27":1,"29":1,"31":2,"39":2,"53":3,"61":1,"72":1,"90":1,"92":1,"156":1,"157":1}}],["pythonimport",{"2":{"131":1}}],["pytorch教程",{"0":{"129":1}}],["pytorch",{"2":{"121":1}}],["pytorch笔记",{"0":{"121":1}}],["pdf查看器用于在",{"2":{"119":1}}],["pdflatex",{"2":{"107":9,"108":13,"119":9}}],["pdf",{"0":{"118":1},"2":{"107":5,"108":7,"111":4,"112":5,"115":8,"116":16,"117":4,"119":15}}],["pfa",{"2":{"81":1,"82":1}}],["pfenet等",{"2":{"64":1}}],["publicly",{"2":{"72":2}}],["pp",{"2":{"70":1,"72":1}}],["p",{"2":{"70":1,"101":1,"119":1}}],["pc",{"2":{"70":2}}],["plain",{"2":{"70":1}}],["pst900",{"2":{"58":1}}],["pspnet",{"2":{"29":1}}],["ptc",{"2":{"47":1}}],["pmg可以将目标对象清晰地划分为不同的互补部分区域",{"2":{"41":1}}],["pmg",{"2":{"41":5}}],["pq",{"2":{"29":1}}],["pixels",{"2":{"70":1,"81":3}}],["pixel",{"2":{"43":1,"61":1,"70":2,"92":1,"128":1}}],["pixelnerf",{"2":{"29":1}}],["ping",{"2":{"29":1}}],["powered",{"2":{"157":1}}],["power",{"2":{"90":1}}],["powerful",{"2":{"41":1,"53":1}}],["poses",{"2":{"43":1}}],["posed",{"2":{"29":1}}],["points",{"2":{"41":1}}],["popular",{"2":{"7":1}}],["peft",{"2":{"27":3}}],["performed",{"2":{"70":1}}],["perform",{"2":{"53":1,"92":1}}],["performance",{"2":{"7":1,"31":1,"41":1,"53":1,"61":1,"72":3,"81":2,"90":1,"92":1}}],["per",{"2":{"29":1,"70":1}}],["perception",{"2":{"17":1,"41":1}}],["padding=",{"2":{"131":13}}],["padding=0",{"2":{"130":2,"131":5}}],["padding",{"2":{"130":6}}],["pacc=∑inii∑iti该指标表示所有像素中分类正确的比例",{"2":{"128":1}}],["package",{"2":{"107":1,"108":1,"119":1}}],["pack",{"2":{"103":1}}],["pasca",{"2":{"70":1}}],["pascal数据集",{"0":{"139":1}}],["pascal−5i",{"2":{"61":1}}],["pascal",{"2":{"12":1,"17":1,"23":1,"43":1,"44":1,"48":2,"62":1,"66":2,"70":4,"72":1,"73":1,"81":1,"82":1,"86":1,"92":2,"93":2,"96":1,"99":1}}],["pas",{"2":{"70":1}}],["pat在三个流行的fss基准测试中创造了新的最优性能",{"2":{"41":1}}],["pat在四个任务中表现优异",{"2":{"41":1}}],["pat与其他fss方法的结合",{"2":{"41":1}}],["pat取得了最佳的分割性能",{"2":{"41":1}}],["pat",{"2":{"41":6}}],["patterns",{"2":{"31":2,"53":1}}],["pattern",{"2":{"31":1,"41":1}}],["patch",{"2":{"29":1,"96":1}}],["patnet等",{"2":{"10":1}}],["partial",{"2":{"85":1}}],["partition",{"2":{"43":1}}],["particular",{"2":{"17":1}}],["part",{"2":{"41":2}}],["paragraphs",{"2":{"72":1}}],["paradigm",{"2":{"39":1,"41":1,"53":1}}],["parameter",{"2":{"27":2}}],["parameters",{"2":{"27":3,"39":1,"53":3}}],["parser",{"2":{"39":1}}],["parse",{"0":{"39":1},"2":{"39":2}}],["paper",{"2":{"7":1,"27":1,"41":1,"70":1,"81":1}}],["page",{"0":{"3":1,"4":1},"2":{"0":7,"3":1,"156":1}}],["print",{"2":{"131":1}}],["privacy",{"2":{"90":1}}],["primary",{"2":{"61":1}}],["prior",{"2":{"39":2,"90":1}}],["practical",{"2":{"31":1}}],["probing",{"2":{"92":1}}],["problem",{"2":{"31":1}}],["project",{"2":{"90":1}}],["properties",{"2":{"92":1}}],["propagation",{"2":{"81":1,"92":1}}],["propagates",{"2":{"81":1}}],["propaga",{"2":{"53":1}}],["proposing",{"2":{"39":1,"61":1}}],["proposes",{"2":{"41":1,"81":1}}],["proposed",{"2":{"31":2,"39":1,"43":1,"61":1}}],["propose",{"2":{"7":3,"17":1,"29":1,"31":1,"43":2,"53":1,"61":2,"70":1,"72":1}}],["progressive",{"0":{"42":1},"1":{"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1}}],["promptable",{"2":{"90":2}}],["prompter",{"2":{"53":2,"57":2,"59":2}}],["prompts",{"2":{"41":3,"53":1}}],["prompting",{"0":{"51":1},"1":{"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1},"2":{"41":2}}],["prompt",{"0":{"40":1},"1":{"41":1},"2":{"41":5,"53":3,"57":1}}],["prostate",{"2":{"31":1}}],["pro",{"2":{"29":1}}],["produces",{"2":{"27":1}}],["processing",{"2":{"39":1,"43":1}}],["process",{"2":{"27":1}}],["prototypes",{"2":{"61":1,"81":1,"82":1}}],["prototype",{"0":{"15":1,"80":1},"1":{"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1},"2":{"17":1,"21":1,"61":1,"81":1,"82":1}}],["provides",{"2":{"157":1}}],["provided",{"2":{"0":1,"156":1}}],["proven",{"2":{"7":1}}],["predefined",{"2":{"72":1}}],["predict",{"2":{"70":1}}],["predictions",{"2":{"39":1}}],["prediction",{"2":{"7":2}}],["preserve",{"2":{"43":1}}],["presents",{"2":{"81":1}}],["present",{"2":{"29":1}}],["precision",{"2":{"43":1}}],["precise",{"2":{"29":1,"39":1}}],["precisely",{"2":{"27":1,"41":1}}],["pretrained",{"2":{"41":1}}],["pre",{"2":{"27":1,"53":2,"70":1,"92":3}}],["prevent",{"2":{"7":1}}],["previous",{"2":{"7":1,"17":1,"61":1}}],["pre>",{"2":{"0":6}}]],"serializationVersion":2}';export{e as default};
