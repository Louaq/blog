const e='{"documentCount":223,"nextId":223,"documentIds":{"0":"/blog/api-examples.html#runtime-api-examples","1":"/blog/api-examples.html#results","2":"/blog/api-examples.html#theme-data","3":"/blog/api-examples.html#page-data","4":"/blog/api-examples.html#page-frontmatter","5":"/blog/api-examples.html#more","6":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation","7":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#南京信息工程大学、青海师范大学、澳门大学、中国科学院","8":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#摘要","9":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#翻译","10":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#研究背景","11":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#研究现状","12":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#提出的模型","13":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#实验-compared-with-the-state-of-the-art-models-and-ablation-experiments","14":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#comparison-with-the-state-of-the-arts","15":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#ablation-experiments","16":"/blog/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.html#结论","17":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#cc4s-encouraging-certainty-and-consistency-in-scribble-supervised-semantic-segmentation","18":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#摘要","19":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#翻译","20":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#研究背景","21":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#研究现状","22":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#提出的模型","23":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#减少神经表示的不确定性","24":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#神经特征空间的自监督学习","25":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#带有颜色约束的伪标签再训练","26":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#实验-compared-with-sota","27":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#实验-ablation-experiments","28":"/blog/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation.html#结论","29":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#corrmatch-label-propagation-via-correlation-matching-for-semi-supervised-semantic-segmentation","30":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#摘要","31":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#翻译","32":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#研究背景","33":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#研究现状","34":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#提出的模型","35":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#实验-compared-with-sota","36":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#实验-ablation-experiments","37":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#其他实验","38":"/blog/column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation.html#结论","39":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation","40":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#摘要","41":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#翻译","42":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#研究背景","43":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#研究现状","44":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#提出的模型","45":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#实验-compared-with-sota","46":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#实验-ablation-study","47":"/blog/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.html#结论","48":"/blog/column/Paper/DGSS.html#stronger-fewer-superior-harnessing-vision-foundation-models-for-domain-generalized-semantic-segmentation-dgss","49":"/blog/column/Paper/DGSS.html#中国科学技术大学-上海人工智能实验室","50":"/blog/column/Paper/High_Quality_Segmentation.html#high-quality-segmentation-for-ultra-high-resolution-images","51":"/blog/column/Paper/High_Quality_Segmentation.html#香港中文大学-adobe-等","52":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#llmformer-large-languagemodel-for-open-vocabulary-semantic-segmentation","53":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#摘要","54":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#翻译","55":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#研究背景","56":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#研究现状","57":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#提出的模型","58":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#实验-compared-with-sota","59":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#实验-ablation-experiments","60":"/blog/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic.html#结论","61":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#learning-generalized-medical-image-segmentation-from-decoupled-feature-queries","62":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#摘要","63":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#翻译","64":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#研究背景","65":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#研究现状","66":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#提出的模型","67":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#实验-compared-with-sota","68":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#实验-ablation-experiments","69":"/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html#结论","70":"/blog/column/Paper/Night-time_Semantic_Segmentation.html#disentangle-then-parse-night-time-semantic-segmentation-with-illumination-disentanglement","71":"/blog/column/Paper/PAT.html#prompt-and-transfer-dynamic-class-aware-enhancement-for-few-shot-segmentation","72":"/blog/column/Paper/PAT.html#中科院","73":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#progressive-feature-self-reinforcement-for-weakly-supervised-semantic-segmentation","74":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#摘要","75":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#翻译","76":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#研究背景","77":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#研究现状","78":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#提出的模型","79":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#实验-compared-with-sota","80":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#实验-alabtion-experiments","81":"/blog/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation.html#结论","82":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#prompting-multi-modal-image-segmentation-with-semantic-grouping","83":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#university-of-chinese-academy-of-sciences","84":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#摘要","85":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#翻译","86":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#研究背景","87":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#研究现状","88":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#提出的模型","89":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#实验过程-与sota方法的对比","90":"/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html#实验过程-消融实验","91":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#relevant-intrinsic-feature-enhancement-network-for-few-shot-semantic-segmentation","92":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#摘要","93":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#翻译","94":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#研究背景","95":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#研究现状","96":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#提出的模型","97":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#实验-compared-with-sota","98":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#实验-ablation-experiments","99":"/blog/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation.html#结论","100":"/blog/column/Paper/SED.html#sed-a-simple-encoder-decoder-for-open-vocabulary-semantic-segmentation","101":"/blog/column/Paper/SED.html#天津大学-重庆大学等","102":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#scaling-up-multi-domain-semantic-segmentation-with-sentence-embeddings","103":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#摘要","104":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#翻译","105":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#研究背景","106":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#研究现状","107":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#提出的模型","108":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#实验-compared-with-sota-and-ablation-experiment","109":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#_1-数据集与实现细节","110":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#_2-实验内容","111":"/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html#结论","112":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#scribble-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label","113":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#摘要","114":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#翻译","115":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#研究背景","116":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#研究现状","117":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#提出的模型","118":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#实验-compared-with-sota","119":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#_1-scribblesup数据集上的比较","120":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#_2-涂鸦收缩和丢弃实验","121":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#实验-ablation-experiments","122":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#_3-组件消融实验","123":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#_4-伪标签消融实验","124":"/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label.html#结论","125":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation","126":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#摘要","127":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#翻译","128":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#研究背景","129":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#研究现状","130":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#提出的模型","131":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#实验过程-compared-with-sota","132":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#实验过程-ablation-experiments","133":"/blog/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.html#结论","134":"/blog/column/Paper/Segment Anything.html#segment-anything","135":"/blog/column/Paper/Segment Anything.html#meta-ai","136":"/blog/column/Paper/Self-supervised_ViT.html#self-supervised-vision-transformers-for-semantic-segmentation","137":"/blog/column/Paper/Self-supervised_ViT.html#摘要","138":"/blog/column/Paper/Self-supervised_ViT.html#翻译","139":"/blog/column/Paper/Self-supervised_ViT.html#研究背景","140":"/blog/column/Paper/Self-supervised_ViT.html#研究现状","141":"/blog/column/Paper/Self-supervised_ViT.html#提出的模型","142":"/blog/column/Paper/Self-supervised_ViT.html#实验-compared-with-sota","143":"/blog/column/Paper/Self-supervised_ViT.html#实验-ablation-experiments","144":"/blog/column/Paper/Self-supervised_ViT.html#结论","145":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#towards-open-vocabulary-semantic-segmentation-without-semantic-labels","146":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#摘要","147":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#翻译","148":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#研究背景","149":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#研究现状","150":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#提出的模型","151":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#实验-compared-with-sota","152":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#实验-ablation-experiments","153":"/blog/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels.html#结论","154":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#use-universal-segment-embeddings-for-open-vocabulary-image-segmentation","155":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#摘要","156":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#翻译","157":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#研究背景","158":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#研究现状","159":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#提出的模型","160":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#实验-compared-with-sota","161":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#实验-ablation-experiments","162":"/blog/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation.html#结论","163":"/blog/column/Paper/Visual Studio Code latex.html#vscode配置latex环境","164":"/blog/column/Paper/Visual Studio Code latex.html#_1-tex-live-下载与安装","165":"/blog/column/Paper/Visual Studio Code latex.html#_2-vscode下载与安装","166":"/blog/column/Paper/Visual Studio Code latex.html#_3-中文语言环境配置","167":"/blog/column/Paper/Visual Studio Code latex.html#_4-latex的支持插件-latex-workshop安装","168":"/blog/column/Paper/Visual Studio Code latex.html#_5-打开latex环境设置页面","169":"/blog/column/Paper/Visual Studio Code latex.html#_6-latex环境的代码配置","170":"/blog/column/Paper/Visual Studio Code latex.html#_6-1-latex配置代码展示","171":"/blog/column/Paper/Visual Studio Code latex.html#_6-2-latex配置代码解读","172":"/blog/column/Paper/Visual Studio Code latex.html#_7-tex文件编译","173":"/blog/column/Paper/Visual Studio Code latex.html#_7-1-tex测试文件下载","174":"/blog/column/Paper/Visual Studio Code latex.html#_7-2-tex-测试文件编译","175":"/blog/column/Paper/Visual Studio Code latex.html#_8-sumatrapdf-安装设置-可选","176":"/blog/column/Paper/Visual Studio Code latex.html#_8-1-sumatrapdf下载与安装","177":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-使用sumatrapdf查看的代码配置","178":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-1-代码展示","179":"/blog/column/Paper/Visual Studio Code latex.html#_8-2-2-代码解读","180":"/blog/column/Paper/Visual Studio Code latex.html#_9-sumatrapdf-的使用","181":"/blog/column/Paper/Visual Studio Code latex.html#_10-pdf-内部查看与外部查看的切换","182":"/blog/column/Paper/Visual Studio Code latex.html#_11-个人完整配置","183":"/blog/column/Paper/#论文阅读笔记","184":"/blog/column/Pytorch/#pytorch笔记","185":"/blog/column/deepLearning/#深度学习笔记","186":"/blog/column/image_segmentation/20250224-语义分割概述.html#语义分割概述","187":"/blog/column/image_segmentation/20250224-语义分割概述.html#_1-什么是语义分割","188":"/blog/column/image_segmentation/20250224-语义分割概述.html#_2-模型的输入和输出","189":"/blog/column/image_segmentation/20250224-语义分割概述.html#_3-常见的分割模型","190":"/blog/column/image_segmentation/20250224-语义分割概述.html#_4-语义分割的思路","191":"/blog/column/image_segmentation/20250224-语义分割概述.html#_5-评价指标","192":"/blog/column/image_segmentation/20250312-Pytorch教程.html#pytorch教程","193":"/blog/column/image_segmentation/220250224-语义分割上采样.html#上采样","194":"/blog/column/image_segmentation/FCN模型讲解.html#fcn模型讲解","195":"/blog/column/image_segmentation/#图像分割学习笔记","196":"/blog/column/image_segmentation/图像分割基础.html#_1-基本概念","197":"/blog/column/image_segmentation/图像分割基础.html#_1-1-什么是图像分割","198":"/blog/column/image_segmentation/图像分割基础.html#_1-2-图像分割的应用场景","199":"/blog/column/image_segmentation/图像分割基础.html#_1-3-图像分割的前景和背景","200":"/blog/column/image_segmentation/图像分割基础.html#_1-4-图像分割的三个层次","201":"/blog/column/image_segmentation/图像分割基础.html#_2-经典数据集","202":"/blog/column/image_segmentation/图像分割基础.html#_2-1-pascal数据集","203":"/blog/column/image_segmentation/图像分割基础.html#_2-1cityscape-用于自动驾驶场景","204":"/blog/column/image_segmentation/图像分割基础.html#_2-3-coco数据集","205":"/blog/column/image_segmentation/图像分割基础.html#_3-评估指标和优化目标","206":"/blog/column/image_segmentation/图像分割基础.html#_3-1-语义分割评估指标","207":"/blog/column/image_segmentation/图像分割基础.html#_3-2-语义分割常用优化目标","208":"/blog/column/image_segmentation/图像分割基础.html#_4-上采样","209":"/blog/column/image_segmentation/图像分割基础.html#_4-1-图像分割网络的两个模块","210":"/blog/column/image_segmentation/图像分割基础.html#_4-2-上采样实现方法-插值法","211":"/blog/column/image_segmentation/图像分割基础.html#_4-3-典型的图像分割网络","212":"/blog/column/image_segmentation/语义分割基础模型.html#fcn","213":"/blog/column/image_segmentation/语义分割基础模型.html#fcn基本原理","214":"/blog/column/image_segmentation/语义分割基础模型.html#fcn细节","215":"/blog/column/image_segmentation/语义分割基础模型.html#fcn结果","216":"/blog/column/image_segmentation/语义分割基础模型.html#segnet","217":"/blog/column/image_segmentation/语义分割基础模型.html#segnet的基本原理","218":"/blog/column/image_segmentation/语义分割基础模型.html#unet","219":"/blog/markdown-examples.html#markdown-extension-examples","220":"/blog/markdown-examples.html#syntax-highlighting","221":"/blog/markdown-examples.html#custom-containers","222":"/blog/markdown-examples.html#more"},"fieldIds":{"title":0,"titles":1,"text":2},"fieldLength":{"0":[3,1,52],"1":[1,3,1],"2":[2,4,2],"3":[2,4,2],"4":[2,4,2],"5":[1,3,11],"6":[12,1,1],"7":[5,12,1],"8":[2,12,103],"9":[2,12,33],"10":[2,12,34],"11":[2,12,16],"12":[2,12,73],"13":[12,12,1],"14":[6,24,6],"15":[2,24,66],"16":[2,12,24],"17":[10,1,5],"18":[1,10,167],"19":[1,10,33],"20":[1,10,30],"21":[1,10,16],"22":[1,10,20],"23":[1,11,1],"24":[1,11,1],"25":[1,11,1],"26":[5,10,23],"27":[4,10,1],"28":[1,10,14],"29":[11,1,8],"30":[1,11,134],"31":[1,11,40],"32":[1,11,35],"33":[1,11,21],"34":[1,11,43],"35":[5,11,41],"36":[4,11,34],"37":[1,1,11],"38":[1,1,29],"39":[11,1,14],"40":[1,11,162],"41":[1,11,43],"42":[1,11,28],"43":[1,11,28],"44":[1,11,55],"45":[5,11,30],"46":[4,11,7],"47":[1,11,16],"48":[15,1,1],"49":[2,15,259],"50":[7,1,1],"51":[3,1,341],"52":[8,1,4],"53":[1,8,130],"54":[1,8,33],"55":[1,8,14],"56":[1,8,20],"57":[1,8,63],"58":[5,8,42],"59":[4,8,42],"60":[1,8,34],"61":[9,1,8],"62":[1,9,164],"63":[1,9,38],"64":[1,9,19],"65":[1,9,15],"66":[1,9,35],"67":[5,9,39],"68":[4,9,25],"69":[1,9,18],"70":[10,1,284],"71":[11,1,1],"72":[1,11,379],"73":[9,1,7],"74":[1,9,130],"75":[1,9,43],"76":[1,9,35],"77":[1,9,14],"78":[1,9,34],"79":[5,9,27],"80":[4,9,58],"81":[1,9,29],"82":[8,1,1],"83":[5,8,1],"84":[2,8,133],"85":[2,8,33],"86":[2,8,25],"87":[2,8,15],"88":[2,8,64],"89":[3,8,10],"90":[3,8,43],"91":[10,1,8],"92":[1,10,109],"93":[1,10,37],"94":[1,10,27],"95":[1,10,26],"96":[1,10,63],"97":[5,10,30],"98":[4,10,25],"99":[1,10,22],"100":[10,1,1],"101":[2,10,332],"102":[9,1,1],"103":[1,9,189],"104":[1,9,56],"105":[1,9,19],"106":[1,9,19],"107":[1,9,24],"108":[8,9,1],"109":[2,16,72],"110":[2,16,55],"111":[1,9,22],"112":[12,1,5],"113":[1,12,139],"114":[1,12,36],"115":[1,12,15],"116":[1,12,17],"117":[1,12,14],"118":[5,12,6],"119":[2,16,27],"120":[2,16,17],"121":[4,12,1],"122":[2,16,29],"123":[2,16,9],"124":[1,12,24],"125":[9,1,8],"126":[1,9,98],"127":[1,9,34],"128":[1,9,31],"129":[1,9,21],"130":[1,9,28],"131":[5,9,14],"132":[4,9,23],"133":[1,9,19],"134":[2,1,1],"135":[2,2,214],"136":[7,1,1],"137":[1,7,135],"138":[1,7,44],"139":[1,7,26],"140":[1,7,19],"141":[1,7,87],"142":[5,7,25],"143":[4,7,33],"144":[1,7,18],"145":[7,1,8],"146":[1,7,108],"147":[1,7,23],"148":[1,7,33],"149":[1,7,16],"150":[1,7,22],"151":[5,7,12],"152":[4,7,8],"153":[1,7,18],"154":[9,1,10],"155":[1,9,119],"156":[1,9,28],"157":[1,9,18],"158":[1,9,23],"159":[1,9,50],"160":[5,9,28],"161":[4,9,10],"162":[1,9,16],"163":[1,1,63],"164":[4,1,132],"165":[2,1,30],"166":[2,1,33],"167":[4,1,34],"168":[2,1,34],"169":[2,1,1],"170":[3,3,105],"171":[3,3,257],"172":[2,1,1],"173":[3,3,75],"174":[4,3,88],"175":[5,1,31],"176":[3,6,11],"177":[3,6,1],"178":[4,7,45],"179":[3,7,105],"180":[3,1,39],"181":[3,1,9],"182":[2,1,158],"183":[1,1,2],"184":[1,1,2],"185":[1,1,3],"186":[1,1,1],"187":[2,1,28],"188":[2,1,28],"189":[2,1,11],"190":[2,1,7],"191":[2,1,29],"192":[1,1,1],"193":[1,1,94],"194":[1,1,105],"195":[1,1,1],"196":[2,1,1],"197":[2,2,2],"198":[3,2,4],"199":[3,2,2],"200":[3,2,1],"201":[2,1,1],"202":[3,2,1],"203":[4,2,1],"204":[3,2,1],"205":[2,1,1],"206":[3,2,1],"207":[3,2,1],"208":[2,1,1],"209":[3,2,1],"210":[4,2,1],"211":[3,2,1],"212":[1,1,1],"213":[1,1,1],"214":[1,1,1],"215":[1,1,1],"216":[1,1,1],"217":[1,1,1],"218":[1,1,1],"219":[3,1,14],"220":[2,3,27],"221":[2,3,21],"222":[1,3,11]},"averageFieldLength":[2.7847533632286994,6.60986547085202,39.46636771300449],"storedFields":{"0":{"title":"Runtime API Examples","titles":[]},"1":{"title":"Results","titles":["Runtime API Examples"]},"2":{"title":"Theme Data","titles":["Runtime API Examples","Results"]},"3":{"title":"Page Data","titles":["Runtime API Examples","Results"]},"4":{"title":"Page Frontmatter","titles":["Runtime API Examples","Results"]},"5":{"title":"More","titles":["Runtime API Examples"]},"6":{"title":"A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","titles":[]},"7":{"title":"南京信息工程大学、青海师范大学、澳门大学、中国科学院  💯","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"8":{"title":"摘要：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"9":{"title":"翻译：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"10":{"title":"研究背景：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"11":{"title":"研究现状：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"12":{"title":"提出的模型：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"13":{"title":"实验（compared with the state-of-the-art models and ablation experiments）","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"14":{"title":"Comparison with the State-of-the-Arts","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","实验（compared with the state-of-the-art models and ablation experiments）"]},"15":{"title":"ablation experiments","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","实验（compared with the state-of-the-art models and ablation experiments）"]},"16":{"title":"结论：","titles":["A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"]},"17":{"title":"CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation","titles":[]},"18":{"title":"摘要","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"19":{"title":"翻译","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"20":{"title":"研究背景","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"21":{"title":"研究现状","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"22":{"title":"提出的模型","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"23":{"title":"减少神经表示的不确定性","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation","提出的模型"]},"24":{"title":"神经特征空间的自监督学习","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation","提出的模型"]},"25":{"title":"带有颜色约束的伪标签再训练","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation","提出的模型"]},"26":{"title":"实验（Compared with SOTA）","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"27":{"title":"实验（Ablation Experiments）🥇","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"28":{"title":"结论","titles":["CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation"]},"29":{"title":"CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation","titles":[]},"30":{"title":"摘要","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"31":{"title":"翻译","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"32":{"title":"研究背景","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"33":{"title":"研究现状","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"34":{"title":"提出的模型","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"35":{"title":"实验（Compared with SOTA）","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"36":{"title":"实验（Ablation Experiments）🥇","titles":["CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation"]},"37":{"title":"其他实验","titles":[]},"38":{"title":"结论","titles":["其他实验"]},"39":{"title":"Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇","titles":[]},"40":{"title":"摘要","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"41":{"title":"翻译","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"42":{"title":"研究背景","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"43":{"title":"研究现状","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"44":{"title":"提出的模型","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"45":{"title":"实验（Compared with SOTA）","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"46":{"title":"实验（Ablation Study）","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"47":{"title":"结论","titles":["Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇"]},"48":{"title":"Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）","titles":[]},"49":{"title":"中国科学技术大学，上海人工智能实验室","titles":["Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）"]},"50":{"title":"High Quality Segmentation for Ultra High-resolution Images","titles":[]},"51":{"title":"香港中文大学  Adobe 等","titles":[]},"52":{"title":"LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation","titles":[]},"53":{"title":"摘要","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"54":{"title":"翻译","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"55":{"title":"研究背景","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"56":{"title":"研究现状","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"57":{"title":"提出的模型","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"58":{"title":"实验（Compared with SOTA）","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"59":{"title":"实验（Ablation Experiments）🥇","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"60":{"title":"结论","titles":["LLMFormer Large LanguageModel for Open-Vocabulary Semantic Segmentation"]},"61":{"title":"Learning Generalized Medical Image Segmentation from Decoupled Feature Queries","titles":[]},"62":{"title":"摘要","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"63":{"title":"翻译","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"64":{"title":"研究背景","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"65":{"title":"研究现状","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"66":{"title":"提出的模型","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"67":{"title":"实验（Compared with SOTA）","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"68":{"title":"实验（Ablation Experiments）🥇","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"69":{"title":"结论","titles":["Learning Generalized Medical Image Segmentation from Decoupled Feature Queries"]},"70":{"title":"Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement","titles":[]},"71":{"title":"Prompt-and-Transfer：Dynamic Class-Aware Enhancement for Few-Shot Segmentation","titles":[]},"72":{"title":"中科院","titles":["Prompt-and-Transfer：Dynamic Class-Aware Enhancement for Few-Shot Segmentation"]},"73":{"title":"Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation","titles":[]},"74":{"title":"摘要","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"75":{"title":"翻译","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"76":{"title":"研究背景","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"77":{"title":"研究现状","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"78":{"title":"提出的模型","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"79":{"title":"实验（Compared with SOTA）","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"80":{"title":"实验（Alabtion Experiments）","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"81":{"title":"结论","titles":["Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation"]},"82":{"title":"Prompting Multi-Modal Image Segmentation with Semantic Grouping","titles":[]},"83":{"title":"University of Chinese Academy of Sciences","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"84":{"title":"摘要：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"85":{"title":"翻译：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"86":{"title":"研究背景：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"87":{"title":"研究现状：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"88":{"title":"提出的模型：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"89":{"title":"实验过程（与SOTA方法的对比）：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"90":{"title":"实验过程（消融实验）：","titles":["Prompting Multi-Modal Image Segmentation with Semantic Grouping"]},"91":{"title":"Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation","titles":[]},"92":{"title":"摘要","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"93":{"title":"翻译","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"94":{"title":"研究背景","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"95":{"title":"研究现状","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"96":{"title":"提出的模型","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"97":{"title":"实验（Compared with SOTA）","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"98":{"title":"实验（Ablation Experiments）","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"99":{"title":"结论","titles":["Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation"]},"100":{"title":"SED:A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation","titles":[]},"101":{"title":"天津大学，重庆大学等","titles":["SED:A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation"]},"102":{"title":"Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings","titles":[]},"103":{"title":"摘要","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"104":{"title":"翻译","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"105":{"title":"研究背景","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"106":{"title":"研究现状","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"107":{"title":"提出的模型","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"108":{"title":"实验（Compared with SOTA and ablation experiment）","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"109":{"title":"1. 数据集与实现细节","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings","实验（Compared with SOTA and ablation experiment）"]},"110":{"title":"2. 实验内容","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings","实验（Compared with SOTA and ablation experiment）"]},"111":{"title":"结论","titles":["Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings"]},"112":{"title":"Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","titles":[]},"113":{"title":"摘要","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"114":{"title":"翻译","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"115":{"title":"研究背景","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"116":{"title":"研究现状","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"117":{"title":"提出的模型","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"118":{"title":"实验（Compared with SOTA）","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"119":{"title":"1. ScribbleSup数据集上的比较","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","实验（Compared with SOTA）"]},"120":{"title":"2. 涂鸦收缩和丢弃实验","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","实验（Compared with SOTA）"]},"121":{"title":"实验（Ablation Experiments）🥇","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"122":{"title":"3. 组件消融实验","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","实验（Ablation Experiments）🥇"]},"123":{"title":"4. 伪标签消融实验","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","实验（Ablation Experiments）🥇"]},"124":{"title":"结论","titles":["Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label"]},"125":{"title":"Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation","titles":[]},"126":{"title":"摘要","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"127":{"title":"翻译","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"128":{"title":"研究背景","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"129":{"title":"研究现状","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"130":{"title":"提出的模型","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"131":{"title":"实验过程（Compared with SOTA）","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"132":{"title":"实验过程（Ablation Experiments）","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"133":{"title":"结论","titles":["Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation"]},"134":{"title":"Segment Anything","titles":[]},"135":{"title":"Meta AI","titles":["Segment Anything"]},"136":{"title":"Self-supervised vision transformers for semantic segmentation","titles":[]},"137":{"title":"摘要","titles":["Self-supervised vision transformers for semantic segmentation"]},"138":{"title":"翻译","titles":["Self-supervised vision transformers for semantic segmentation"]},"139":{"title":"研究背景","titles":["Self-supervised vision transformers for semantic segmentation"]},"140":{"title":"研究现状","titles":["Self-supervised vision transformers for semantic segmentation"]},"141":{"title":"提出的模型","titles":["Self-supervised vision transformers for semantic segmentation"]},"142":{"title":"实验（Compared with SOTA）🥇","titles":["Self-supervised vision transformers for semantic segmentation"]},"143":{"title":"实验（Ablation Experiments）🥇","titles":["Self-supervised vision transformers for semantic segmentation"]},"144":{"title":"结论","titles":["Self-supervised vision transformers for semantic segmentation"]},"145":{"title":"Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels","titles":[]},"146":{"title":"摘要","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"147":{"title":"翻译","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"148":{"title":"研究背景","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"149":{"title":"研究现状","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"150":{"title":"提出的模型","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"151":{"title":"实验（Compared with SOTA）","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"152":{"title":"实验（Ablation Experiments）🥇","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"153":{"title":"结论","titles":["Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels"]},"154":{"title":"USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation","titles":[]},"155":{"title":"摘要","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"156":{"title":"翻译","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"157":{"title":"研究背景","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"158":{"title":"研究现状","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"159":{"title":"提出的模型","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"160":{"title":"实验（Compared with SOTA）","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"161":{"title":"实验（Ablation Experiments）🥇","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"162":{"title":"结论","titles":["USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation"]},"163":{"title":"vscode配置Latex环境","titles":[]},"164":{"title":"1 TeX Live 下载与安装","titles":["vscode配置Latex环境"]},"165":{"title":"2 vscode下载与安装","titles":["vscode配置Latex环境"]},"166":{"title":"3 中文语言环境配置","titles":["vscode配置Latex环境"]},"167":{"title":"4 LaTeX的支持插件 LaTeX Workshop安装","titles":["vscode配置Latex环境"]},"168":{"title":"5 打开LaTeX环境设置页面","titles":["vscode配置Latex环境"]},"169":{"title":"6 LaTeX环境的代码配置","titles":["vscode配置Latex环境"]},"170":{"title":"6.1 LaTeX配置代码展示","titles":["vscode配置Latex环境","6 LaTeX环境的代码配置"]},"171":{"title":"6.2 LaTeX配置代码解读","titles":["vscode配置Latex环境","6 LaTeX环境的代码配置"]},"172":{"title":"7 tex文件编译","titles":["vscode配置Latex环境"]},"173":{"title":"7.1 tex测试文件下载","titles":["vscode配置Latex环境","7 tex文件编译"]},"174":{"title":"7.2 tex 测试文件编译","titles":["vscode配置Latex环境","7 tex文件编译"]},"175":{"title":"8 SumatraPDF 安装设置（可选）","titles":["vscode配置Latex环境"]},"176":{"title":"8.1 SumatraPDF下载与安装","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）"]},"177":{"title":"8.2 使用SumatraPDF查看的代码配置","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）"]},"178":{"title":"8.2.1 代码展示","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）","8.2 使用SumatraPDF查看的代码配置"]},"179":{"title":"8.2.2 代码解读","titles":["vscode配置Latex环境","8 SumatraPDF 安装设置（可选）","8.2 使用SumatraPDF查看的代码配置"]},"180":{"title":"9 SumatraPDF 的使用","titles":["vscode配置Latex环境"]},"181":{"title":"10 pdf 内部查看与外部查看的切换","titles":["vscode配置Latex环境"]},"182":{"title":"11 个人完整配置","titles":["vscode配置Latex环境"]},"183":{"title":"论文阅读笔记","titles":[]},"184":{"title":"Pytorch笔记","titles":[]},"185":{"title":"深度学习笔记","titles":[]},"186":{"title":"语义分割概述","titles":[]},"187":{"title":"1.什么是语义分割","titles":["语义分割概述"]},"188":{"title":"2.模型的输入和输出","titles":["语义分割概述"]},"189":{"title":"3.常见的分割模型","titles":["语义分割概述"]},"190":{"title":"4.语义分割的思路","titles":["语义分割概述"]},"191":{"title":"5.评价指标","titles":["语义分割概述"]},"192":{"title":"Pytorch教程","titles":[]},"193":{"title":"上采样","titles":[]},"194":{"title":"FCN模型讲解","titles":[]},"195":{"title":"图像分割学习笔记","titles":[]},"196":{"title":"1. 基本概念","titles":[]},"197":{"title":"1.1 什么是图像分割","titles":["1. 基本概念"]},"198":{"title":"1.2 图像分割的应用场景","titles":["1. 基本概念"]},"199":{"title":"1.3 图像分割的前景和背景","titles":["1. 基本概念"]},"200":{"title":"1.4 图像分割的三个层次","titles":["1. 基本概念"]},"201":{"title":"2.经典数据集","titles":[]},"202":{"title":"2.1 PASCAL数据集","titles":["2.经典数据集"]},"203":{"title":"2.1Cityscape(用于自动驾驶场景)","titles":["2.经典数据集"]},"204":{"title":"2.3 COCO数据集","titles":["2.经典数据集"]},"205":{"title":"3. 评估指标和优化目标","titles":[]},"206":{"title":"3.1 语义分割评估指标","titles":["3. 评估指标和优化目标"]},"207":{"title":"3.2 语义分割常用优化目标","titles":["3. 评估指标和优化目标"]},"208":{"title":"4. 上采样","titles":[]},"209":{"title":"4.1 图像分割网络的两个模块","titles":["4. 上采样"]},"210":{"title":"4.2 上采样实现方法--插值法","titles":["4. 上采样"]},"211":{"title":"4.3 典型的图像分割网络","titles":["4. 上采样"]},"212":{"title":"FCN","titles":[]},"213":{"title":"FCN基本原理","titles":["FCN"]},"214":{"title":"FCN细节","titles":["FCN"]},"215":{"title":"FCN结果","titles":["FCN"]},"216":{"title":"SegNet","titles":[]},"217":{"title":"SegNet的基本原理","titles":["SegNet"]},"218":{"title":"UNet","titles":[]},"219":{"title":"Markdown Extension Examples","titles":[]},"220":{"title":"Syntax Highlighting","titles":["Markdown Extension Examples"]},"221":{"title":"Custom Containers","titles":["Markdown Extension Examples"]},"222":{"title":"More","titles":["Markdown Extension Examples"]}},"dirtCount":0,"index":[["```",{"2":{"220":1}}],["典型的图像分割网络",{"0":{"211":1}}],["典型例证如",{"2":{"138":1}}],["插值法",{"0":{"210":1}}],["插值等操作",{"2":{"193":1}}],["什么是图像分割",{"0":{"197":1}}],["什么是语义分割",{"0":{"187":1}}],["第五个卷积块",{"2":{"194":1}}],["第四个卷积块",{"2":{"194":1}}],["第三个卷积块",{"2":{"194":1}}],["第二个卷积块",{"2":{"194":1}}],["第一个卷积块",{"2":{"194":1}}],["第一个",{"2":{"171":1}}],["填充",{"2":{"194":1}}],["步长",{"2":{"194":1}}],["步长为0",{"2":{"109":1}}],["舍弃最后的全连接层",{"2":{"194":1}}],["修改而来",{"2":{"194":1}}],["修改如下图",{"2":{"179":1}}],["−2∗padding",{"2":{"193":2}}],["∗stride",{"2":{"193":2}}],["补",{"2":{"193":1}}],["情况下的反卷积则体现为",{"2":{"193":1}}],["卷积核元素之间的间距",{"2":{"193":1}}],["卷积核的大小",{"2":{"193":1}}],["卷积核大小",{"2":{"193":1,"194":1}}],["卷积产生的通道数",{"2":{"193":1}}],["卷积步长",{"2":{"193":2}}],["卷积变压器架构",{"2":{"15":1}}],["小特征图",{"2":{"193":1}}],["小数据集微调评估",{"2":{"110":1}}],["反卷积是一种特殊的正向卷积",{"2":{"193":1}}],["反池化",{"2":{"193":1}}],["反向同步测试",{"2":{"174":1}}],["映射回一个较大的",{"2":{"193":1}}],["概念一致",{"2":{"191":1}}],["评价指标",{"0":{"191":1}}],["评估性能",{"2":{"160":1}}],["评估方法",{"2":{"160":1}}],["评估模型在不同收缩或丢弃比率下的鲁棒性很重要",{"2":{"120":1}}],["评估指标和优化目标",{"0":{"205":1},"1":{"206":1,"207":1}}],["评估指标",{"2":{"109":1}}],["评估设置",{"2":{"51":1}}],["评估并利用视觉基础模型",{"2":{"49":1}}],["评估并应用了多种视觉基础模型",{"2":{"49":1}}],["遍历一些小区域",{"2":{"190":1}}],["滑动窗口的思路可以概括如下",{"2":{"190":1}}],["事实上",{"2":{"188":1}}],["回顾一下之前的全连接网络的分类模型",{"2":{"188":1}}],["那么实际上模型的输出为w×d×n",{"2":{"188":1}}],["那么输出是什么呢",{"2":{"188":1}}],["那么可以点击上图左下角的",{"2":{"164":1}}],["很显然",{"2":{"188":1}}],["很多博主也只是贴上了配置代码",{"2":{"163":1}}],["又要区分出同个类别中的不同实例",{"2":{"187":1}}],["又能促使网络专注于对显著区域生成一致预测",{"2":{"19":1}}],["检测",{"2":{"187":1}}],["检查安装是否正常",{"2":{"164":1}}],["阅读论文",{"2":{"183":1}}],["阅读功能的同时很轻量",{"2":{"175":1}}],["论文阅读笔记",{"0":{"183":1}}],["论文提出的方法显著优于现有最优方法",{"2":{"67":1}}],["非常感谢",{"2":{"182":1}}],["给笔者一点小小的激励",{"2":{"182":1}}],["希望您能够不吝点赞",{"2":{"182":1}}],["希望能够对大家有所帮助",{"2":{"163":1}}],["另",{"2":{"182":1}}],["另一方面",{"2":{"86":1}}],["另一方面下游任务标注数据往往匮乏",{"2":{"85":1}}],["争取以后学得更扎实再编写这些文字",{"2":{"182":1}}],["欢迎您在评论区批评指正",{"2":{"182":1}}],["或双击",{"2":{"182":1}}],["或",{"2":{"181":1}}],["或者复制以下代码进行文档的简单编译测试",{"2":{"173":1}}],["或者在前一页面",{"2":{"164":1}}],["内部查看与外部查看的切换",{"0":{"181":1}}],["内置",{"2":{"179":1}}],["变为了在",{"2":{"180":1}}],["变量有两种",{"2":{"171":1}}],["路径修改",{"2":{"179":1}}],["是一张张的图片",{"2":{"188":1}}],["是一个有前途的发展方向",{"2":{"123":1}}],["是语义分割和实例分割的结合",{"2":{"187":1}}],["是语义分割或全像素语义分割的子类型",{"2":{"187":1}}],["是触发synctex的扩展名为",{"2":{"182":1}}],["是当触发synctex被触发时",{"2":{"179":1}}],["是生成pdf文件的绝对路径的占位符",{"2":{"179":1,"182":1}}],["是行号",{"2":{"179":1,"182":1}}],["是用于生成pdf文件的绝对路径的占位符",{"2":{"179":1,"182":1}}],["请注意中间为",{"2":{"179":1}}],["请记得在最后一句",{"2":{"170":1}}],["链接",{"2":{"179":1,"182":1}}],["命令上的",{"2":{"179":1}}],["让您进行查看",{"2":{"178":1}}],["达到与内置",{"2":{"175":1}}],["达到了当前最优性能",{"2":{"47":1,"124":1}}],["要更加让人舒服一些",{"2":{"175":1}}],["要注意的是",{"2":{"164":1}}],["外部查看器了",{"2":{"180":1}}],["外部查看器展示出来的",{"2":{"175":1}}],["外部查看器的优势是能够看到",{"2":{"175":1}}],["外观和拍摄角度差异较大的情况下",{"2":{"97":1}}],["跳转到对应代码",{"2":{"174":1}}],["跳跃连接",{"2":{"143":1}}],["鼠标左键双击或ctrl+鼠标左键单击",{"2":{"174":1}}],["按ctrl+alt+v",{"2":{"180":1}}],["按ctrl+alt+j",{"2":{"174":1}}],["按win",{"2":{"164":1}}],["正向同步和反向同步",{"2":{"180":1}}],["正向同步测试",{"2":{"174":1}}],["正常情况下只需更改磁盘盘符即可",{"2":{"179":1}}],["正则化方法常忽略利用高层语义信息",{"2":{"128":1}}],["正则化损失",{"2":{"115":1}}],["快捷键",{"2":{"174":1}}],["左侧工具栏",{"2":{"174":1}}],["左边为设置false情况",{"2":{"171":1}}],["符号",{"2":{"174":1}}],["符号时",{"2":{"174":1}}],["符合word设定",{"2":{"173":1}}],["√",{"2":{"174":1}}],["选中想要跳转行",{"2":{"174":1}}],["选中需要跳转的代码所在行",{"2":{"174":1}}],["选中",{"2":{"174":2}}],["选中tex文件的代码页面",{"2":{"174":1}}],["选择第一个latex",{"2":{"167":1}}],["选择第一个chinese",{"2":{"166":1}}],["选择mit",{"2":{"131":1}}],["选择mgmatting作为掩码引导抠图方法",{"2":{"51":1}}],["选择3次",{"2":{"72":1}}],["选择panopticfcn和entity",{"2":{"51":1}}],["选择cascadepsp作为超高清图像的主要对比方法",{"2":{"51":1}}],["选择clip",{"2":{"49":1}}],["选择方法",{"2":{"49":1}}],["世界",{"2":{"173":1}}],["你好",{"2":{"173":1}}],["\\t",{"2":{"173":2}}],["功能是否比较完整",{"2":{"173":1}}],["清除辅助文件",{"2":{"171":1}}],["速度快",{"2":{"171":1}}],["速度提升",{"2":{"101":1}}],["允许用户使用操作系统字体来代替",{"2":{"171":1}}],["区别如下",{"2":{"171":1}}],["区域为不规则的物体边界",{"2":{"191":1}}],["区域级",{"2":{"187":1}}],["区域级自监督预训练方法",{"2":{"140":1}}],["区域传播策略在此基础上分别再提高0",{"2":{"36":1}}],["区域传播策略进一步提高了性能",{"2":{"36":1}}],["区域传播",{"2":{"34":1}}],["则为鼠标左键双击",{"2":{"174":1}}],["则无法进行编译",{"2":{"174":1}}],["则会导致编译不出完整结果甚至编译失败",{"2":{"171":1}}],["则该拓展能够从使用的宏包中自动提取命令和环境",{"2":{"171":1}}],["则安装正常",{"2":{"164":1}}],["菜单中多了两个选项",{"2":{"171":1}}],["右侧就会跳转到相应行",{"2":{"174":1}}],["右边为设置true情况",{"2":{"171":1}}],["右键菜单",{"2":{"182":1}}],["右键以管理员身份运行",{"2":{"164":1}}],["右键",{"2":{"164":1}}],["新的",{"2":{"171":1}}],["编译工具和命令",{"2":{"182":1}}],["编译出错时设置是否弹出气泡设置",{"2":{"182":1}}],["编译成功",{"2":{"174":1}}],["编译bixtex文件",{"2":{"173":1}}],["编译",{"2":{"171":2}}],["编译模式与",{"2":{"171":1}}],["编译器中能够看到的编译顺序",{"2":{"171":1}}],["编译链的存在是为了更方便编译",{"2":{"171":1}}],["编译链中被使用的编译命令",{"2":{"171":1}}],["编译链自动构建",{"2":{"171":1}}],["编码为向量嵌入替代标签",{"2":{"111":1}}],["编码器",{"2":{"78":1,"95":1}}],["切记",{"2":{"170":1}}],["否则就会报错",{"2":{"170":1}}],["否则后期手动添加比较麻烦",{"2":{"164":1}}],["除了代码块儿最后一句",{"2":{"170":1}}],[">",{"2":{"170":4,"171":4,"182":4}}],["查看效果图",{"2":{"180":1}}],["查看",{"2":{"179":1}}],["查看器查看",{"2":{"179":1}}],["查看器",{"2":{"179":1}}],["查看器设置",{"2":{"170":1}}],["查看具有相同的效果",{"2":{"175":1}}],["查询框架基础上增加了一个无标签分支",{"2":{"96":1}}],["查询由深层特征生成",{"2":{"66":1}}],["查询对的密集对应关系",{"2":{"43":1}}],["配置",{"2":{"182":1}}],["配置代码如下",{"2":{"170":1}}],["配备pat提出的动态类别感知编码器后",{"2":{"72":1}}],["处打开",{"2":{"168":1}}],["处理注意力偏差",{"2":{"16":1}}],["转置卷积",{"2":{"193":2}}],["转发到外部查看器时要执行的命令",{"2":{"179":1}}],["转到",{"2":{"168":1}}],["转换为概率",{"2":{"160":1}}],["页面定位到代码相应位置",{"2":{"174":1}}],["页面相应位置",{"2":{"174":1}}],["页面",{"2":{"174":1}}],["页面右下角跳出如下弹窗",{"2":{"167":1}}],["页面和笔者所用图片中展示的页面有略微不同",{"2":{"163":1}}],["记得更改安装路径并记住",{"2":{"176":1}}],["记得修改安装路径",{"2":{"165":1}}],["记为lseg",{"2":{"66":1}}],["官网下载",{"2":{"165":1,"176":1}}],["若使用笔者的代码",{"2":{"174":1}}],["若未选中",{"2":{"174":1}}],["若因网络原因无法连接到github导致无法下载",{"2":{"173":1}}],["若无特殊需求",{"2":{"171":1}}],["若不想了解",{"2":{"167":1}}],["若您感觉此文写得勉强还行",{"2":{"182":1}}],["若您想要更改",{"2":{"174":1}}],["若您想要了解新版本增加的功能",{"2":{"167":1}}],["若您不想要配置外部查看器以及了解内部查看和外部查看之间切换操作",{"2":{"170":1}}],["若您的",{"2":{"163":1}}],["若在安装完该插件之后在",{"2":{"167":1}}],["若输出了一些版本信息",{"2":{"164":1}}],["⑩",{"2":{"164":1}}],["几分钟左右",{"2":{"164":1}}],["⑨",{"2":{"164":1}}],["个人完整配置",{"0":{"182":1}}],["个性化安装",{"2":{"164":1}}],["个主流测试集上达到了监督学习的顶尖水平",{"2":{"104":1}}],["⑧",{"2":{"164":1}}],["安装包不到",{"2":{"175":1}}],["安装设置",{"0":{"175":1},"1":{"176":1,"177":1,"178":1,"179":1}}],["安装好之后",{"2":{"165":1}}],["安装",{"2":{"164":1}}],["故而选择xelatex",{"2":{"174":1}}],["故而您可以根据自己的需要更改编译链顺序",{"2":{"171":1}}],["故而笔者使用了onfailed",{"2":{"171":1}}],["故而笔者设置均设置为false",{"2":{"171":1}}],["故而取消",{"2":{"164":1}}],["故此项笔者设置为true",{"2":{"171":1}}],["故笔者写下了此文",{"2":{"163":1}}],["出现如下图页面",{"2":{"180":1}}],["出现编译好的",{"2":{"174":1}}],["出现下图后",{"2":{"164":1}}],["出现基于两阶段和单阶段框架的方法",{"2":{"101":1}}],["⑦",{"2":{"164":1}}],["⑥",{"2":{"164":1,"174":1}}],["⑤",{"2":{"164":1,"166":1,"174":1,"180":1}}],["打开编译出的",{"2":{"180":1}}],["打开测试文件所在文件夹",{"2":{"174":1}}],["打开latex环境设置页面",{"0":{"168":1}}],["打开拓展",{"2":{"166":1,"167":1}}],["打开命令行窗口",{"2":{"164":1}}],["打开运行",{"2":{"164":1}}],["打开",{"2":{"164":1,"165":1}}],["资源管理器",{"2":{"164":1}}],["④",{"2":{"164":1,"166":1,"167":1,"168":1,"174":1,"180":1}}],["直到下载速度在您的可接受范围内即可",{"2":{"164":1}}],["直接上图",{"2":{"163":1}}],["③",{"2":{"164":1,"165":1,"166":1,"167":1,"168":1,"174":1,"180":1}}],["②",{"2":{"164":1,"165":1,"166":1,"167":1,"168":1,"174":1,"180":1}}],["①",{"2":{"164":1,"165":1,"166":1,"167":1,"168":1,"174":1,"180":1}}],["接口与参数说明",{"2":{"193":1}}],["接下来是",{"2":{"164":1}}],["接着就会出现下图",{"2":{"164":1}}],["接着",{"2":{"9":1}}],["系统是",{"2":{"164":1}}],["系列等方法不断发展",{"2":{"51":1}}],["文中如果出现错误的地方",{"2":{"182":1}}],["文档时的默认编译链",{"2":{"171":1}}],["文档编译有时需要用到辅助文件",{"2":{"171":1}}],["文件清理",{"2":{"182":1}}],["文件同步到外部查看器时latex",{"2":{"179":1}}],["文件路径",{"2":{"179":2}}],["文件内后",{"2":{"180":1}}],["文件内",{"2":{"178":1}}],["文件在查看器中的目录",{"2":{"175":1}}],["文件的完整展现效果",{"2":{"175":1}}],["文件可以从",{"2":{"173":1}}],["文件指定位置跳转到",{"2":{"171":1}}],["文件没有正常更新的情况",{"2":{"171":1}}],["文件修改后进行编译时",{"2":{"171":1}}],["文件也会有些字体没有嵌入",{"2":{"171":1}}],["文件默认嵌入所有字体",{"2":{"171":1}}],["文件仍需要根文件完整路径",{"2":{"171":1}}],["文件相应位置",{"2":{"171":1}}],["文件编写规则",{"2":{"170":1}}],["文件中任意的代码",{"2":{"174":2}}],["文件中相应代码所在位置",{"2":{"171":1}}],["文件中",{"2":{"170":1}}],["文件",{"2":{"164":1,"168":1,"171":1,"173":1,"174":2,"180":2}}],["文末有完整的个人配置代码",{"2":{"163":1}}],["文本数据集",{"2":{"158":1}}],["文本数据集中学习",{"2":{"101":1}}],["文本数据中学习",{"2":{"158":1}}],["文本对比损失来训练模型",{"2":{"159":1}}],["文本对来提高模型训练的效率和鲁棒性",{"2":{"158":1}}],["文本对",{"2":{"156":1,"159":2}}],["文本对数据中学习视觉特征",{"2":{"101":1}}],["文本相似度和熵正则化来确定掩码的分配",{"2":{"150":1}}],["文本到掩码任务的探索还不够成熟",{"2":{"135":1}}],["文本到掩码任务待完善",{"2":{"135":1}}],["文本代价图",{"2":{"101":1}}],["文本编码器冻结",{"2":{"101":1}}],["文本成本图",{"2":{"101":1}}],["您注意更改路径",{"2":{"179":1}}],["您可根据个人适应选择相应的方法",{"2":{"181":1}}],["您可自行选择是否需要设置此部分内容",{"2":{"175":1}}],["您可以通过https",{"2":{"49":1}}],["您无需担心",{"2":{"163":1}}],["注意修改路径",{"2":{"178":3,"179":3,"182":3}}],["注意到",{"2":{"174":1}}],["注意力模块插入和上下文先验等",{"2":{"95":1}}],["注意力机制在目标类别内差异大时",{"2":{"10":1}}],["注意力偏差和空间感知偏差导致的背景干扰问题",{"2":{"16":1}}],["注意力偏差和空间感知偏差导致的对背景噪声敏感的问题",{"2":{"12":1}}],["注意力偏差和空间感知偏差问题",{"2":{"15":1}}],["注意力偏差和空间感知偏差",{"2":{"9":1}}],["注",{"2":{"163":3,"168":2,"170":1,"171":2,"173":1,"174":1,"179":1,"182":1}}],["没有详细的介绍说明",{"2":{"163":1}}],["没有充分利用上下文信息",{"2":{"101":1}}],["找到",{"2":{"164":1}}],["找到下载好的压缩包",{"2":{"164":1}}],["找到最显著的类别",{"2":{"34":1}}],["找了很多资料",{"2":{"163":1}}],["笔者会虚心接受这些产生错误的地方",{"2":{"182":1}}],["笔者也只是一个初学者",{"2":{"182":1}}],["笔者将快捷键设置为ctrl+alt+r",{"2":{"174":1}}],["笔者编写了一份简单的",{"2":{"173":1}}],["笔者选择",{"2":{"175":1}}],["笔者选择使用lastused",{"2":{"171":1}}],["笔者选用的",{"2":{"164":1}}],["笔者此处设置为",{"2":{"171":1}}],["笔者觉得菜单多了此选项较方便",{"2":{"171":1}}],["笔者只对几个要点进行提及",{"2":{"165":1}}],["笔者进入了清华大学镜像网站",{"2":{"164":1}}],["笔者配置了好久",{"2":{"163":1}}],["笔者前期使用的是texstudio进行文档的编译的",{"2":{"163":1}}],["都不清除辅助文件",{"2":{"171":1}}],["都不一样",{"2":{"163":1}}],["都选择清除辅助文件",{"2":{"171":1}}],["都需要加上英文状态下的",{"2":{"170":1}}],["都对分割结果有积极贡献",{"2":{"68":1}}],["颜值也很高",{"2":{"163":1}}],["话不多说",{"2":{"163":1}}],["头秃",{"2":{"163":1}}],["至少对笔者而言是如此",{"2":{"163":1}}],["秃头专业",{"2":{"163":1}}],["段嵌入头",{"2":{"159":1}}],["合并段",{"2":{"159":1}}],["合并了7个高质量语义分割数据集",{"2":{"109":1}}],["改进图像",{"2":{"158":1}}],["端到端方法和两阶段方法",{"2":{"158":1}}],["端到端的方法不能将基础模型生成的图像段作为输入或提示来分配类标签",{"2":{"157":1}}],["端到端微调结果",{"2":{"142":1}}],["简单的说",{"2":{"187":1}}],["简单替换clip即可带来即时改进",{"2":{"153":1}}],["简化训练过程并取得不错效果",{"2":{"77":1}}],["聚类与提示学习作用",{"2":{"153":1}}],["聚合像素上所有片段的概率进行类别预测",{"2":{"160":1}}],["聚合类感知表示并辅助建模公共统计信息",{"2":{"90":1}}],["聚合辅助模态的类感知表示",{"2":{"88":1}}],["聚合模块",{"2":{"78":1}}],["聚合多级别支持掩码等",{"2":{"11":1}}],["消融研究",{"2":{"152":1}}],["消融实验显示",{"2":{"99":1}}],["消融实验表明",{"2":{"81":1}}],["消融实验验证",{"2":{"81":1}}],["消融实验",{"0":{"90":1},"2":{"51":1,"70":1}}],["动量编码器",{"2":{"150":1}}],["动态调整各像素点的置信权重以降低不确定性",{"2":{"114":1}}],["动态构建教师网络",{"2":{"77":1}}],["动态驱动编码器关注特定对象",{"2":{"72":1}}],["动态类别感知提示范式",{"2":{"72":1}}],["动态阈值策略与标签传播策略配合良好",{"2":{"36":1}}],["动态阈值",{"2":{"34":1}}],["他们在像语义分割这样的像素级识别任务中挣扎",{"2":{"147":1}}],["像clip这样的大规模视觉语言模型已经为图像级任务展示了令人印象深刻的开放词汇表能力",{"2":{"147":1}}],["像素分类准确率",{"2":{"191":1}}],["像素分类精度",{"2":{"191":1}}],["像素级",{"2":{"187":1}}],["像素级标注耗时耗力",{"2":{"129":1}}],["像素传播策略在92",{"2":{"36":1}}],["像素传播策略带来了一定的性能提升",{"2":{"36":1}}],["像素传播",{"2":{"34":1}}],["像素间的相关性可反映成对相似性",{"2":{"32":1}}],["晦涩",{"2":{"145":1}}],["韩国科学技术院",{"2":{"145":1}}],["位置编码",{"2":{"143":1}}],["添加偏置",{"2":{"193":1}}],["添加到path",{"2":{"165":1}}],["添加两个跳跃连接时性能最佳",{"2":{"143":1}}],["添加创建的嵌入可以持续提高所有数据集的性能",{"2":{"110":1}}],["默认",{"2":{"182":1}}],["默认会放大一些",{"2":{"175":1}}],["默认使用两个块的解码器",{"2":{"143":1}}],["默认从骨干网络提取特征",{"2":{"36":1}}],["低样本微调结果",{"2":{"142":1}}],["低样本学习",{"2":{"141":1}}],["低分辨率训练和超高分辨率测试",{"2":{"51":1}}],["低分辨率训练和超高清测试",{"2":{"51":1}}],["微调",{"2":{"141":1}}],["微调方法",{"2":{"49":2}}],["线性探测miou提高了4",{"2":{"143":1}}],["线性探测miou从67",{"2":{"143":1}}],["线性探测结果",{"2":{"142":1}}],["线性探测",{"2":{"141":1}}],["教师网络通过指数移动平均",{"2":{"141":1}}],["教导模型避免学习有标签输入的特定样本偏差",{"2":{"96":1}}],["混合注意力机制",{"2":{"141":1}}],["混合数据的异构约束",{"2":{"107":1}}],["促进训练过程中浅层的梯度传播",{"2":{"141":1}}],["促进跨模态语义对齐",{"2":{"85":1}}],["组成",{"2":{"141":1}}],["组件消融实验",{"0":{"122":1}}],["组件有效性",{"2":{"36":1}}],["组件分析",{"2":{"15":1,"72":1}}],["密集预测预训练",{"2":{"140":1}}],["密集标注的",{"2":{"101":1}}],["降低对大规模标注数据的需求",{"2":{"139":1}}],["及其在视觉transformer上的应用在计算机视觉领域取得显著进展",{"2":{"139":1}}],["机器人操作等领域应用广泛",{"2":{"139":1}}],["必须通过密集的自监督信号同时实现上述两个目标",{"2":{"138":1}}],["已被广泛应用于各类下游任务",{"2":{"138":1}}],["已有多种方法被提出",{"2":{"33":1}}],["需要清除辅助文件了",{"2":{"171":1}}],["需要进行路径的更改",{"2":{"164":1}}],["需要结合全局上下文信息",{"2":{"138":1}}],["需要像素级的细粒度理解",{"2":{"138":1,"139":1}}],["需要更多的努力来改进",{"2":{"135":1}}],["需要额外的计算资源",{"2":{"51":1}}],["需要额外网络或训练阶段",{"2":{"32":1}}],["专门的交互式分割方法通常会优于sam",{"2":{"135":1}}],["放大",{"2":{"135":1}}],["含超10亿掩码",{"2":{"135":1}}],["支持灵活提示",{"2":{"135":1}}],["支持和查询语义的重要性",{"2":{"72":1}}],["边缘检测",{"2":{"135":1}}],["边界框和图像级标签等",{"2":{"129":1}}],["边界框和图像级标签提供更多关键语义信息",{"2":{"128":1}}],["边界框监督仍然缺乏可靠和有效的措施来产生高质量的物体掩膜",{"2":{"21":1}}],["边界框监督",{"2":{"20":1,"21":1}}],["尚无网络规模的数据源",{"2":{"135":1}}],["许多问题缺乏充足的训练数据",{"2":{"135":1}}],["许多方法为类别标签生成语义嵌入",{"2":{"106":1}}],["项目页面可访问https",{"2":{"147":1}}],["项目",{"2":{"135":1,"171":1}}],["项目代码已开源",{"2":{"63":1}}],["下面给出具体的计算公式",{"2":{"191":1}}],["下面进行代码注释解读",{"2":{"171":1}}],["下文配置需要使用其路径",{"2":{"176":1}}],["下文会进行提及",{"2":{"171":1}}],["下图展示两者区别",{"2":{"171":1}}],["下载步骤如图",{"2":{"173":1}}],["下载",{"2":{"164":1,"173":1}}],["下载页面",{"2":{"164":1}}],["下载与安装",{"0":{"164":1}}],["下均优于大多数现有方法",{"2":{"141":1}}],["下一个创新点",{"2":{"133":1}}],["下游应用",{"2":{"111":1}}],["下游应用提升评估",{"2":{"110":1}}],["下游应用测试",{"2":{"109":1}}],["发现它的零样本表现非常优秀",{"2":{"135":1}}],["发现基于transformer的骨干网络在效率和性能上限方面表现更优",{"2":{"132":1}}],["发现所提出的dfq方案在消除非对角元素方面表现最佳",{"2":{"67":1}}],["骨干网络影响",{"2":{"132":1}}],["骨干网络设置消融实验",{"2":{"72":1}}],["局部原型动态更新全局原型",{"2":{"130":1}}],["局部查询原型生成",{"2":{"96":1}}],["初始预测",{"2":{"130":1}}],["初始时",{"2":{"98":1}}],["辅助任务会引入额外数据和预测误差",{"2":{"128":1}}],["辅助任务和标签扩散等",{"2":{"128":1,"129":1}}],["辅助建模模态公共统计信息",{"2":{"88":1}}],["伪建议",{"2":{"128":1,"129":1}}],["伪标签方法耗时",{"2":{"128":1}}],["伪标签消融实验",{"0":{"123":1}}],["伪标签细化和分割联合训练",{"2":{"77":1}}],["校正被误导的前景特征",{"2":{"124":1}}],["约67",{"2":{"122":1}}],["约束前景语义一致性",{"2":{"99":1}}],["显示如下",{"2":{"166":1}}],["显示出其鲁棒性",{"2":{"120":1}}],["显著超过现有方法",{"2":{"132":1}}],["显著超过之前的sota方法fc",{"2":{"58":1}}],["显著超越现有监督方法",{"2":{"104":1}}],["显著提升了前景实例的语义一致性",{"2":{"93":1}}],["显著提升泛化性",{"2":{"49":1}}],["显著优于其他单阶段方法",{"2":{"79":1}}],["显著扩大高置信度区域",{"2":{"38":1}}],["时操作步骤相同",{"2":{"180":1}}],["时构建项目",{"2":{"171":1}}],["时方法性能较好",{"2":{"132":1}}],["时",{"2":{"120":1}}],["收缩比率",{"2":{"120":1}}],["收集大规模标注数据集并非易事",{"2":{"20":1}}],["公平性处理",{"2":{"119":1}}],["λc",{"2":{"119":1,"122":1}}],["λs",{"2":{"119":1}}],["部署resnet101作为骨干网络",{"2":{"119":1}}],["部分工作构建多分辨率特征图用于密集输出",{"2":{"140":1}}],["部分工作探索了与少样本学习的结合",{"2":{"72":1}}],["部分引入像素",{"2":{"140":1}}],["部分研究尝试在无密集标注语义标签的情况下进行开放词汇语义分割",{"2":{"148":1,"149":1}}],["部分研究还改进了训练目标和架构",{"2":{"140":1}}],["部分研究开始探索适用于fss的特征编码器",{"2":{"72":1}}],["部分新方法尝试自适应生成伪标签",{"2":{"116":1}}],["部分采用视觉变换器提升长程建模能力",{"2":{"77":1}}],["部分掩码数量影响",{"2":{"72":1}}],["部分掩码生成器",{"2":{"72":2}}],["部分方法在弱监督语义分割中探索了原型的使用",{"2":{"129":1}}],["部分方法引入文本信息用于分类",{"2":{"72":1}}],["部分方法开始应用于计算机视觉",{"2":{"49":1}}],["部分模型可理解视觉内容",{"2":{"56":1}}],["技术",{"2":{"116":1}}],["考虑到直接使用伪标签可能误导分割模型",{"2":{"114":1}}],["本研究提出了一种类驱动的涂鸦增强网络",{"2":{"114":1}}],["本文转载自https",{"2":{"182":1}}],["本文使用图片均为笔者自身编辑器截图或笔者朋友的编辑器截图",{"2":{"163":1}}],["本文作者探索一种适用于语义分割的自监督预训练方法",{"2":{"139":1}}],["本文作者尝试采用llm的知识解决开放词汇语义分割中的挑战",{"2":{"55":1}}],["本文旨在开发一个可提示的模型",{"2":{"135":1}}],["本文重点介绍了三项关键技术",{"2":{"72":1}}],["本文模仿人类的视觉感知方式",{"2":{"72":1}}],["本文当前的工作关注于提升开放词汇的分类能力",{"2":{"60":1}}],["本文的作者提出了通用片段嵌入框架",{"2":{"157":1}}],["本文的sed方法都表现出较好的效果",{"2":{"101":1}}],["本文的研究目标",{"2":{"76":1}}],["本文的研究背景",{"2":{"49":1}}],["本文的创新点",{"2":{"49":1}}],["本文提出基于原型",{"2":{"127":1}}],["本文提出相关内在特征增强网络",{"2":{"94":1}}],["本文提出一种新方法pixelclip",{"2":{"148":1}}],["本文提出一种渐进式特征自我强化方法",{"2":{"76":1}}],["本文提出一种基于双重匹配变换的网络",{"2":{"42":1}}],["本文提出了相关内在特征增强网络",{"2":{"96":1}}],["本文提出了分组提示调优框架",{"2":{"88":1}}],["本文提出了一种用于语义分割的自监督预训练框架",{"2":{"141":1}}],["本文提出了一种简单的编码器",{"2":{"101":1}}],["本文提出了一种简单但高性能的半监督语义分割方法",{"2":{"31":1}}],["本文提出了一种全新的夜间语义分割方法",{"2":{"70":1}}],["本文提出了一种基于提示学习的",{"2":{"72":1}}],["本文提出了一种基于双重匹配变换的网络",{"2":{"44":1}}],["本文提出了一种基于双重匹配转换的网络",{"2":{"41":1}}],["本文提出了一种基于transformer的自适应原型匹配网络",{"2":{"12":1}}],["本文提出了一种名为cc4s",{"2":{"22":1}}],["本文提出了包含网络结构设计",{"2":{"19":1}}],["本文聚焦于构建图像分割基础模型",{"2":{"135":1}}],["本文聚焦于弱监督语义分割",{"2":{"76":1}}],["本文聚焦于",{"2":{"72":1}}],["本文聚焦于在领域泛化语义分割",{"2":{"49":1}}],["本文聚焦于涂鸦监督语义分割领域",{"2":{"20":1}}],["本文聚焦半监督语义分割领域",{"2":{"32":1}}],["蒸馏效果评估",{"2":{"110":1}}],["异质损失聚合效果评估",{"2":{"110":1}}],["鲁棒性评估",{"2":{"110":1}}],["δτ",{"2":{"109":1}}],["粗标注数据集",{"2":{"107":1}}],["创建语言嵌入",{"2":{"107":1}}],["创新点",{"2":{"70":1,"135":1}}],["创新性地使用解耦的深度特征作为查询",{"2":{"69":1}}],["零样本掩码分类",{"2":{"151":1}}],["零样本学习标签编码",{"2":{"106":1}}],["零样本语义分割",{"2":{"106":1}}],["限制了其可扩展性",{"2":{"148":1}}],["限制了其在现实场景中的应用潜力",{"2":{"105":1}}],["限制了其应用",{"2":{"32":1}}],["手动的处理大量的数据集费时费力",{"2":{"115":1}}],["手动统一标签集和重新标注的方法不仅费力",{"2":{"105":1}}],["手动生成像素级语义分割标注平均耗时3",{"2":{"20":1}}],["农业机器人和医学等领域应用广泛",{"2":{"105":1}}],["万张图像的精细标注数据与公开的带噪声弱标注数据",{"2":{"104":1}}],["替代传统类别标签",{"2":{"104":1}}],["替代标准多层感知器",{"2":{"12":1}}],["开始编译文件",{"2":{"174":1}}],["开放词汇图像分割的重要性正在迅速增长",{"2":{"158":1}}],["开放词汇图像分割的目的是将图像分割成语义上有意义的片段",{"2":{"157":1}}],["开放词汇图像分割",{"2":{"158":1}}],["开放词汇图像分割任务包括将图像划分为语义上有意义的片段",{"2":{"156":1}}],["开放词汇表图像分割的主要挑战在于将这些片段准确地分类到文本定义的类别中",{"2":{"156":1}}],["开放词汇语义分割旨在将像素划分到一个开放类别集中的不同语义组",{"2":{"101":1}}],["开放词汇语义分割",{"2":{"56":1,"101":1,"151":1}}],["开放词汇语义分割分为两种",{"2":{"55":1}}],["开放词汇语义分割近年来受到越来越多的关注",{"2":{"54":1}}],["开发跨领域通用的鲁棒语义分割模型成为研究热点",{"2":{"104":1}}],["缩短了推理时间",{"2":{"101":1}}],["缩放视觉和关系三种注意力模块",{"2":{"60":1}}],["缩放视觉注意力",{"2":{"57":2,"59":1}}],["×",{"2":{"101":2,"174":1}}],["次迭代",{"2":{"101":1}}],["共有两种操作方式",{"2":{"181":1}}],["共",{"2":{"101":1}}],["共修正了2554个标签图",{"2":{"70":1}}],["倍的因子",{"2":{"101":1}}],["均为笔者所安装的其余插件以及其余设置所致",{"2":{"163":1}}],["均为",{"2":{"101":1}}],["均超越了当前最先进的语义分割方法",{"2":{"93":1}}],["渐进式融合编码器",{"2":{"101":1}}],["也叫反卷积",{"2":{"193":1}}],["也称为全像素语义分割",{"2":{"187":1}}],["也就是出现在工具栏中的链名称",{"2":{"171":1}}],["也就是上图所完成的功能",{"2":{"164":1}}],["也是众多视觉应用的基础模块",{"2":{"138":1}}],["也会增加计算资源",{"2":{"101":1}}],["也取得了更优的性能",{"2":{"79":1}}],["也取得了令人满意的结果",{"2":{"72":1}}],["运行于单个a6000显卡",{"2":{"101":1}}],["每一块的输出",{"2":{"194":1}}],["每一层在",{"2":{"194":1}}],["每个像素的分类类别均为",{"2":{"188":1}}],["每个代码语句",{"2":{"170":1}}],["每个使用者的",{"2":{"163":1}}],["每个使用者都能够根据自己的需求和想法下载相应的插件",{"2":{"163":1}}],["每个段随机采样一个文本描述来计算文本嵌入",{"2":{"159":1}}],["每个簇由clip文本特征表示为质心",{"2":{"150":1}}],["每个标记与特定实例对应",{"2":{"49":1}}],["每张图像的推理时间为82毫秒",{"2":{"101":1}}],["完成中文环境配置",{"2":{"166":1}}],["完成分割任务",{"2":{"101":1}}],["完整模型通过整合llm关系先验到自注意力图中",{"2":{"59":1}}],["完整模型在a",{"2":{"59":1}}],["完整的corrmatch在92和1464分割比例下miou分别达到76",{"2":{"36":1}}],["层对特征图进行上采样",{"2":{"141":1}}],["层次化骨干网络能够更好地捕捉局部空间信息",{"2":{"101":1}}],["层感知器和权重归一化的全连接层组成",{"2":{"78":1}}],["称为sed",{"2":{"101":1}}],["称为神经特征空间",{"2":{"19":1}}],["天津大学",{"0":{"101":1}}],["综上",{"2":{"99":1}}],["综合实验表明",{"2":{"19":1}}],["干扰查询预测",{"2":{"98":1}}],["准确率反而下降",{"2":{"98":1}}],["数量和质量远超现有数据集",{"2":{"135":1}}],["数量继续增加",{"2":{"98":1}}],["数据过滤旨在通过过滤噪声的图像",{"2":{"158":1}}],["数据过滤和数据改进",{"2":{"158":1}}],["数据方案和通用片段嵌入模型",{"2":{"157":1}}],["数据创新",{"2":{"135":1}}],["数据融合",{"2":{"111":1}}],["数据增强对性能的影响有限",{"2":{"80":1}}],["数据增强",{"2":{"80":1}}],["数据驱动方法的局限",{"2":{"72":1}}],["数据分布差异",{"2":{"64":1}}],["数据标注难题",{"2":{"20":1}}],["数据集的创新",{"2":{"135":1}}],["数据集的涂鸦监督语义分割任务中达到了当前最佳水平",{"2":{"127":1}}],["数据集大小和训练计算量的增加而提升",{"2":{"135":1}}],["数据集与实现细节",{"0":{"109":1}}],["数据集合并难题",{"2":{"105":1}}],["数据集上的大量实验表明",{"2":{"75":1}}],["数据集上实现了",{"2":{"31":1}}],["数据集贡献",{"2":{"70":1}}],["数据集和对比方法选择",{"2":{"51":1}}],["数据集",{"2":{"14":1,"26":1,"35":1,"45":1,"58":1,"70":1,"79":1,"89":1,"97":1,"101":1,"118":1,"131":1,"142":1}}],["五样本设置下高约2",{"2":{"97":1}}],["五和六个领域的asd指标分别提高了0",{"2":{"67":1}}],["权重β经验性地设置为0",{"2":{"96":1}}],["激活包含目标类对象的像素并停用其他像素",{"2":{"96":1}}],["帮助模型学习保证语义一致性",{"2":{"96":1}}],["帮助缓解空间感知偏差",{"2":{"9":1}}],["少数研究探索了无标签数据的利用",{"2":{"95":1}}],["少样本学习",{"2":{"72":2}}],["少样本语义分割范式受到关注",{"2":{"94":1}}],["少样本语义分割和跨域少样本语义分割的几种先进方法进行比较",{"2":{"45":1}}],["少样本语义分割",{"2":{"42":1,"43":1}}],["少样本分割中无标签数据利用",{"2":{"95":1}}],["少样本分割",{"2":{"10":1,"72":2,"95":1}}],["原本内嵌输出的",{"2":{"180":1}}],["原因在于它们忽略了语义分割的特定属性",{"2":{"139":1}}],["原因是无标签增强效果过强会使特征挖掘注意力转向无标签分支",{"2":{"98":1}}],["原因是随着有标签图像增加",{"2":{"97":1}}],["原型对应",{"2":{"140":1}}],["原型设置",{"2":{"132":1}}],["原型提取与更新",{"2":{"130":1}}],["原型方法",{"2":{"129":1}}],["原型学习方法以较低计算成本取得不错效果",{"2":{"95":1}}],["原始的nightcity是最大的夜间语义分割数据集",{"2":{"70":1}}],["空间相关方法虽保留空间结构",{"2":{"95":1}}],["主干网络对空间信息变得不敏感",{"2":{"101":1}}],["主损失使用dice损失计算查询输入的预测结果与真实标签之间的差异",{"2":{"96":1}}],["主流方法分为原型提取和空间相关两类",{"2":{"95":1}}],["主要是通过输入边缘的",{"2":{"193":1}}],["主要是因为夜间的光照条件复杂且不足",{"2":{"70":1}}],["主要原因在于忽视了该任务的三个核心特性",{"2":{"138":1}}],["主要组件",{"2":{"88":1}}],["主要组件的影响",{"2":{"59":1}}],["主要分为基于对齐和基于聚合的融合方法",{"2":{"87":1}}],["主要有原型匹配",{"2":{"72":1}}],["主要通过强制预测一致性来利用",{"2":{"34":1}}],["扩展名为",{"2":{"179":1}}],["扩展实验",{"2":{"51":1}}],["扩大前景和背景的类间差异",{"2":{"94":1}}],["同一类的不同物体也要进行分割",{"2":{"187":1}}],["同一类不同实例存在语义模糊",{"2":{"94":1}}],["同",{"2":{"101":1}}],["同时解决了上述两个问题",{"2":{"171":1}}],["同时获取全局图像特征",{"2":{"159":1}}],["同时引入跳跃连接",{"2":{"141":1}}],["同时引入定位校正模块",{"2":{"117":1}}],["同时使用两种原型增强时性能最佳",{"2":{"132":1}}],["同时使用所有组件可获得最佳性能",{"2":{"122":1}}],["同时在深度估计",{"2":{"104":1}}],["同时两阶段和单阶段的方法都存在不足",{"2":{"101":1}}],["同时保留各模态独有的特征模式",{"2":{"85":1}}],["同时我们提出",{"2":{"75":1}}],["同时也具有重要的临床应用价值",{"2":{"63":1}}],["同时精度更高",{"2":{"59":1}}],["同时",{"2":{"46":1,"99":1,"152":1}}],["同时提出整合支持图像的广义逆来优化查询图像的广义逆",{"2":{"44":1}}],["同时考虑弱增强和强增强图像在高置信区域的对数似然一致性",{"2":{"34":1}}],["同时忽略语义异构的背景区域",{"2":{"19":1}}],["基本概念",{"0":{"196":1},"1":{"197":1,"198":1,"199":1,"200":1}}],["基本更改",{"2":{"164":1}}],["基础模型",{"2":{"135":1}}],["基础模型发展",{"2":{"135":1}}],["基准测试中",{"2":{"93":1}}],["基于上述的存在的问题",{"2":{"157":1}}],["基于上述问题",{"2":{"10":1,"42":1}}],["基于视觉的基础模型",{"2":{"156":1}}],["基于掩码图像建模的方法取得了不错的成果",{"2":{"140":1}}],["基于这些问题",{"2":{"139":1}}],["基于这些关键要素",{"2":{"138":1}}],["基于这些观察",{"2":{"31":1}}],["基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力",{"2":{"135":1}}],["基于大语言模型",{"2":{"59":1}}],["基于标签扩散的方法通过将标记的像素扩散到未标记的像素来生成像素级伪标签",{"2":{"115":1}}],["基于一致性学习的方法旨在捕获不变特征",{"2":{"115":1}}],["基于正则化损失的方法设计了特定的损失函数来提高模型的稳定性",{"2":{"115":1}}],["基于涂鸦的wsss的内在挑战在于稀疏标签提供的部分监督",{"2":{"115":1}}],["基于涂鸦的弱监督语义分割",{"2":{"114":1,"124":1}}],["基于",{"2":{"101":1}}],["基于分层编码器的代价图",{"2":{"101":1}}],["基于层次编码器的成本图生成使用层次化的骨干网络",{"2":{"101":1}}],["基于层次编码器的成本图生成和逐渐融合的解码器",{"2":{"101":1}}],["基于聚合的融合易忽略模态内传播",{"2":{"86":1}}],["基于对齐的融合因信息交换弱",{"2":{"86":1}}],["基于cam的选择略优于基于边缘的选择",{"2":{"80":1}}],["基于图像级标签的wsss常用方法是先训练图像分类网络",{"2":{"76":1}}],["基于全卷积网络",{"2":{"70":1}}],["基于属性先验的尺度选择方法优于其他非选择方法",{"2":{"59":1}}],["基于属性先验知识",{"2":{"57":1}}],["基于先验的语义分割方法可以识别固定集的目标种类",{"2":{"55":1}}],["基于传播的方法面临计算和内存限制",{"2":{"51":1}}],["基于人类从粗略到精细地逐步区分物体的方式",{"2":{"51":1}}],["基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机",{"2":{"49":1}}],["基于此",{"2":{"49":1}}],["基于此生成前景和背景的预测图并进行监督",{"2":{"41":1}}],["基于ema的阈值更新策略对不同初始值不敏感",{"2":{"36":1}}],["基于具有弱到强一致性正则化的简单框架构建",{"2":{"34":1}}],["基于自训练策略的方法",{"2":{"33":1}}],["基于以上背景",{"2":{"20":1,"32":1,"128":1}}],["基于深度学习的语义分割方法通常需要大量像素级标注图像",{"2":{"32":1}}],["基于深度学习的语义分割方法虽表现出色",{"2":{"20":1}}],["基于深度学习的方法在语义分割中取得了令人印象深刻的性能",{"2":{"19":1}}],["基于像素匹配的方法建立支持像素和查询像素的密集关联",{"2":{"11":1}}],["基于原型的方法用原型代表目标类信息进行匹配预测",{"2":{"11":1}}],["基于度量学习的fss主要分为基于原型和基于像素匹配两类方法",{"2":{"11":1}}],["之间的多粒度互补关系",{"2":{"93":1}}],["捕捉细节特征",{"2":{"93":1}}],["捕捉对象先验与掩码之间的对应关系",{"2":{"57":1}}],["表示两个区域的交并比",{"2":{"191":1}}],["表现更优",{"2":{"129":1}}],["表征整体类别特征",{"2":{"93":1}}],["表明其在实际场景中能实现更高效的语义分割",{"2":{"142":1}}],["表明涂鸦和伪标签提供了互补的监督",{"2":{"122":1}}],["表明注意力加权机制优于平均加权",{"2":{"80":1}}],["表明unc",{"2":{"80":1}}],["表明强化确定特征与unc",{"2":{"80":1}}],["表明强化不确定特征对语义澄清很重要",{"2":{"80":1}}],["表明该编码器能灵活地为不同的新类别生成类别感知特征",{"2":{"72":1}}],["表明从支持和查询图像中提取特定类别的线索能显著增强类别感知能力",{"2":{"72":1}}],["表明dfq使来自不同领域的样本更均匀地混合",{"2":{"67":1}}],["表明引入的模块有效解决了固有偏差",{"2":{"15":1}}],["背景的像素级分类精度不足",{"2":{"93":1}}],["就是从像素层面上对图像进行描述",{"2":{"187":1}}],["就要找失败原因了",{"2":{"174":1}}],["就需要进行多次不同命令的转换编译",{"2":{"171":1}}],["就在多个下游多模态图像分割任务中取得了sota性能",{"2":{"90":1}}],["就在多个多模态图像分割基准任务中刷新了最高性能记录",{"2":{"85":1}}],["就能在各项指标上取得优于现有方法的性能",{"2":{"88":1}}],["优化策略",{"2":{"88":1}}],["优于之前的单阶段解决方案",{"2":{"79":1}}],["根据",{"2":{"170":1}}],["根据个人想法可以选择是否在开始菜单文件夹创建",{"2":{"165":1}}],["根据您的需要进行相应的更改",{"2":{"164":1}}],["根据输入段从图像块嵌入中提取段嵌入",{"2":{"159":1}}],["根据涂鸦和伪标签边界确定可靠区域",{"2":{"124":1}}],["根据显式分组的语义相似性",{"2":{"88":1}}],["根据对数似然输出迭代更新阈值",{"2":{"34":1}}],["平衡了模态内和模态间的语义传播",{"2":{"90":1}}],["平衡模态内和模态间的语义传播",{"2":{"88":1}}],["平均交并比",{"2":{"191":1}}],["平均准确率",{"2":{"191":1}}],["平均性能也优于端到端方法",{"2":{"162":1}}],["平均miou提高了16",{"2":{"151":1,"153":1}}],["平均dsc提高了1",{"2":{"67":1}}],["平均边界准确率",{"2":{"51":1}}],["把标记送入分组提示器",{"2":{"88":1}}],["输入通道数",{"2":{"194":1}}],["输入的每一条边补充0的层数",{"2":{"193":1}}],["输入的图像大小为w×h的",{"2":{"188":1}}],["输入信号的通道数",{"2":{"193":1}}],["输入",{"2":{"166":1,"167":1}}],["输入cmd",{"2":{"164":1}}],["输入模型",{"2":{"109":1}}],["输入处理",{"2":{"88":1}}],["输出通道数",{"2":{"194":1}}],["输出尺寸的计算公式为",{"2":{"193":1}}],["输出边补充0的层数",{"2":{"193":1}}],["输出也可能会比实际输入更大或者更小一些",{"2":{"188":1}}],["输出节点数为",{"2":{"188":1}}],["输出经过调整大小后传递给分类头",{"2":{"96":1}}],["输出步长多设为",{"2":{"51":1}}],["核心模型是多尺度编码器",{"2":{"141":1}}],["核心挑战在于如何从有限标注数据中提取类别本质特征",{"2":{"93":1}}],["核心思想",{"2":{"88":1}}],["核心网络包含两个模块",{"2":{"22":1}}],["参数",{"2":{"179":1}}],["参数多",{"2":{"95":1}}],["参数存储负担大",{"2":{"86":1}}],["参数高效微调",{"2":{"49":2}}],["现在编译的结果为内部查看器查看",{"2":{"174":1}}],["现存的方法有三种",{"2":{"115":1}}],["现存方法的挑战",{"2":{"86":1}}],["现有的工作可以分为两类",{"2":{"158":1}}],["现有的方法可以分为两类",{"2":{"158":1}}],["现有的开放词汇图像分割方法面临着挑战",{"2":{"157":1}}],["现有涂鸦监督语义分割方法主要依赖正则化损失",{"2":{"128":1,"129":1}}],["现有解决开放集问题的方法多在小数据集上实验",{"2":{"105":1}}],["现有零样本方法的缺陷",{"2":{"105":1}}],["现有公开数据集的不同分类标准阻碍了它们的直接融合",{"2":{"104":1}}],["现有多模态分割方法主要分为基于对齐和基于聚合的融合",{"2":{"86":1}}],["现有fss方法的问题",{"2":{"72":1}}],["现有fss模型虽有成果",{"2":{"10":1}}],["现有白天方法在夜间性能会下降",{"2":{"70":1}}],["现有领域泛化医学图像分割方法主要分为学习形状不变特征和明确学习多源域间的域间偏移两类",{"2":{"64":1}}],["现有主要的cd",{"2":{"42":1}}],["现有半监督语义分割方法多采用复杂训练策略",{"2":{"32":1}}],["现有方法在语义分割任务中效果欠佳",{"2":{"138":1}}],["现有方法",{"2":{"129":1}}],["现有方法通常先加载基于rgb的预训练模型参数",{"2":{"87":1}}],["现有方法的局限性",{"2":{"76":1,"128":1}}],["现有方法分学习形状不变特征和学习域间偏移两类",{"2":{"65":1}}],["现有方法分为基于度量和基于关系两类",{"2":{"43":1}}],["现有方法不足",{"2":{"64":1}}],["现有方法主要通过基于局部特征线索的像素扩散机制",{"2":{"114":1}}],["现有方法主要聚焦于学习形状不变性表征或实现源域间的特征共识",{"2":{"63":1}}],["现有方法主要基于语义一致性进行预测",{"2":{"12":1}}],["现有方法存在不足",{"2":{"32":1}}],["现有方法大多采用复杂的训练策略来利用未标注数据",{"2":{"31":1}}],["现有方法包括利用辅助任务信息",{"2":{"21":1}}],["现有方法利用单层注意力机制建立支持集和查询集的关系",{"2":{"12":1}}],["现有方法多依赖语义相关性",{"2":{"10":1}}],["红外等",{"2":{"85":1}}],["启发",{"2":{"81":1,"156":1}}],["加入高斯模糊或日光化处理时",{"2":{"80":1}}],["经典数据集",{"0":{"201":1},"1":{"202":1,"203":1,"204":1}}],["经典pascal",{"2":{"35":1}}],["经过对方同意",{"2":{"163":1}}],["经过滤和合并后",{"2":{"160":1}}],["经过1×1卷积和激活操作",{"2":{"96":1}}],["经验上掩码比率r",{"2":{"80":1}}],["略高于多阶段的mctformer",{"2":{"79":1}}],["甚至被其他应用程序修改",{"2":{"171":1}}],["甚至在某些基准测试中超过了这些方法",{"2":{"151":1}}],["甚至在某些情况下表现更好",{"2":{"135":1}}],["甚至超过了一些复杂的多阶段方法",{"2":{"79":1,"81":1}}],["甚至可与全标签监督方法相媲美",{"2":{"28":1}}],["投影器",{"2":{"78":1}}],["借助全局上下文理解",{"2":{"139":1}}],["借助语义约束分离图像",{"2":{"70":1}}],["借鉴",{"2":{"78":1}}],["防止过度平滑",{"2":{"78":1}}],["影响了对复杂场景的理解和处理能力",{"2":{"101":1}}],["影响分割效果",{"2":{"94":1}}],["影响分割精度",{"2":{"76":1}}],["影响模型对未见领域的泛化能力",{"2":{"64":1}}],["再进行普通的卷积",{"2":{"193":1}}],["再点击安装即可",{"2":{"164":1}}],["再点击安装",{"2":{"164":1}}],["再基于此和分层编码器的不同特征图",{"2":{"101":1}}],["再迁移到查询输入进行测试",{"2":{"94":1}}],["再输入到基础模型的下一层",{"2":{"88":1}}],["再在特定下游任务数据集上微调",{"2":{"87":1}}],["再训练分割模型评估性能",{"2":{"77":1}}],["再将其细化为伪分割标签来监督分割网络",{"2":{"76":1}}],["再以此为指导在整个查询特征图中寻找特征相似度高的点",{"2":{"12":1}}],["点进去之后就可以进行下载了",{"2":{"165":1}}],["点击编辑页面任意位置来选中",{"2":{"180":1}}],["点击选中",{"2":{"174":1}}],["点击下图",{"2":{"168":1}}],["点击设置",{"2":{"168":1}}],["点击设置图标",{"2":{"168":1}}],["点击页面右下角跳出窗口中的",{"2":{"166":1}}],["点击拓展图标",{"2":{"166":1,"167":1}}],["点击关闭即可",{"2":{"164":1}}],["点击",{"2":{"164":1,"166":1,"167":2}}],["点击红框圈画链接进行",{"2":{"164":1}}],["点击图示红框圈画位置进入随机的镜像网站",{"2":{"164":1}}],["点",{"2":{"129":1}}],["点级和边界框级标注也是常见方式",{"2":{"116":1}}],["点和图像级标签等",{"2":{"76":1}}],["点监督通过在每个图像对象内标记带有类别信息的点来完成注释",{"2":{"21":1}}],["点监督和涂鸦监督",{"2":{"21":1}}],["点监督和涂鸦监督等",{"2":{"20":1}}],["确定区域应当具备足够的鲁棒性以保持全局语义特征",{"2":{"75":1}}],["确保在自适应变换过程中前景对象的不变性",{"2":{"44":1}}],["来扩大输入图像的尺寸",{"2":{"193":1}}],["来自dino",{"2":{"141":1}}],["来自原始的",{"2":{"101":1}}],["来增强全局上下文信息的传播",{"2":{"141":1}}],["来实现这一目标",{"2":{"135":1}}],["来结合两种监督的优势",{"2":{"117":1}}],["来预测像素级的图像",{"2":{"101":1}}],["来恢复局部特征信息",{"2":{"75":1}}],["来进行超高分辨率图像的分割细化任务",{"2":{"51":1}}],["面临一个关键挑战",{"2":{"75":1}}],["面对显著的分布差异",{"2":{"63":1}}],["zhihu",{"2":{"182":1}}],["zhuanlan",{"2":{"182":1}}],["zhejiang",{"2":{"73":2}}],["zihao",{"2":{"173":1}}],["zxl19990529",{"2":{"113":1,"114":1}}],["zero",{"2":{"72":1,"103":1,"104":1,"135":3}}],["做法后",{"2":{"72":1}}],["盲目使用变压器提取特征可能无法带来预期的性能提升",{"2":{"72":1}}],["块的数量并非越多越好",{"2":{"72":1}}],["块数量的影响",{"2":{"72":1}}],["较小的vit",{"2":{"72":1}}],["较为合适",{"2":{"72":1}}],["高宽都增加padding",{"2":{"193":1}}],["高宽都增加2",{"2":{"193":1}}],["高质量标注数据集",{"2":{"107":1}}],["高出2",{"2":{"97":1}}],["高斯抑制的作用",{"2":{"72":1}}],["高置信区域的像素数量和完整性明显优于未使用时",{"2":{"37":1}}],["产生冗余和噪声",{"2":{"72":1}}],["产生空间感知偏差",{"2":{"10":1}}],["继续增加数量",{"2":{"72":1}}],["体现了背景语义对分割的重要性",{"2":{"72":1}}],["说明",{"2":{"191":1}}],["说明编译失败",{"2":{"174":1}}],["说明编译成功",{"2":{"174":1}}],["说明安装完毕",{"2":{"164":1}}],["说明两者对于查询图像的分割都至关重要",{"2":{"72":1}}],["说明挖掘细粒度的部分语义能进一步发挥提示的作用",{"2":{"72":1}}],["说明未标记数据的利用率得到有效提高",{"2":{"37":1}}],["挖掘细粒度语义提示",{"2":{"72":1}}],["挖掘目标对象自身的空间一致性",{"2":{"12":1}}],["精确转移语义",{"2":{"72":1}}],["精确的图像分割细化",{"2":{"51":1}}],["构建了包含多尺度编码器",{"2":{"144":1}}],["构建数据引擎收集sa",{"2":{"135":1}}],["构建不同粒度原型之间的交互",{"2":{"96":1}}],["构建部分掩码生成器",{"2":{"72":1}}],["构建三个关键增强点",{"2":{"72":1}}],["构建框架",{"2":{"49":1}}],["源于自然语言处理",{"2":{"72":1}}],["源数据集和目标数据集存在较大领域差距",{"2":{"42":1}}],["人类能够以独特的视觉感知模式选择性地关注视线中的关键对象",{"2":{"72":1}}],["人类视觉感知的启示",{"2":{"72":1}}],["人类可以轻松聚焦于视线中的特定物体",{"2":{"72":1}}],["会将所有的非",{"2":{"171":1}}],["会出现一些配置文件的安装运行写入",{"2":{"164":1}}],["会先出现下图",{"2":{"164":1}}],["会激活与目标类别无关的对象",{"2":{"72":1}}],["会导致注意力偏差",{"2":{"10":1}}],["弱标注数据集",{"2":{"107":1}}],["弱标签甚至零样本等更现实的场景时",{"2":{"72":1}}],["弱标签和零样本分割等场景",{"2":{"72":1}}],["弱标签fss和零",{"2":{"72":1}}],["弱",{"2":{"76":1}}],["弱监督语义分割方法",{"2":{"77":1}}],["弱监督语义分割",{"2":{"21":1,"81":1}}],["弱监督方法兴起",{"2":{"20":1,"149":1}}],["遥感领域",{"2":{"72":1}}],["令人惊喜的是",{"2":{"72":1}}],["令人惊讶的是",{"2":{"72":1}}],["令牌长度和秩对模型性能的影响",{"2":{"49":1}}],["感兴趣的物体",{"2":{"72":1}}],["尤其是在当前大模型时代",{"2":{"72":1}}],["类目标",{"2":{"101":1}}],["类间相似性使像素级二分类困难",{"2":{"94":1}}],["类感知单模态提示器",{"2":{"88":1}}],["类别识别局限",{"2":{"101":1}}],["类别模板数量",{"2":{"101":1}}],["类别早期拒接",{"2":{"101":1}}],["类别",{"2":{"72":1}}],["类内差异会导致查询图像出现语义错误",{"2":{"94":1}}],["类内差异表示在三种不同的注意力机制中都有益",{"2":{"15":1}}],["类内差异表示利用一组可学习向量建模支持集和查询集之间的差异",{"2":{"12":1}}],["类内外观差异以及信息利用不充分等关键问题",{"2":{"42":1}}],["框架来解决这一挑战",{"2":{"156":1}}],["框架",{"2":{"70":1}}],["框架能够在不受光照影响的情况下提取反射成分",{"2":{"70":1}}],["框架能够以端到端方式无缝集成于transformer分割模型",{"2":{"63":1}}],["夜间语义分割",{"2":{"70":1}}],["夜间场景光照强度低且人工光源复杂",{"2":{"70":1}}],["难以泛化到新领域和标签",{"2":{"105":1}}],["难以提取用于语义分割的判别特征",{"2":{"70":1}}],["难以对单张图像中不同实例的特征进行细化",{"2":{"49":1}}],["视觉transformer",{"2":{"140":1}}],["视觉编码器形式的预训练",{"2":{"101":1}}],["视觉",{"2":{"101":1}}],["视觉提示学习应用",{"2":{"87":1}}],["视觉变压器",{"2":{"78":1}}],["视觉系统近一半时间需在光照不足且复杂的夜间环境下工作",{"2":{"70":1}}],["视觉基础模型应用",{"2":{"149":1}}],["视觉基础模型",{"2":{"49":2}}],["光照感知解析器",{"2":{"70":1}}],["y",{"2":{"194":3}}],["youtubevis",{"2":{"109":1}}],["yvanyin",{"2":{"103":1,"104":1}}],["yield",{"2":{"70":1}}],["years",{"2":{"53":1}}],["展现出优异的跨领域泛化能力",{"2":{"104":1}}],["展现出高效性和优越性",{"2":{"88":1}}],["展现出显著优势",{"2":{"85":1}}],["展现出了卓越的领域泛化能力",{"2":{"69":1}}],["展示了fsr在不确定区域",{"2":{"80":1}}],["展示了该方法在图像分割细化上的高效性和快速性",{"2":{"51":1}}],["展示将crm应用于全景分割的可视化结果",{"2":{"51":1}}],["展示cascadepsp",{"2":{"51":1}}],["引用查看",{"2":{"179":1}}],["引导整个框架学习不同领域相似的通道特征模式",{"2":{"69":1}}],["引入定位校正模块",{"2":{"124":1}}],["引入辅助无标签分支作为有效的数据利用方法",{"2":{"96":1}}],["引入了对齐诱导的跨模态提示器",{"2":{"90":1}}],["引入了三种新型注意力模块",{"2":{"57":1}}],["引入显式语义分组机制到提示学习中",{"2":{"88":1}}],["引入额外的语言信息有助于生成更强大的提示",{"2":{"72":1}}],["引入额外类别语义",{"2":{"72":1}}],["引入背景提示",{"2":{"72":1}}],["引入前景提示",{"2":{"72":1}}],["引入跨模态语言信息初始化提示",{"2":{"72":1}}],["引入跨模态的语言信息来初始化每个任务的提示",{"2":{"72":1}}],["引入光照感知解析器",{"2":{"70":2}}],["引入隐函数",{"2":{"51":1}}],["引入深度卷积网络",{"2":{"51":1}}],["引入基于卷积transformer架构的多尺度局部感知调制transformer进行多尺度特征提取",{"2":{"12":1}}],["各组件有效性",{"2":{"132":1}}],["各组件实验",{"2":{"68":1}}],["各尺度实验",{"2":{"68":1}}],["眼底图像分割基准",{"2":{"67":1}}],["最突出的特点就是其强大的插件功能",{"2":{"163":1}}],["最让人头疼的是",{"2":{"163":1}}],["最近的基础模型在将图像的像素分为有意义的片段上效果显著",{"2":{"157":1}}],["最高提升了2",{"2":{"67":1}}],["最后通过线性层映射为段嵌入",{"2":{"159":1}}],["最后通过基于掩码的非极大值抑制",{"2":{"159":1}}],["最后提出本文的不足",{"2":{"135":1}}],["最后计算该预测表示与高置信伪标签之间的相关损失作为监督",{"2":{"34":1}}],["最后将两个掩码相加得到最终的查询前景分割图",{"2":{"12":1}}],["最后",{"2":{"9":1}}],["四",{"2":{"67":1}}],["∑i=14lirdwt",{"2":{"66":1}}],["解码器深度",{"2":{"143":1}}],["解码器由全局注意力阶段",{"2":{"141":1}}],["解码器视觉变压器",{"2":{"141":1}}],["解码器",{"2":{"78":1}}],["解码器架构mevt和自监督训练策略的框架",{"2":{"144":1}}],["解码器架构生成具有全局上下文传播能力的高分辨率特征",{"2":{"138":1}}],["解码器架构",{"2":{"101":1}}],["解码器架构被广泛应用",{"2":{"95":1}}],["解码器架构实现图像级监督的语义分割",{"2":{"78":1}}],["解码器架构的方法成为主流",{"2":{"70":1}}],["解码泛化表示",{"2":{"66":1}}],["解决现有方法的不足",{"2":{"148":1}}],["解决模型对噪声标签过拟合问题",{"2":{"124":1}}],["解决固定编码器类别无关问题",{"2":{"72":1}}],["解决固有偏差",{"2":{"16":1}}],["解决方案",{"2":{"66":2}}],["解决该问题将是未来工作方向",{"2":{"51":1}}],["解决了传统方法中阈值选择困难",{"2":{"38":1}}],["解决特征分类阶段的空间感知偏差问题",{"2":{"12":1}}],["解决特征匹配阶段的注意力偏差问题",{"2":{"12":1}}],["深层",{"2":{"80":1}}],["深层特征查询对不同领域浅层表示的一致性施加了隐式约束",{"2":{"66":1}}],["深度估计",{"2":{"109":1}}],["深度估计在nyudv2",{"2":{"109":1}}],["深度多模态融合展现出比单模态分割更显著的优势",{"2":{"86":1}}],["深度学习",{"2":{"185":1}}],["深度学习笔记",{"0":{"185":1}}],["深度学习技术推动了深度神经网络在图像分割的发展",{"2":{"128":1}}],["深度学习技术推动医学图像分割发展",{"2":{"65":1}}],["深度学习方法在特定高质量数据集上取得显著成果",{"2":{"106":1}}],["深度学习推动下",{"2":{"86":1}}],["深度学习在计算机视觉任务中取得显著进展",{"2":{"72":1}}],["深度表示解耦",{"2":{"70":1}}],["深度神经网络倾向于在多个通道中提取相似模式",{"2":{"66":1}}],["深度特征与浅层特征间的长程依赖关系可经由自注意力充分挖掘",{"2":{"63":1}}],["键和值基于浅层特征",{"2":{"66":1}}],["通常与之前的完全监督方法相当",{"2":{"135":1}}],["通道解耦特征虽增强了深度神经网络在跨领域场景中的表示能力",{"2":{"66":1}}],["通过调整其与",{"2":{"175":1}}],["通过网址",{"2":{"164":1}}],["通过网格搜索找到距离熵损失所有组件的最佳超参数组合",{"2":{"122":1}}],["通过提示sam生成掩码",{"2":{"160":1}}],["通过提示工程实现零样本迁移到下游分割任务",{"2":{"135":1}}],["通过提示工程可适应多种任务和数据分布",{"2":{"135":2}}],["通过多级别特征合并",{"2":{"159":1}}],["通过特定提示生成图像中对象及其属性的详细描述",{"2":{"159":1}}],["通过消融研究验证了模型设计的关键组件",{"2":{"152":1}}],["通过消融实验",{"2":{"90":1}}],["通过消融实验分析rein各组件的有效性",{"2":{"49":1}}],["通过消融实验评估不同的dcm组件",{"2":{"15":1}}],["通过简单替换clip模型和权重",{"2":{"151":1}}],["通过优化图像",{"2":{"150":1}}],["通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别",{"2":{"12":1}}],["通过引导模型在哪里来调整clip图像编码器以进行像素级理解",{"2":{"147":1}}],["通过引入特定于模态的类标记",{"2":{"88":1}}],["通过引入额外的色彩约束正则化项对网络进行重训练",{"2":{"19":1}}],["通过最小化交叉熵损失将知识从教师网络蒸馏到学生网络",{"2":{"141":1}}],["通过",{"2":{"141":1}}],["通过预训练减少对大量标注样本的依赖显得尤为重要",{"2":{"138":1}}],["通过加权平均形成局部原型",{"2":{"130":1}}],["通过部分交叉熵损失",{"2":{"130":1}}],["通过挖掘特征原型",{"2":{"127":1}}],["通过使用我们高效的模型进行数据收集",{"2":{"135":1}}],["通过使用涂鸦生成的标注进行模型训练",{"2":{"127":1}}],["通过使用稀疏涂鸦监督正逐渐受到关注",{"2":{"114":1}}],["通过参考其他前景位置的特征表示",{"2":{"124":1}}],["通过涂鸦标注与伪标签边界的可靠区域确定机制",{"2":{"114":1}}],["通过整合包含",{"2":{"104":1}}],["通过对语义切分和零件切分基准的综合实验研究",{"2":{"156":1}}],["通过对全局特征进行掩码平均池化",{"2":{"96":1}}],["通过对训练样本的子集进行重采样作为无标签数据",{"2":{"96":1}}],["通过对特定模态的类令牌进行分组",{"2":{"90":1}}],["通过指导模型提取对类内差异具有鲁棒性的本质特征",{"2":{"93":1}}],["通过指数移动平均",{"2":{"34":1}}],["通过在youtubevis数据集上采样具有5个零样本标签的约2100张图像进行实验",{"2":{"110":1}}],["通过在解码器的早期层次中剔除不存在的类别",{"2":{"101":1}}],["通过在提示学习中引入显式语义分组",{"2":{"90":1}}],["通过在模型执行的三个阶段进行策略性和高效交互",{"2":{"10":1}}],["通过冻结预训练的基础模型",{"2":{"88":1}}],["通过共享提示参数聚合不同模态的类特征表示",{"2":{"85":1}}],["通过聚类同类模态特征",{"2":{"85":1}}],["通过语义分组机制学习模态专属提示",{"2":{"85":1}}],["通过分析注意力机制",{"2":{"80":1}}],["通过学生和教师管道实现自蒸馏",{"2":{"78":1}}],["通过自适应阈值",{"2":{"107":1}}],["通过自适应划分图像内容为确定区域和不确定区域并分别处理",{"2":{"76":1}}],["通过自蒸馏知识",{"2":{"75":1}}],["通过约束高置信区域与经过数据增强的同类别图像视图之间的语义一致性来强化模型",{"2":{"75":1}}],["通过精确地将图像中的类别特定语义迁移到提示中",{"2":{"72":1}}],["通过观察到光照成分可以作为一些语义模糊区域的线索",{"2":{"70":1}}],["通过t",{"2":{"67":1}}],["通过计算段在每个图像块内的面积并归一化",{"2":{"159":1}}],["通过计算并可视化特征查询的协方差矩阵",{"2":{"67":1}}],["通过计算相关图并将其传播到预测中",{"2":{"34":1}}],["通过一个由权重w1和偏置b1参数化的线性层对学习到的泛化表示进行特征融合",{"2":{"66":1}}],["通过交叉注意力机制",{"2":{"63":1}}],["通过向llm输入问题",{"2":{"57":1}}],["通过可学习令牌对特征图进行实例级细化",{"2":{"49":1}}],["通过广泛的实验验证",{"2":{"49":1}}],["通过减少可训练的参数",{"2":{"49":1}}],["通过利用更强大的预训练模型和更少的可训练参数",{"2":{"49":1}}],["通过实验研究得出以下结论",{"2":{"162":1}}],["通过实验和消融研究",{"2":{"141":1}}],["通过实验确定了tsf策略中微调编码器层能取得最佳性能",{"2":{"46":1}}],["通过实验分析得出以下结论",{"2":{"38":1}}],["通过尝试预测支持图像的真实掩码来微调网络",{"2":{"44":1}}],["通过求解线性方程得到变换矩阵",{"2":{"44":1}}],["通过测量支持局部原型与查询全局特征之间的相似性",{"2":{"44":1}}],["通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配",{"2":{"44":1}}],["通过查询图像自身构建特定的转换矩阵",{"2":{"41":1}}],["通过非参数测量工具分割查询图像",{"2":{"43":1}}],["通过设计两种新颖的标签传播策略",{"2":{"32":1}}],["通过从关联图中提取精确的类别无关掩码来增强伪标签质量",{"2":{"31":1}}],["通过建模像素对的相似性关系来扩展高置信度像素区域",{"2":{"31":1}}],["通过修改模型采用不同的注意力机制",{"2":{"15":1}}],["通过与其他方法在计算量和准确性方面进行对比实验",{"2":{"15":1}}],["通过增强查询特征的语义和空间感知能力",{"2":{"9":1}}],["从输入通道到输出通道的阻塞连接数",{"2":{"193":1}}],["从使用的包中自动补全命令和环境",{"2":{"182":1}}],["从不自动编译",{"2":{"171":1}}],["从上文整个代码块儿可以看出此规则",{"2":{"170":1}}],["从字幕中提取名词短语并扩展为指代表达",{"2":{"159":1}}],["从字幕中进行指代表达定位",{"2":{"159":1}}],["从大规模图像",{"2":{"158":1}}],["从一个块增加到两个块可提高线性探测和微调性能",{"2":{"143":1}}],["从涂鸦监督初始结果的置信部分提取原型",{"2":{"133":1}}],["从初始预测图的高置信区域中提取对应特征向量",{"2":{"130":1}}],["从维基百科收集标签简短描述",{"2":{"111":1}}],["从wikipedia收集每个类别的简短描述",{"2":{"107":1}}],["从查询分支中额外提取局部原型",{"2":{"96":1}}],["从查询分支中提取局部原型",{"2":{"96":1}}],["从支持特征中提取全局原型",{"2":{"96":2}}],["从1增加到8时",{"2":{"72":1}}],["从解耦特征查询中学习",{"2":{"66":1}}],["从而补全正在编写的代码",{"2":{"171":1}}],["从而可以对",{"2":{"170":1}}],["从而可以更有效地利用未标记的数据",{"2":{"38":1}}],["从而直接完成相应设置",{"2":{"168":1}}],["从而将之配置为高度个性化的编辑器",{"2":{"163":1}}],["从而推动图像分割进入基础模型时代",{"2":{"135":1}}],["从而完成整幅图像的语义分割",{"2":{"127":1}}],["从而通过一致性损失来提高细粒度分割性能",{"2":{"115":1}}],["从而获得更好的性能",{"2":{"110":1}}],["从而在不牺牲精度的情况下",{"2":{"101":1}}],["从而影响",{"2":{"75":1}}],["从而提高分割精度",{"2":{"72":1}}],["从而提高预测的精度",{"2":{"70":1}}],["从而提高无标签数据的利用率",{"2":{"34":1}}],["从而指导泛化表征的学习",{"2":{"63":1}}],["从而选择合适尺度的特征图",{"2":{"57":1}}],["从而为查询图像生成更准确的掩码",{"2":{"44":1}}],["从而增强分割结果",{"2":{"41":1}}],["从而产生不一致的预测结果",{"2":{"20":1}}],["从而减轻固有的偏差",{"2":{"9":1}}],["问题的类驱动涂鸦提升网络",{"2":{"124":1}}],["问题",{"2":{"66":2}}],["早期采用传统交互式分割方法",{"2":{"116":1}}],["早期采用cnn架构",{"2":{"56":1}}],["早期深度学习图像分类成果推动特征可视化工作",{"2":{"116":1}}],["早期通过学习特征映射对齐视觉和文本特征",{"2":{"101":1}}],["早期基于预训练的视觉和语言模型开发",{"2":{"101":1}}],["早期因缺乏大规模标注数据集",{"2":{"70":1}}],["早期u",{"2":{"65":1}}],["网络为捕捉各领域模式会在多通道学习相似模式",{"2":{"64":1}}],["网络往往通过多个通道捕捉相似模式",{"2":{"63":1}}],["浅层特征的特征不对齐问题明显",{"2":{"64":1}}],["只不过相比较目标检测的矩形框",{"2":{"191":1}}],["只是提醒该插件已经更新到了8",{"2":{"167":1}}],["只需将此变量设置为true即可恢复菜单",{"2":{"171":1}}],["只需要更改下图框选出的部分即可",{"2":{"164":1}}],["只需按照图片中所指向图标进行配置即可",{"2":{"163":1}}],["只训练图像编码器和解码器",{"2":{"101":1}}],["只关注通道之间的相关性",{"2":{"66":1}}],["只能泛化到训练中见过的目标域",{"2":{"64":1}}],["只微调编码器的少数参数",{"2":{"44":1}}],["过多的部分掩码可能导致目标对象无法清晰划分",{"2":{"72":1}}],["过去医学图像分割的领域适应研究需目标域样本参与训练",{"2":{"64":1}}],["过滤无关像素",{"2":{"11":1}}],["医学影像",{"2":{"198":1}}],["医学",{"2":{"72":1}}],["医学图像分割",{"2":{"198":1}}],["医学图像分割领域泛化旨在从源域学习泛化到任意未见目标域的语义表示",{"2":{"65":1}}],["医学图像分割技术发展",{"2":{"65":1}}],["医学图像来自不同医院",{"2":{"64":1}}],["医学诊断等领域带来了新机遇",{"2":{"51":1}}],["指标分别以1",{"2":{"63":1}}],["所谓的分割",{"2":{"187":1}}],["所以得到的输出",{"2":{"188":1}}],["所以",{"2":{"188":1}}],["所以生成pdf时",{"2":{"171":1}}],["所以传统的语义分割取得了飞速进步",{"2":{"10":1}}],["所有组件都能提高性能",{"2":{"122":1}}],["所提方法显著优于其他单阶段方法",{"2":{"81":1}}],["所提出的方法显示出更精确和合理的预测",{"2":{"67":1}}],["所提出的解耦特征查询",{"2":{"63":1}}],["理想的泛化表征应确保跨域图像在相同特征通道上呈现相似的模式响应",{"2":{"63":1}}],["域泛化医学图像分割任务要求模型能够从多个源域学习",{"2":{"63":1}}],["域随机化",{"2":{"43":1}}],["jsexport",{"2":{"220":1}}],["js",{"2":{"178":1,"179":1,"182":1}}],["json界面设置",{"2":{"181":1}}],["json",{"2":{"168":1,"170":2,"178":1,"180":1}}],["j",{"2":{"103":1}}],["jarvis",{"2":{"61":1}}],["just",{"2":{"49":1}}],["细节处理欠佳",{"2":{"135":1}}],["细节重建难",{"2":{"51":1}}],["细化预测结果",{"2":{"130":1}}],["细化nightcity数据集",{"2":{"70":1}}],["细粒度分类和分割问题",{"2":{"60":1}}],["绝对提升最高达7",{"2":{"60":1}}],["知识进行开放词汇语义分割",{"2":{"60":1}}],["描述所有对象",{"2":{"59":1}}],["描述图像",{"2":{"57":1}}],["详细提示",{"2":{"59":1}}],["成对相对位置偏差的效果优于其他位置偏差",{"2":{"143":1}}],["成本比较",{"2":{"59":1}}],["成果显著",{"2":{"33":1}}],["交叉注意力在整合先验的方法中表现最优",{"2":{"59":1}}],["取得了最佳的miou",{"2":{"58":1}}],["取得了新的最先进性能",{"2":{"38":1}}],["学习掩码之间的关系",{"2":{"57":1}}],["学习率为2",{"2":{"51":1}}],["预测目标的轮廓",{"2":{"197":1}}],["预测尺度选择分数",{"2":{"57":1}}],["预览编译好的pdf文件",{"2":{"182":1}}],["预训练骨干网络选择",{"2":{"161":1}}],["预训练骨干网络有固有偏差",{"2":{"10":1}}],["预训练解码器",{"2":{"143":1}}],["预训练模型进行全局微调",{"2":{"85":1}}],["预训练的额外语义知识",{"2":{"54":1}}],["关注物体位置",{"2":{"148":1}}],["关键是将图像级的模型适应为像素级的分割任务",{"2":{"101":1}}],["关键组件有效性",{"2":{"98":1}}],["关键组件",{"2":{"57":1}}],["关系注意力",{"2":{"57":1,"59":1}}],["关联图不仅能够轻松实现同类像素的聚类",{"2":{"31":1}}],["属性和关系",{"2":{"59":1}}],["属性和关系先验知识进行分割",{"2":{"60":1}}],["属性和关系先验知识",{"2":{"57":1}}],["属性先验可进一步增强结果",{"2":{"59":1}}],["属性先验和关系先验",{"2":{"54":1}}],["属性的尺度选择和关系注意力可进一步提高性能",{"2":{"59":1}}],["适用性与改进",{"2":{"153":1}}],["适用于具有挑战性的语义分割任务",{"2":{"141":1}}],["适用于低分辨率训练和超高分辨率测试",{"2":{"51":1}}],["适配器",{"2":{"57":1}}],["整体架构",{"2":{"57":1,"88":1,"96":1}}],["整体目标函数是监督损失和无监督损失的组合",{"2":{"34":1}}],["语言联合空间",{"2":{"159":1}}],["语言嵌入合并训练数据效果评估",{"2":{"110":1}}],["语言模型进行分割",{"2":{"101":1}}],["语言模型",{"2":{"101":2,"148":3,"149":1,"153":1}}],["语言信息比支持平均令牌更具类别代表性",{"2":{"72":1}}],["语言信息的优势",{"2":{"72":1}}],["语言预训练模型提取知识",{"2":{"56":1}}],["语义提示转移",{"2":{"72":1}}],["语义提示迁移",{"2":{"72":1}}],["语义导向解耦",{"2":{"70":1}}],["语义注意力模块能显著提升性能",{"2":{"59":1}}],["语义注意力",{"2":{"57":1,"59":1}}],["语义信息有限",{"2":{"56":1}}],["语义分割常用优化目标",{"0":{"207":1}}],["语义分割的思路",{"0":{"190":1}}],["语义分割模型",{"2":{"189":1}}],["语义分割只需要对像素进行分类就行了",{"2":{"187":1}}],["语义分割概述",{"0":{"186":1},"1":{"187":1,"188":1,"189":1,"190":1,"191":1}}],["语义分割领域已经取得很大的进步",{"2":{"115":1}}],["语义分割评估指标",{"0":{"206":1}}],["语义分割评估",{"2":{"110":1}}],["语义分割测试",{"2":{"109":1}}],["语义分割是计算机视觉中的基础任务",{"2":{"148":1}}],["语义分割是计算机视觉的基础任务",{"2":{"105":1,"138":1,"139":1}}],["语义分割是计算机视觉领域的基础且关键任务",{"2":{"94":1}}],["语义分割方法",{"2":{"101":1}}],["语义分割旨在为场景中每个像素分配语义类别",{"2":{"86":1}}],["语义分割",{"2":{"51":1,"70":1,"95":1,"106":1,"109":1,"187":1}}],["语义分割近年来依赖大规模标注数据集取得快速发展",{"2":{"42":1}}],["单目深度估计",{"2":{"110":1}}],["单阶段直接扩展视觉",{"2":{"101":1}}],["单阶段的框架存在不足",{"2":{"101":1}}],["单阶段方法将分类",{"2":{"77":1}}],["单阶段方法虽有改进",{"2":{"56":1}}],["单样本设置下增益更大",{"2":{"97":1}}],["单样本设置下高出2",{"2":{"97":1}}],["单独使用涂鸦或伪标签作为基本监督产生的结果不理想",{"2":{"122":1}}],["单独使用dcam增强类别原型的判别能力可提升2",{"2":{"15":1}}],["单独使用tem增强查询前景特征可使性能提升0",{"2":{"15":1}}],["两阶段的方法由于人类标签的限制",{"2":{"157":1}}],["两阶段的框架存在不足",{"2":{"101":1}}],["两阶段先生成掩码提案再分类",{"2":{"101":1}}],["两阶段方法依赖训练良好的掩码生成器",{"2":{"56":1}}],["两者结合时",{"2":{"98":1}}],["两者结合可进一步提高伪标签和预测标签的质量",{"2":{"81":1}}],["两种传统方法进行比较",{"2":{"80":1}}],["两个基本基线",{"2":{"49":1}}],["全景分割",{"2":{"187":1}}],["全景质量",{"2":{"51":1}}],["全局支持原型生成",{"2":{"96":1}}],["全局上下文建模和transformer等方法",{"2":{"56":1}}],["全监督语义分割取得显著成功",{"2":{"94":1}}],["全微调虽有效",{"2":{"86":1}}],["全微调方法的局限",{"2":{"86":1}}],["后续有多种方法基于此生成语义伪标签以训练分割网络",{"2":{"116":1}}],["后续resnet",{"2":{"106":1}}],["后如clip从大规模图像",{"2":{"101":1}}],["后者如zs3net用生成模型生成像素特征",{"2":{"106":1}}],["后者将transformer用作骨干网络或分割解码器",{"2":{"101":1}}],["后者运用特定算子组合多模态子网络",{"2":{"87":1}}],["后者构建支持",{"2":{"43":1}}],["后deeplab及改进模型成为趋势",{"2":{"65":1}}],["后引入多尺度组合",{"2":{"56":1}}],["固定集语义分割",{"2":{"56":1}}],["尽管mit",{"2":{"131":1}}],["尽管完全没有使用这些测试集的训练图像",{"2":{"104":1}}],["尽管取得了不错的效果",{"2":{"55":1}}],["尽管涂鸦监督语义分割取得了一定进展",{"2":{"20":1}}],["一般来说",{"2":{"191":1}}],["一定要选上",{"2":{"165":1}}],["一定程度上改善了夜间场景表现",{"2":{"70":1}}],["一个池化层到上一个池化层之间的部分认为一个卷积块",{"2":{"194":1}}],["一个通用的片段嵌入模型",{"2":{"156":1}}],["一个数据解决方案",{"2":{"156":1}}],["一个全新的图像分割任务",{"2":{"135":1}}],["一些方法聚焦实例级",{"2":{"140":1}}],["一些专门的工具可能会比sam表现更好",{"2":{"135":1}}],["一致性监督",{"2":{"130":1}}],["一致性损失未在类别层面提供直接监督",{"2":{"128":1}}],["一致性损失",{"2":{"128":1,"129":1}}],["一致性学习和标签扩散三类方法",{"2":{"116":1}}],["一致性学习和标签扩散",{"2":{"115":1}}],["一致",{"2":{"101":1}}],["一方面",{"2":{"86":1}}],["一方面模型可迁移性较弱",{"2":{"85":1}}],["一阶段和两阶段",{"2":{"55":1}}],["一是特征提取阶段",{"2":{"10":1}}],["尺度视觉关注和关系关注三个新的关注模块来利用大语言模型先验",{"2":{"54":1}}],["受视觉变压器启发",{"2":{"95":1}}],["受大语言模型中提示学习方法取得突破的启发",{"2":{"85":1}}],["受此启发",{"2":{"72":1}}],["受其最新突破的启发",{"2":{"54":1}}],["受到广泛关注",{"2":{"32":1}}],["受到越来越多关注",{"2":{"20":1}}],["前端的选项",{"2":{"164":1}}],["前景和背景的局部特征易混淆",{"2":{"94":1}}],["前者如xian等将像素特征转换到语义词嵌入空间",{"2":{"106":1}}],["前者通过融合深浅层特征",{"2":{"101":1}}],["前者通过条件损失对齐子网络嵌入",{"2":{"87":1}}],["前者将支持图像表示为类原型",{"2":{"43":1}}],["前列腺分割基准",{"2":{"67":1}}],["前所未有的能力引入了一种新的范式",{"2":{"54":1}}],["虽有nightcity等大规模夜间数据集及相关方法提出",{"2":{"70":1}}],["虽然之前的开放词汇语义分割方法依赖于来自视觉语言",{"2":{"54":1}}],["虽能取得较好性能",{"2":{"51":1}}],["不然会报错",{"2":{"179":1}}],["不需要进行更改",{"2":{"171":1}}],["不包含外部",{"2":{"170":1}}],["不要对其进行修改",{"2":{"164":1}}],["不怎么好用",{"2":{"164":1}}],["不在本文探讨范围之内",{"2":{"163":1}}],["不能满足本文的任务需求",{"2":{"158":1}}],["不能识别出来在训练集中没有的未知场景",{"2":{"101":1}}],["不依赖语义标签",{"2":{"148":1}}],["不够稳健",{"2":{"135":1}}],["不相连的虚假组件",{"2":{"135":1}}],["不过",{"2":{"101":1,"111":1}}],["不确定特征掩码在大多数情况下比随机特征掩码性能更高",{"2":{"80":1}}],["不确定特征选择分析",{"2":{"80":1}}],["不适合夜间复杂的光照条件",{"2":{"70":1}}],["不足及展望",{"2":{"60":1}}],["不足",{"2":{"51":1,"101":1}}],["不同的是",{"2":{"168":1}}],["不同级别括号用不同颜色标注了",{"2":{"163":1}}],["不同标签体系的数据",{"2":{"104":1}}],["不同超参数",{"2":{"98":1}}],["不同类但纹理相似的对象同时出现时",{"2":{"94":1}}],["不同模态有效信息不同",{"2":{"86":1}}],["不同成像机制的模态存在异质差距",{"2":{"86":1}}],["不同骨干网络中",{"2":{"72":1}}],["不同骨干网络的性能",{"2":{"72":1}}],["不同提示",{"2":{"59":1}}],["不同注意力方法",{"2":{"59":1}}],["不同初始值",{"2":{"36":1}}],["不同采样策略",{"2":{"36":1}}],["目前的研究还很少",{"2":{"158":1}}],["目前",{"2":{"156":1}}],["目前尚不清楚如何设计简单的提示来实现语义和全景分割",{"2":{"135":1}}],["目前采用",{"2":{"51":2}}],["目标属性和目标关系",{"2":{"55":1}}],["目标增强模块",{"2":{"9":1,"12":1,"15":1}}],["未见标签泛化能力评估",{"2":{"110":1}}],["未来计划将此方法应用于其他任务",{"2":{"133":1}}],["未来将探索设计类别注意力策略或使用大规模细粒度数据集来解决该挑战",{"2":{"101":1}}],["未来将研究效率",{"2":{"60":1}}],["未来方向",{"2":{"60":1}}],["未来工作待明确",{"2":{"51":1}}],["未来展望",{"2":{"51":1}}],["未标记数据利用不充分等问题",{"2":{"38":1}}],["即要将输入扩大的倍数",{"2":{"193":1}}],["即要对所有目标都检测出来",{"2":{"187":1}}],["即某一个像素属于哪一类物体",{"2":{"187":1}}],["即从",{"2":{"174":1}}],["即从代码定位到",{"2":{"174":1}}],["即从代码定位到编译出来的",{"2":{"171":1}}],["即从编译出的",{"2":{"171":1}}],["即此命令设置是否将编译文档的选项出现在鼠标右键的菜单中",{"2":{"171":1}}],["即变量设置为false",{"2":{"171":1}}],["即变为",{"2":{"170":1}}],["即需编写者手动编译文档",{"2":{"171":1}}],["即当检测到代码被更改时就自动编译tex文件",{"2":{"171":1}}],["即什么时候自动进行代码的编译",{"2":{"171":1}}],["即可使用",{"2":{"180":1}}],["即可完成",{"2":{"170":1}}],["即可",{"2":{"167":1}}],["即可带来即时的性能提升",{"2":{"151":1}}],["即λs",{"2":{"122":1}}],["即使只在裁剪的边界框区域进行粗略的知识蒸馏",{"2":{"110":1}}],["即使加入额外的网络来提供空间信息",{"2":{"101":1}}],["即使从低分辨率细化到高分辨率",{"2":{"51":1}}],["即l=lseg+λ",{"2":{"66":1}}],["即医学图像采集自不同医院和扫描设备",{"2":{"63":1}}],["即利用大语言模型描述中的目标名字",{"2":{"55":1}}],["即减少神经表示的不确定性以产生可靠结果",{"2":{"28":1}}],["可直接完整复制文末笔者的个人配置到自己的编译器内",{"2":{"178":1}}],["可选",{"0":{"175":1},"1":{"176":1,"177":1,"178":1,"179":1}}],["可自行通过上文方式设置为您想要的快捷键",{"2":{"174":1}}],["可对其设置快捷键",{"2":{"174":1}}],["可能会导致",{"2":{"171":1}}],["可能无法有效消除通道相关性",{"2":{"66":1}}],["可根据其数字来判断安装所需时间",{"2":{"164":1}}],["可以看成是w×h个分类任务",{"2":{"188":1}}],["可以看到的是",{"2":{"164":1,"171":2,"180":1}}],["可以看到",{"2":{"163":1}}],["可以将较小的",{"2":{"193":1}}],["可以将",{"2":{"180":1}}],["可以实时进行跳转",{"2":{"175":1}}],["可以根据上文进行配置",{"2":{"174":1}}],["可以使用自己的tex文件进行测试",{"2":{"173":1}}],["可以更改的代码为",{"2":{"171":1}}],["可以跳过该小节",{"2":{"171":1}}],["可以直接复制上述代码至",{"2":{"170":1}}],["可以直接按ctrl",{"2":{"168":1}}],["可以点击",{"2":{"167":1}}],["可以返回前一页面",{"2":{"164":1}}],["可以查看此篇文章",{"2":{"164":1}}],["可以这么说",{"2":{"163":1}}],["可学习的上采样",{"2":{"193":1}}],["可学习提示令牌长度等因素对模型性能的影响",{"2":{"152":1}}],["可学习类提示和动量编码器的重要性",{"2":{"152":1}}],["可将语义分割推广到无限类别的范围",{"2":{"149":1}}],["可轻松合并多数据集",{"2":{"111":1}}],["可降低标注成本",{"2":{"76":1}}],["可分为优化和度量两类",{"2":{"72":1}}],["可提升现有日间方法在夜间的性能",{"2":{"70":1}}],["可处理低分辨率训练和超高清测试之间的分辨率差距",{"2":{"51":1}}],["可视化比较显示",{"2":{"119":1}}],["可视化的类到补丁注意力图显示类令牌可以自适应地学习关注目标区域",{"2":{"80":1}}],["可视化分割结果",{"2":{"67":1}}],["可视化对比",{"2":{"51":1}}],["可视化结果显示",{"2":{"37":1,"72":1}}],["泛化能力不足",{"2":{"105":1}}],["泛化能力",{"2":{"51":1}}],["有三个参数变量",{"2":{"179":1}}],["有三个选项",{"2":{"171":1}}],["有三种变量参数",{"2":{"179":1}}],["有的时候",{"2":{"175":1}}],["有的地方需要更改路径",{"2":{"163":1}}],["有以下三种方法",{"2":{"174":1}}],["有两个变量",{"2":{"171":1}}],["有具体说明",{"2":{"163":1}}],["有望推动相关领域的研究发展",{"2":{"162":1}}],["有时会生成小的",{"2":{"135":1}}],["有时会丢弃一些对象",{"2":{"120":1}}],["有方法合并分割数据集提升性能和泛化能力",{"2":{"106":1}}],["有助于零样本标签的分割",{"2":{"107":1}}],["有助于最小化领域差距",{"2":{"67":1}}],["有助于聚合特征以重建高分辨率掩码上的细节",{"2":{"51":1}}],["有助于提高性能直至收敛",{"2":{"51":1}}],["有效提升推理速度",{"2":{"101":1}}],["有效提升相似类别的可区分性",{"2":{"93":1}}],["有效提升伪标签的整体质量",{"2":{"38":1}}],["有效捕捉多模态数据的共有统计规律",{"2":{"85":1}}],["有效降低计算成本",{"2":{"51":1}}],["有效减少了背景干扰",{"2":{"16":1}}],["推动了开放词汇语义分割的发展",{"2":{"148":1,"149":1}}],["推动了半监督语义分割技术的发展",{"2":{"33":1}}],["推理时",{"2":{"109":1}}],["推理连续性的影响",{"2":{"51":1}}],["二",{"2":{"67":1}}],["二者协同作用能提升性能",{"2":{"51":1}}],["二是特征匹配阶段",{"2":{"10":1}}],["验证cam和隐式函数都是crm不可或缺的部分",{"2":{"51":1}}],["验证了sed方法的有效性",{"2":{"101":1}}],["验证了smt",{"2":{"46":1}}],["验证了corrmatch不同组件的有效性",{"2":{"36":1}}],["验证了该方法的有效性",{"2":{"9":1}}],["掩码的语义聚类",{"2":{"150":1}}],["掩码相似度图",{"2":{"150":1}}],["掩码集成到clip特征",{"2":{"150":1}}],["掩码预测和分类能力",{"2":{"57":1}}],["掩码细节和整体分割效果都有显著改善",{"2":{"51":1}}],["掩码注意力",{"2":{"15":1}}],["应如下图页面所示",{"2":{"165":1}}],["应用前景",{"2":{"144":1}}],["应用unc",{"2":{"80":1}}],["应用示例",{"2":{"51":1}}],["应运而生",{"2":{"42":1,"72":1}}],["它并不是正向卷积的完全逆过程",{"2":{"193":1}}],["它将每个像素分类为属于对象类以及该类的实体",{"2":{"187":1}}],["它是将每个像素分类为属于对象类的过程",{"2":{"187":1}}],["它不仅能够对代码高亮",{"2":{"163":1}}],["它对于括号根本就没有高亮",{"2":{"163":1}}],["它们在分类不同粒度的片段方面仍然受到限制",{"2":{"157":1}}],["它们的分割性能得到增强",{"2":{"51":1}}],["它主要关注对象最具判别性的区域",{"2":{"76":1}}],["它可以利用标签传播和相关匹配来发现更准确的高置信度区域",{"2":{"38":1}}],["定义完成后",{"2":{"171":1}}],["定性比较",{"2":{"161":1}}],["定性结果也证明了其有效性",{"2":{"99":1}}],["定性结果也证明了rifenet的有效性",{"2":{"97":1}}],["定性结果展示",{"2":{"51":1}}],["定性评估",{"2":{"72":1}}],["定性分析",{"2":{"37":1}}],["定量结果评估",{"2":{"51":1}}],["扰动掩码是在真实掩码上随机扰动得到",{"2":{"51":1}}],["总共有ncl个不同的分类",{"2":{"191":1}}],["总训练图像约200万张",{"2":{"109":1}}],["总损失函数l是标准的二元交叉熵损失和dice损失",{"2":{"66":1}}],["总推理时间也不到",{"2":{"51":1}}],["总推理时间不到cascadepsp的一半",{"2":{"51":1}}],["总步数45",{"2":{"51":1}}],["训练与损失",{"2":{"159":1}}],["训练时",{"2":{"109":1}}],["训练时剪裁图像",{"2":{"101":1}}],["训练数据",{"2":{"109":1}}],["训练数据集",{"2":{"51":1}}],["训练多领域语义分割模型是提升模型鲁棒性和泛化能力的自然途径",{"2":{"105":1}}],["训练集",{"2":{"101":1,"160":1}}],["训练",{"2":{"58":1}}],["训练资源限制",{"2":{"51":1}}],["训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块",{"2":{"51":1}}],["训练设置",{"2":{"51":1}}],["进入设置页面",{"2":{"168":1}}],["进入代码设置页面",{"2":{"168":1}}],["进入镜像列表",{"2":{"164":1}}],["进入",{"2":{"164":1}}],["进而计算加权平均嵌入",{"2":{"159":1}}],["进一步增强了背景和前景的区分度",{"2":{"99":1}}],["进一步证明了该方法的优越性",{"2":{"79":1}}],["进一步提高分割性能",{"2":{"47":1}}],["进行分屏",{"2":{"180":1}}],["进行文件内容查看",{"2":{"174":1}}],["进行解压",{"2":{"173":1}}],["进行编译的速度比",{"2":{"171":1}}],["进行查看",{"2":{"167":1}}],["进行",{"2":{"166":1}}],["进行等待即可",{"2":{"164":1}}],["进行安装",{"2":{"164":1,"166":1,"167":1}}],["进行重新点击",{"2":{"164":1}}],["进行评估",{"2":{"109":3}}],["进行亲和性学习",{"2":{"78":1}}],["进行差异化处理",{"2":{"75":1}}],["进行超高清图像评估",{"2":{"51":1}}],["测试文件编译",{"0":{"174":1}}],["测试所用的",{"2":{"173":1}}],["测试结果从pascal",{"2":{"119":1}}],["测试数据",{"2":{"109":1}}],["测试数据集",{"2":{"51":1}}],["测试时直接放缩图像到",{"2":{"101":1}}],["测试时自微调策略",{"2":{"44":1}}],["测试集",{"2":{"101":1,"160":1}}],["测试",{"2":{"58":1}}],["包含以下创新",{"2":{"138":1}}],["包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像",{"2":{"135":1}}],["包含",{"2":{"101":2}}],["包含大约",{"2":{"101":1}}],["包含2975张训练图像和500张验证图像",{"2":{"70":1}}],["包含36",{"2":{"51":1}}],["包括全局语义聚类",{"2":{"152":1}}],["包括标准fss",{"2":{"72":1}}],["包括交并比",{"2":{"51":1}}],["包括硬无监督损失",{"2":{"36":1}}],["包括采用自对齐模块",{"2":{"15":1}}],["遵循cascadepsp的设置",{"2":{"51":1}}],["还可以促进其他下游任务",{"2":{"156":1}}],["还出现利用像素相关性",{"2":{"116":1}}],["还有基于自注意力机制和transformer的网络被应用",{"2":{"70":1}}],["还能在无预定义候选类别的情况下预测ov分割结果",{"2":{"60":1}}],["还能提升现有全景分割模型的性能",{"2":{"51":1}}],["还能提升现有全景分割模型性能",{"2":{"51":1}}],["还在重新标注的pascal",{"2":{"51":1}}],["还包含良好的形状信息",{"2":{"32":1}}],["还包含了被以往研究忽略的优质形状信息",{"2":{"31":1}}],["隐式函数表示",{"2":{"51":1}}],["级联解码器方法能取得较好效果",{"2":{"51":1}}],["传播方法有计算和内存限制",{"2":{"51":1}}],["传统语义分割方法主要有基于fcn和基于transformer的方法",{"2":{"101":1}}],["传统的方法只能分割训练集的种类",{"2":{"101":1}}],["传统的深度白化变换",{"2":{"66":1}}],["传统方法存在语义模糊和类间相似性问题",{"2":{"99":1}}],["传统方法常受限于语义模糊性和类间相似性",{"2":{"93":1}}],["传统方法聚焦提升模型跨多未见领域的预测准确性",{"2":{"49":1}}],["传统dgss方法的局限",{"2":{"49":1}}],["针对这一局限性",{"2":{"127":1}}],["针对这一矛盾",{"2":{"63":1}}],["针对150个类别的miou得分为31",{"2":{"101":1}}],["针对不确定区域",{"2":{"75":1}}],["针对",{"2":{"51":1}}],["丢失了细节并破坏了全局上下文",{"2":{"51":1}}],["无论何时",{"2":{"171":1}}],["无论是否编译成功",{"2":{"171":1}}],["无论是定性可视化结果还是定量评估指标",{"2":{"93":1}}],["无引导查询原型的无标签分支性能比基线更差",{"2":{"98":1}}],["无标签增强和多级原型策略共同作用时",{"2":{"99":1}}],["无标签图像数量设置为2时效果最佳",{"2":{"98":1}}],["无标签分支设计选择",{"2":{"98":1}}],["无标签分支的积极影响减小",{"2":{"97":1}}],["无标签分支与查询分支共享参数",{"2":{"96":1}}],["无标签数据与有标签图像的比例从2降至0",{"2":{"97":1}}],["无标签数据特征增强",{"2":{"96":1}}],["无需进行更改",{"2":{"171":1}}],["无需在意",{"2":{"167":1}}],["无需理会",{"2":{"164":1}}],["无需额外无标注数据支持且不增加计算负担",{"2":{"93":1}}],["无需微调",{"2":{"51":1}}],["无法充分利用预训练模型知识获得通用表示",{"2":{"86":1}}],["无法在输出掩码上构建细粒度细节",{"2":{"51":1}}],["无监督损失包括无监督硬损失",{"2":{"34":1}}],["超参数调整",{"2":{"122":1}}],["超参数设置为",{"2":{"119":1}}],["超过现有最先进的方法",{"2":{"110":1}}],["超过使用vit",{"2":{"58":1}}],["超过fc",{"2":{"58":1}}],["超高清图像",{"2":{"51":1}}],["超高分辨率图像也给经典图像分割方法带来了挑战",{"2":{"51":1}}],["超越现有方法",{"2":{"49":1}}],["工业缺陷检测等众多视觉任务中应用广泛",{"2":{"94":1}}],["工业缺陷检测",{"2":{"51":1}}],["随着伪标签基础准确率的提高",{"2":{"123":1}}],["随着丢弃或收缩比率的增加",{"2":{"120":1}}],["随着无标签图像数量增加",{"2":{"98":1}}],["随着卷积神经网络和基于transformer方法的发展",{"2":{"94":1}}],["随着传感器技术发展",{"2":{"86":1}}],["随着大语言模型的发展",{"2":{"55":1}}],["随着分辨率增加",{"2":{"51":1}}],["随着相机和显示设备的快速发展",{"2":{"51":1}}],["随机进入另一镜像网站进行下载尝试",{"2":{"164":1}}],["随机初始化提示",{"2":{"72":1}}],["随机iou阈值在0",{"2":{"51":1}}],["随机采样效果更好",{"2":{"36":1}}],["更多详情可以访问",{"2":{"171":1}}],["更多的语义迁移次数通常能带来更高的分割精度",{"2":{"72":1}}],["更多的缩放比例意味着推理分辨率的连续性更好",{"2":{"51":1}}],["更新",{"2":{"141":1}}],["更能反映类别间的语义相似性",{"2":{"107":1}}],["更有效地减少了特征冗余",{"2":{"66":1}}],["更适用于实际应用",{"2":{"60":1}}],["更重要的是",{"2":{"51":1}}],["更接近现实场景",{"2":{"32":1}}],["逐步聚合特征",{"2":{"51":1}}],["连续细化模型",{"2":{"51":1}}],["往往会错误地激活与目标类别无关的物体",{"2":{"72":1}}],["往往难以很好地平衡准确性和计算成本",{"2":{"51":1}}],["往往对背景噪声过于敏感",{"2":{"9":1}}],["等等",{"2":{"189":1}}],["等功能",{"2":{"173":1}}],["等待安装完成",{"2":{"166":1,"167":1}}],["等会儿会消失",{"2":{"164":1}}],["等数据集上",{"2":{"138":1}}],["等指标",{"2":{"51":1}}],["等",{"0":{"51":1},"2":{"51":1}}],["等方法缩小域差距",{"2":{"43":1}}],["香港中文大学",{"0":{"51":1}}],["quot",{"2":{"164":1,"165":1,"166":6,"167":6,"179":4}}],["queries",{"0":{"61":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"62":1}}],["querying",{"2":{"155":1}}],["query",{"2":{"8":1,"40":6,"62":1}}],["qualities",{"2":{"113":1}}],["qualitative",{"2":{"92":1}}],["quality",{"0":{"50":1}}],["quantitative",{"2":{"51":1,"92":1}}],["写作启发",{"2":{"49":1,"135":1}}],["证明该方法的有效性源于学习到的判别性和语义特征",{"2":{"98":1}}],["证明其能够自适应地生成不同的部分掩码",{"2":{"72":1}}],["证明了mevt能够学习到具有细粒度和全局上下文感知能力的视觉表示",{"2":{"141":1}}],["证明了利用图像级类别信息生成全局伪标签用于基于涂鸦的弱监督语义分割的有效性",{"2":{"124":1}}],["证明了其在少样本语义分割任务中的有效性",{"2":{"96":1}}],["证明了该框架的优越性和泛化性",{"2":{"90":1}}],["证明了gopt已经达到了sota的标准",{"2":{"90":1}}],["证明了基于transformer的单阶段训练的有效性",{"2":{"81":1}}],["证明了强化不确定特征的有效性",{"2":{"80":1}}],["证明了动态类别感知提示范式对少样本分割",{"2":{"72":1}}],["证明了模型的开放词汇能力",{"2":{"58":1}}],["证明了vfms在dgss领域的巨大潜力以及rein方法的有效性",{"2":{"49":1}}],["证明了优化类别原型和查询特征的必要性",{"2":{"15":1}}],["证实其强大泛化能力",{"2":{"49":1}}],["领域展现出强大的零样本和少样本泛化能力",{"2":{"135":1}}],["领域和未见过类别上的泛化能力仍然有限",{"2":{"104":1}}],["领域",{"2":{"72":1,"76":1}}],["领域泛化研究",{"2":{"65":1}}],["领域泛化语义分割",{"2":{"49":2}}],["领域偏移使深度学习模型同一通道中不同领域的医学图像激活模式差异大",{"2":{"64":1}}],["领域广义语义分割",{"2":{"49":1}}],["首次在",{"2":{"49":1}}],["首先是类感知单模态提示器",{"2":{"85":1}}],["首先通过网络编码器后的线性层提取特征",{"2":{"34":1}}],["首先",{"2":{"31":1}}],["大特征图",{"2":{"193":1}}],["大幅超越现有最先进的两阶段方法",{"2":{"162":1}}],["大幅超越现有方法",{"2":{"49":1}}],["大模型助力开放词汇语义分割",{"2":{"149":1}}],["大模型易过拟合而浅细化网络细化能力有限等问题",{"2":{"51":1}}],["大规模预训练视觉",{"2":{"148":1,"149":1}}],["大小",{"2":{"101":1,"175":1}}],["大多数fss方法直接使用预训练编码器",{"2":{"72":1}}],["大多数少样本分割",{"2":{"72":1}}],["大多数现有的方法尝试使用预训练的视觉",{"2":{"101":1}}],["大多数现有的语义分割方法都是为白天场景设计的",{"2":{"70":1}}],["大多数现有方法通过插值对最终预测进行4到8倍上采样",{"2":{"51":1}}],["大多现存的语义分割方法都是基于白天场景开发的",{"2":{"70":1}}],["大语言模型在自然语言处理",{"2":{"135":1}}],["大语言模型提供了更丰富的知识库",{"2":{"54":1}}],["大语言模型",{"2":{"54":1,"56":1}}],["大量下游任务实验表明",{"2":{"90":1}}],["大量实验验证了该方案的先进性",{"2":{"63":1}}],["大量实验证明llmformer及各注意力模块有效",{"2":{"60":1}}],["大量实验表明",{"2":{"47":1,"69":1,"70":1}}],["大量的输入像素在计算上代价高昂",{"2":{"51":1}}],["大量的消融实验和中间可视化验证了所提解决方案的有效性",{"2":{"28":1}}],["冻结的vfms性能优于先前dgss方法",{"2":{"49":1}}],["得出以下结论",{"2":{"101":1,"133":1,"144":1,"153":1}}],["得出结论",{"2":{"49":1}}],["得到段在每个块内的权重",{"2":{"159":1}}],["得到每个掩码的clip特征",{"2":{"150":1}}],["得到强大鲁棒的分割模型",{"2":{"111":1}}],["得到最终的逐像素分割结果",{"2":{"96":1}}],["得到增强的查询特征",{"2":{"96":1}}],["得到另一种预测表示",{"2":{"34":1}}],["得到基于空间分布概率的掩码用于精确的定位",{"2":{"12":1}}],["批量大小等进行训练",{"2":{"49":1}}],["迭代次数",{"2":{"49":1}}],["方法清晰",{"2":{"135":1}}],["方法构成",{"2":{"101":1}}],["方法设计有效",{"2":{"81":1}}],["方法在验证集和测试集上的miou分别达到75",{"2":{"79":1}}],["方法不仅超越了当前最先进的同类方案",{"2":{"75":1}}],["方法通常会直接使用预训练的编码器",{"2":{"72":1}}],["方法优势",{"2":{"70":1}}],["方法创新",{"2":{"69":1,"144":1}}],["方法有效性",{"2":{"60":1,"153":1,"162":1}}],["方法高出7",{"2":{"54":1}}],["方法的性能",{"2":{"49":1}}],["方法",{"2":{"49":1,"66":1,"72":1}}],["方法多样",{"2":{"33":1}}],["设置vscode内部查看生成的pdf文件",{"2":{"182":1}}],["设置为onfaild",{"2":{"182":1}}],["设置为true时",{"2":{"171":1}}],["设置是否自动编译",{"2":{"182":1}}],["设置其位置参数",{"2":{"179":1}}],["设置外部查看器启动文件sumatrapdf",{"2":{"179":1}}],["设置快捷键步骤如下",{"2":{"174":1}}],["设置边距",{"2":{"173":1}}],["设置页面交互能力较强",{"2":{"168":1}}],["设置页面和代码设置页面均为设置页面",{"2":{"168":1}}],["设置页面",{"2":{"168":1}}],["设置特定学习率",{"2":{"49":1}}],["设置",{"2":{"49":1}}],["设计segment",{"2":{"135":1}}],["设计相应的模型架构",{"2":{"135":1}}],["设计了类感知单模态提示器",{"2":{"90":1}}],["设计了基于激活的掩码策略",{"2":{"81":1}}],["设计了像素传播和区域传播两种标签传播策略",{"2":{"38":1}}],["设计语义提示转移",{"2":{"72":1}}],["设计语义导向解纠缠框架",{"2":{"70":1}}],["设计优化策略",{"2":{"49":1}}],["设计目的",{"2":{"12":3}}],["因其耗时较长",{"2":{"119":1}}],["因缺乏大规模多模态训练集",{"2":{"87":1}}],["因为网络的设计未必那么完美",{"2":{"188":1}}],["因为如果涉及到",{"2":{"171":1}}],["因为毕竟涉及到代码",{"2":{"171":1}}],["因为这些错误和警告信息能够从终端中获取",{"2":{"171":1}}],["因为它可以通过新的",{"2":{"171":1}}],["因为它们能利用llm先验来改进开放词汇语义分割",{"2":{"59":1}}],["因为sam更侧重于通用性和广泛适用性",{"2":{"135":1}}],["因为强增强可能会干扰分割目标",{"2":{"80":1}}],["因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战",{"2":{"15":1}}],["因此它可以零样本迁移到不同的图像类型和任务",{"2":{"135":1}}],["因此弱监督学习方法受关注",{"2":{"129":1}}],["因此备受研究人员推崇",{"2":{"127":1}}],["因此在夜间场景中往往表现不佳",{"2":{"70":1}}],["因此",{"2":{"49":1,"51":1,"72":2,"94":1,"120":1,"128":1,"135":1,"139":1,"148":1}}],["先按照一定的比例通过补",{"2":{"193":1}}],["先给出效果图",{"2":{"170":1}}],["先利用分层编码器生成像素级图像",{"2":{"101":1}}],["先分离再解析",{"2":{"70":1}}],["先解耦再解析",{"2":{"70":1}}],["先前dgss方法多采用传统骨干网络",{"2":{"49":1}}],["先以支持原型为参考在查询特征中选择匹配置信度高的点",{"2":{"12":1}}],["​",{"2":{"49":1,"70":1,"72":4}}],["仅线性层归一化",{"2":{"159":1}}],["仅添加ldc会使模型性能下降到与仅使用lsegc几乎相同的水平",{"2":{"122":1}}],["仅训练不到1",{"2":{"90":1}}],["仅更新分组提示器和分割头的梯度值",{"2":{"88":1}}],["仅微调少量视觉提示参数",{"2":{"88":1}}],["仅微调解码器",{"2":{"72":1}}],["仅需微调模型不足",{"2":{"85":1}}],["仅将支持图像或查询图像的目标语义转移到提示中会导致不同程度的性能下降",{"2":{"72":1}}],["仅在冻结的骨干网络中增加1",{"2":{"49":1}}],["仅使用图像级标签的弱监督语义分割",{"2":{"75":1}}],["仅使用语义分割标注训练就能生成类似抠图的结果",{"2":{"51":1}}],["仅使用涂鸦注释进行语义分割会导致预测结果不确定和不一致",{"2":{"28":1}}],["仅使用基于空间分布概率的分割图时",{"2":{"15":1}}],["仅使用基于语义相似度的掩码可使模型性能提升1",{"2":{"15":1}}],["值得一提的是",{"2":{"49":1,"93":1}}],["效果验证",{"2":{"101":1}}],["效果出乎意料地优于完全参数微调",{"2":{"49":1}}],["效率提升",{"2":{"38":1}}],["获得更好的泛化能力",{"2":{"49":1}}],["获取与这些表达对应的边界框",{"2":{"159":1}}],["获取图像的全面描述",{"2":{"57":1}}],["获取",{"2":{"41":1}}],["上采样实现方法",{"0":{"210":1}}],["上采样",{"0":{"193":1,"208":1},"1":{"209":1,"210":1,"211":1}}],["上查看",{"2":{"180":1}}],["上面代码串中记得进行",{"2":{"179":1}}],["上",{"2":{"174":1}}],["上的实验结果表明",{"2":{"141":1}}],["上的实验表明",{"2":{"101":1}}],["上进行测试",{"2":{"109":1}}],["上进行了广泛实验",{"2":{"88":1}}],["上分别取得",{"2":{"104":1}}],["上预训练的",{"2":{"78":1}}],["上海人工智能实验室",{"0":{"49":1}}],["上取得了一定成果",{"2":{"33":1}}],["移除任何一个模块都会导致平均性能下降",{"2":{"46":1}}],["与上文相同",{"2":{"179":1}}],["与通用软件安装过程一致",{"2":{"176":1}}],["与端到端方法相比平均性能最佳",{"2":{"160":1}}],["与最先进的开放词汇语义分割方法在ade20k和pascal",{"2":{"160":1}}],["与最先进的patnet相比",{"2":{"45":1}}],["与使用图像级监督的方法相比",{"2":{"151":1}}],["与使用图像级标签和现成显著性图的多阶段方法相比",{"2":{"79":1}}],["与字幕监督方法相比",{"2":{"147":1}}],["与当前最先进的方法tel相比",{"2":{"131":1}}],["与基线方法leres相比",{"2":{"110":1}}],["与基线相比",{"2":{"15":1,"72":1,"122":1}}],["与lseg方法在不同粒度的标签集上进行比较",{"2":{"110":1}}],["与lseg比较",{"2":{"110":1}}],["与其他预训练权重相比",{"2":{"110":1}}],["与其他数据增强方法的比较结果表明",{"2":{"80":1}}],["与人工统一标签体系不同",{"2":{"104":1}}],["与普通的transformer相比",{"2":{"101":1}}],["与原始少样本任务设置不符",{"2":{"95":1}}],["与局部原型",{"2":{"93":1}}],["与全局平均池化",{"2":{"80":1}}],["与传统的像素级监督语义分割相比",{"2":{"76":1}}],["与传统的视觉语言预训练相比",{"2":{"54":1}}],["与不确定区域",{"2":{"75":1}}],["与此不同",{"2":{"72":1}}],["与nightcity",{"2":{"70":1}}],["与sota方法的对比",{"0":{"89":1}}],["与sota方法的对比实验",{"2":{"70":1}}],["与spt协同工作",{"2":{"72":1}}],["与san相比",{"2":{"58":1}}],["与以往方法将光照信息与特征混合的做法不同",{"2":{"70":1}}],["与现有方法在pascal",{"2":{"131":1}}],["与现有方法相比",{"2":{"67":1}}],["与现有最先进的方法在6个零样本数据集上进行比较",{"2":{"110":1}}],["与现有最先进的dcama相比",{"2":{"97":1}}],["与现有最优方法对比",{"2":{"67":1}}],["与次优的ram",{"2":{"67":1}}],["与次优方法相比",{"2":{"67":1}}],["与每个特征的lirdwt的组合",{"2":{"66":1}}],["与zegformer和ov",{"2":{"59":1}}],["与fc",{"2":{"58":1}}],["与之前基于vl预训练的工作不同",{"2":{"54":1}}],["与cascadepsp的iou相当",{"2":{"51":1}}],["与迁移学习",{"2":{"45":1}}],["避免训练过程中的不稳定性和预训练知识的遗忘",{"2":{"150":1}}],["避免过度依赖支持图像导致过拟合",{"2":{"47":1}}],["避免对支持图像过拟合",{"2":{"44":1}}],["避免固定阈值过严或过松对模型收敛的不利影响",{"2":{"34":1}}],["分类模型",{"2":{"188":1}}],["分类",{"2":{"187":1}}],["分层编码器",{"2":{"138":1}}],["分割的指标主要有两个",{"2":{"191":1}}],["分割",{"2":{"187":1}}],["分割种类的增加也会增加计算资源",{"2":{"101":1}}],["分割性能提升有限",{"2":{"70":1}}],["分割细化",{"2":{"51":1}}],["分别提高了8",{"2":{"58":1}}],["分别是语义注意力",{"2":{"57":1}}],["分别基于支持前景特征和背景特征与查询特征构建4d相关张量",{"2":{"44":1}}],["分析crm和隐式函数对不同分辨率下性能的影响",{"2":{"51":1}}],["分析数据",{"2":{"49":1}}],["分辨率图像的细化技术能提升分割质量",{"2":{"51":1}}],["分为判别式和生成式方法",{"2":{"106":1}}],["分为单阶段和两阶段方法",{"2":{"56":1}}],["分为域自适应语义分割",{"2":{"43":1}}],["分为四种",{"2":{"21":1}}],["探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性",{"2":{"44":1}}],["探索查询图像与支持图像前景和背景之间的超相关性",{"2":{"41":1}}],["自行选择",{"2":{"165":1}}],["自带的",{"2":{"164":1}}],["自监督预训练策略",{"2":{"141":1}}],["自监督学习",{"2":{"138":1,"139":1,"140":1}}],["自监督损失用于无标签分支的训练",{"2":{"96":1}}],["自注意力机制等的方法",{"2":{"116":1}}],["自全卷积网络",{"2":{"95":1}}],["自蒸馏",{"2":{"81":1}}],["自蒸馏机制",{"2":{"78":1}}],["自蒸馏方法",{"2":{"77":1}}],["自适应特征变换",{"2":{"44":1}}],["自匹配变换模块",{"2":{"44":1}}],["生成框",{"2":{"159":1}}],["生成的",{"2":{"171":1}}],["生成的掩码引导预训练视觉",{"2":{"148":1}}],["生成的未标记图像和掩码来实现的",{"2":{"147":1}}],["生成的伪分割标签监督",{"2":{"78":1}}],["生成全局考虑的伪标签",{"2":{"117":1}}],["生成新的跨模态对齐诱导提示",{"2":{"88":1}}],["生成特定于模态的提示",{"2":{"88":1}}],["生成对应的rgb标记和辅助模态标记",{"2":{"88":1}}],["生成类激活图",{"2":{"76":1}}],["生成并监督前景和背景预测掩码",{"2":{"47":1}}],["生成预测的查询前景掩码和背景掩码",{"2":{"44":1}}],["生成更细粒度的预测查询掩码",{"2":{"44":1}}],["生成鲁棒的支持类别原型",{"2":{"12":1}}],["近年分为正则化损失",{"2":{"116":1}}],["近年来",{"2":{"10":1,"49":1,"138":1,"139":1,"148":1,"156":1}}],["近期方法未能捕捉到正确的全局语义",{"2":{"119":1}}],["近期研究聚焦于多尺度特征融合",{"2":{"95":1}}],["近期也开始应用于视觉任务",{"2":{"87":1}}],["近期有基于nightcity数据集的方法",{"2":{"70":1}}],["近期vision",{"2":{"65":1}}],["近期提出了一些方法",{"2":{"43":1}}],["易导致过拟合",{"2":{"42":1}}],["易优先提取无关特征",{"2":{"10":1}}],["被提出",{"2":{"42":1}}],["被提出用于模拟有限数据和多类别的真实世界场景",{"2":{"10":1}}],["跨数据集测试",{"2":{"101":1}}],["跨领域密集预测",{"2":{"106":1}}],["跨领域特征对齐",{"2":{"67":1}}],["跨领域少样本语义分割",{"2":{"42":1}}],["跨域fss",{"2":{"72":1}}],["跨域语义分割",{"2":{"43":1}}],["跨域少样本语义分割",{"2":{"41":1,"43":1}}],["常见的分割模型",{"0":{"189":1}}],["常见的策略",{"2":{"51":1}}],["常提供无效的跨模态融合",{"2":{"86":1}}],["常用的上采样包括转置卷积",{"2":{"193":1}}],["常用的固定阈值筛选伪标签方法难以有效利用未标注数据",{"2":{"32":1}}],["常用数据集规模远小于imagenet",{"2":{"49":1}}],["常采用元学习",{"2":{"42":1}}],["策略",{"2":{"41":1}}],["策略有效",{"2":{"38":1}}],["模仿人类视觉感知模式",{"2":{"72":1}}],["模块和块尺度参数可训练",{"2":{"159":1}}],["模块",{"2":{"41":1,"57":1}}],["模型优势",{"2":{"141":1}}],["模型架构",{"2":{"141":1}}],["模型创新",{"2":{"135":1}}],["模型和数据集",{"2":{"135":1}}],["模型性能下降",{"2":{"120":1}}],["模型性能可能受语言模型表示限制",{"2":{"111":1}}],["模型配置",{"2":{"119":1,"122":1}}],["模型能分割零样本标签",{"2":{"111":1}}],["模型在",{"2":{"104":1}}],["模型在识别近义词类别时存在困难",{"2":{"101":1}}],["模型有时难以区分近义词类别",{"2":{"101":1}}],["模型设定",{"2":{"101":1}}],["模型分割效果提升",{"2":{"98":1}}],["模型训练方式",{"2":{"87":1}}],["模型的输入是什么",{"2":{"188":1}}],["模型的输入和输出",{"0":{"188":1}}],["模型的可训练参数更少",{"2":{"59":1}}],["模型的注意力方法显著优于掩码注意力和tsg注意力",{"2":{"59":1}}],["模型实现",{"2":{"51":1}}],["模型易过拟合或细化能力有限等问题",{"2":{"51":1}}],["模型结构图",{"2":{"49":1}}],["模型框架",{"2":{"34":1}}],["模型时",{"2":{"31":1}}],["模型整体比基线提升了3",{"2":{"15":1}}],["此功能不受官方支持",{"2":{"182":3}}],["此属性必须是字符串数组",{"2":{"182":1}}],["此路径为",{"2":{"179":1}}],["此命令作用于",{"2":{"179":1}}],["此参数为下文进行pdf内部查看和外部查看进行切换的关键参数",{"2":{"179":1}}],["此处就不再赘述",{"2":{"180":1}}],["此处需要您根据自身情况进行路径更改",{"2":{"179":1}}],["此处设置为auto",{"2":{"179":1}}],["此处选择",{"2":{"179":1}}],["此处快捷键的选择为上文设置",{"2":{"174":1}}],["此处笔者使用的为double",{"2":{"171":1}}],["此处为默认配置",{"2":{"171":1}}],["此菜单默认状态下停用",{"2":{"171":1}}],["此项笔者设置为never",{"2":{"171":1}}],["此工作为构建开放词汇图像分割的基础模型和基于片段的表征学习提供了参考",{"2":{"162":1}}],["此框架简单且强大",{"2":{"144":1}}],["此前探索了多种图像表示解纠缠方法",{"2":{"70":1}}],["此前采用无监督域适应技术将白天知识迁移到夜间",{"2":{"70":1}}],["此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割",{"2":{"41":1}}],["此外",{"2":{"32":1,"41":1,"42":1,"54":1,"63":1}}],["特定领域表现不佳",{"2":{"135":1}}],["特定提示设计困难",{"2":{"135":1}}],["特定场景表现弱",{"2":{"135":1}}],["特征图填充宽度",{"2":{"193":1}}],["特征学习",{"2":{"141":1}}],["特征进行预训练",{"2":{"141":1}}],["特征增强",{"2":{"130":1}}],["特征提取",{"2":{"130":1}}],["特征提取位置",{"2":{"36":1}}],["特征原型在计算机视觉任务中用于增强模型识别能力",{"2":{"129":1}}],["特征激活",{"2":{"96":1}}],["特征激活模块",{"2":{"96":1}}],["特征融合",{"2":{"88":1}}],["特征融合和像素匹配三种方法",{"2":{"72":1}}],["特征自强化分析",{"2":{"80":1}}],["特征查询和松弛深度白化变换",{"2":{"68":1}}],["特征问题",{"2":{"64":1}}],["特别是公式比较多的数学专业",{"2":{"163":1}}],["特别是在元测试阶段",{"2":{"41":1}}],["特别地",{"2":{"54":1}}],["将synctex转发到外部查看器时要执行的命令",{"2":{"182":1}}],["将完整代码复制到自己的",{"2":{"180":1}}],["将之下载后",{"2":{"173":1}}],["将之添加到环境变量",{"2":{"164":1}}],["将编译方式",{"2":{"171":1}}],["将在下文提及",{"2":{"171":1}}],["将边界框转换为掩码",{"2":{"159":1}}],["将解码器纳入预训练框架显著提高了线性探测miou",{"2":{"143":1}}],["将图像分割成一个个小的区域",{"2":{"190":1}}],["将图像的两个增强视图分别输入到教师网络和学生网络中",{"2":{"141":1}}],["将图像短边调整为三种分辨率",{"2":{"109":1}}],["将增强后的特征图再次通过解码器生成增强预测图",{"2":{"130":1}}],["将特征图输入解码器生成语义分割预测图",{"2":{"130":1}}],["将特定领域特征自适应转换为通用特征",{"2":{"47":1}}],["将模型性能从67",{"2":{"122":1}}],["将已标注像素的特征传播至未标注区域来生成伪标签",{"2":{"114":1}}],["将其转换为像素级语言嵌入并输入到深度预测网络中",{"2":{"110":1}}],["将其他数据源的关键模式整合到rgb流中",{"2":{"88":1}}],["将测试图像调整为多个尺度",{"2":{"109":1}}],["将成本图与不同层级骨干网络的特征图进行融合",{"2":{"101":1}}],["将生成的全局和局部原型扩展到特征图的大小",{"2":{"96":1}}],["将语义分割转化为像素级分类后",{"2":{"95":1}}],["将学习到的提示作为残差添加到原始rgb流中",{"2":{"88":1}}],["将rgb图像和辅助模态图像输入到补丁嵌入层",{"2":{"88":1}}],["将确定特征的注意力聚合",{"2":{"80":1}}],["将所有令牌转换到合适的特征空间进行特征学习",{"2":{"78":1}}],["将patch",{"2":{"78":1}}],["将自监督学习与知识蒸馏结合",{"2":{"77":1}}],["将额外的类别语义作为初始提示可以调整编码器",{"2":{"72":1}}],["将对象和属性先验知识嵌入到掩码嵌入中",{"2":{"57":1}}],["将crm添加到panopticfcn和entityseg后",{"2":{"51":1}}],["将crm作为全景分割和实体分割的扩展进行评估",{"2":{"51":1}}],["将msra",{"2":{"51":1}}],["将vfms用于dgss任务存在挑战",{"2":{"49":1}}],["将得到的密集相关图输入到4d卷积金字塔编码器和2d卷积金字塔解码器中",{"2":{"44":1}}],["将支持特征划分为多个局部特征",{"2":{"44":1}}],["将领域特定的查询特征转换为领域无关的特征",{"2":{"41":1}}],["将分割任务解耦为语义对齐和空间对齐两个子任务",{"2":{"12":1}}],["容易导致过拟合",{"2":{"41":1}}],["xetex",{"2":{"171":1}}],["xelatex",{"2":{"170":9,"171":14,"174":1,"182":9}}],["xb534",{"2":{"101":2}}],["x的resnet",{"2":{"51":1}}],["x",{"2":{"45":1,"194":3}}],["xidian",{"2":{"39":1,"73":1}}],["x3c",{"2":{"0":8}}],["kerner",{"2":{"193":1}}],["kernel",{"2":{"193":1,"194":18}}],["kernelsize−stride",{"2":{"193":1}}],["kernelsize=",{"2":{"193":2}}],["kernelsize",{"2":{"193":1}}],["keybinding",{"2":{"170":3,"171":1,"182":1}}],["key",{"2":{"39":1,"70":1,"72":1,"101":1,"137":1,"155":1,"194":2}}],["kaist",{"2":{"146":1,"147":1}}],["korea",{"2":{"145":1}}],["k百分比为8",{"2":{"132":1}}],["kitti",{"2":{"109":2}}],["knowledge",{"2":{"53":4,"74":1,"75":1}}],["kcl",{"2":{"26":1}}],["性能不佳或需大量图像",{"2":{"148":1}}],["性能比基线方法高约4",{"2":{"110":1}}],["性能仍然可以得到显著提升",{"2":{"110":1}}],["性能在预期范围内波动",{"2":{"80":1}}],["性能进一步提升",{"2":{"80":1}}],["性能验证",{"2":{"70":1}}],["性能表现优异",{"2":{"81":1}}],["性能表现",{"2":{"69":1,"111":1,"141":1}}],["性能仅有轻微下降",{"2":{"59":1}}],["性能优于或与当前最先进方法相当",{"2":{"111":1}}],["性能优势",{"2":{"51":1,"60":1}}],["性能优越性",{"2":{"162":1}}],["性能优越",{"2":{"38":1,"144":1}}],["性能随着采样的缩放比例数量增加而提升",{"2":{"51":1}}],["性能会下降",{"2":{"43":1}}],["性能下降2",{"2":{"15":1}}],["重叠区域的特征反复计算",{"2":{"190":1}}],["重启",{"2":{"166":1}}],["重庆大学等",{"0":{"101":1}}],["重建更多细节",{"2":{"51":1}}],["重建图像细节",{"2":{"51":1}}],["重新考虑了相关图的使用",{"2":{"38":1}}],["重要",{"2":{"34":1,"57":1}}],["能否编译参考文献",{"2":{"173":1}}],["能否编译目录",{"2":{"173":1}}],["能否进行引用",{"2":{"173":1}}],["能否插入图片",{"2":{"173":1}}],["能省很多麻烦",{"2":{"165":1}}],["能在无人工标注下以零样本方式有效对图像片段进行分类",{"2":{"162":1}}],["能在多个零样本跨域数据集上取得良好性能",{"2":{"111":1}}],["能有效解决无标签掩码带来的挑战",{"2":{"153":1}}],["能有效估计边界",{"2":{"81":1}}],["能将语义分割推广到无限类别的范围",{"2":{"148":1}}],["能帮助网络学习通用视觉表示",{"2":{"139":1}}],["能作为预训练目标",{"2":{"135":1}}],["能以正确分类像素的原型引导错误分类像素的分类",{"2":{"133":1}}],["能显著减少标注工作量",{"2":{"128":1}}],["能提前拒绝不存在的类别",{"2":{"101":1}}],["能进一步提高分割性能",{"2":{"59":1}}],["能连续对齐特征图与细化目标",{"2":{"51":1}}],["能生成更多细节",{"2":{"51":1}}],["能够在拥有这些优势的同时",{"2":{"175":1}}],["能够在",{"2":{"171":1}}],["能够将精确的片段分类到大量文本定义的类别中",{"2":{"156":1}}],["能够针对不同任务激活相应的类别对象",{"2":{"72":1}}],["能够调整编码器专注于当前任务中的目标类别",{"2":{"72":1}}],["能够直接用于现有的白天分割方法",{"2":{"70":1}}],["能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距",{"2":{"51":1}}],["能够有效扩展高置信区域并填充正确类别",{"2":{"37":1}}],["能够精确地细化并将特征图从每一层传递到骨干网络的下一层",{"2":{"49":1}}],["能利用相关图中的相似性和形状信息",{"2":{"38":1}}],["能应对类内差异的敏感性",{"2":{"15":1}}],["统计挖掘比例和有效伪标签比例",{"2":{"37":1}}],["统计分析",{"2":{"37":1}}],["软损失",{"2":{"36":1}}],["软损失和相关损失",{"2":{"34":1}}],["损失技术",{"2":{"35":1}}],["损失函数进行监督",{"2":{"44":1}}],["损失函数",{"2":{"34":1,"66":1,"96":1,"111":1}}],["损失函数和训练流程的完整解决方案",{"2":{"19":1}}],["4和stage",{"2":{"141":1}}],["4和5",{"2":{"141":1}}],["4和1",{"2":{"35":1}}],["480",{"2":{"109":1}}],["4xa6000",{"2":{"101":1}}],["4效果最佳",{"2":{"80":1}}],["44",{"2":{"72":1}}],["49",{"2":{"67":1}}],["459和pc",{"2":{"58":1}}],["459个类",{"2":{"54":1}}],["459",{"2":{"53":1,"101":1}}],["4×或",{"2":{"51":1}}],["4k和6k分辨率变得常见",{"2":{"51":1}}],["4k",{"2":{"51":2}}],["4",{"0":{"123":1,"167":1,"190":1,"200":1,"208":1,"209":1,"210":1,"211":1},"1":{"209":1,"210":1,"211":1},"2":{"35":1,"36":3,"49":2,"58":2,"72":1,"90":2,"97":1,"101":1,"132":1,"142":2,"168":1,"194":1,"220":1}}],["4分割比例上分别提高12",{"2":{"35":1}}],["比如区分一个像素是猫还是狗",{"2":{"187":1}}],["比如区分一张图片是猫还是狗",{"2":{"187":1}}],["比如检测一个区域是猫还是狗",{"2":{"187":1}}],["比如编译目录和编译参考文献时",{"2":{"171":1}}],["比仅依赖窗口注意力的swin",{"2":{"143":1}}],["比采用swin",{"2":{"142":1}}],["比agmm高1",{"2":{"119":1}}],["比tel高0",{"2":{"119":1}}],["比较便捷",{"2":{"168":1}}],["比较麻烦",{"2":{"168":1,"171":1}}],["比较了两种合并训练数据标签的方法",{"2":{"110":1}}],["比较了基于边缘和基于cam的两种严格选择不确定特征的方法",{"2":{"80":1}}],["比较了随机采样和均匀采样方法",{"2":{"36":1}}],["比较的方法",{"2":{"26":1}}],["比基线分别提高2",{"2":{"36":1}}],["比unimatch在1",{"2":{"35":1}}],["比unimatch在各分割比例上分别高出1",{"2":{"35":1}}],["比监督基线在1",{"2":{"35":1}}],["例如clip",{"2":{"158":1}}],["例如sam",{"2":{"157":1}}],["例如查询和排序",{"2":{"156":1}}],["例如两种vit变体在10个块时效果最佳",{"2":{"72":1}}],["例如流行的clip模型",{"2":{"54":1}}],["例如",{"2":{"35":1,"36":1,"80":1,"128":1,"141":2,"142":1}}],["72",{"2":{"122":1}}],["720",{"2":{"109":1}}],["75",{"2":{"109":1}}],["7倍的加速",{"2":{"101":1}}],["78",{"2":{"72":1}}],["7b可进一步降低计算开销",{"2":{"59":1}}],["746",{"2":{"103":1}}],["74",{"2":{"45":1}}],["7",{"0":{"172":1,"173":1,"174":1},"1":{"173":1,"174":1},"2":{"35":3,"53":1,"58":1,"59":1,"79":1,"90":2,"97":1,"101":1,"103":1,"104":1,"170":1,"194":1}}],["760",{"2":{"103":1}}],["768大小",{"2":{"101":1}}],["768",{"2":{"101":3}}],["76",{"2":{"30":1,"31":1}}],["08",{"2":{"67":1,"173":1}}],["0之间",{"2":{"51":1}}],["000步",{"2":{"51":1}}],["02",{"2":{"45":1,"173":1}}],["01",{"2":{"45":1,"101":1}}],["0",{"2":{"35":6,"36":2,"58":1,"67":3,"79":3,"80":1,"101":1,"109":1,"193":9,"194":2}}],["8提高到71",{"2":{"143":1}}],["82",{"2":{"101":1}}],["88",{"2":{"68":1}}],["80k",{"2":{"101":1}}],["80",{"2":{"67":1,"101":1}}],["847和a",{"2":{"58":1,"59":2}}],["847个类",{"2":{"54":1}}],["847",{"2":{"53":1,"101":1}}],["8×",{"2":{"51":1}}],["8和1",{"2":{"35":1}}],["8",{"0":{"175":1,"176":1,"177":1,"178":1,"179":1},"1":{"176":1,"177":1,"178":2,"179":2},"2":{"35":3,"36":4,"51":1,"58":2,"90":1,"97":1,"143":1,"171":1,"194":1}}],["监督损失是基本监督损失和监督相关损失的组合",{"2":{"34":1}}],["采用",{"2":{"138":1}}],["采用距离熵损失",{"2":{"124":1}}],["采用resnet50作为骨干网络",{"2":{"122":1}}],["采用不同的优化器和学习率衰减策略",{"2":{"109":1}}],["采用绝对相对误差",{"2":{"109":1}}],["采用蒸馏方法",{"2":{"107":1}}],["采用补丁令牌对比",{"2":{"78":1}}],["采用编码器",{"2":{"78":1}}],["采用无监督域适应技术将白天知识迁移到夜间",{"2":{"70":1}}],["采用层共享mlp权重和低秩token序列",{"2":{"49":1}}],["采用adamw优化器",{"2":{"49":1}}],["采用复杂数据增强和领域不变特征提取策略",{"2":{"49":1}}],["采用滑动窗口评估和在线难例挖掘",{"2":{"35":1}}],["采用随机采样的方法",{"2":{"34":1}}],["采用多尺度自适应局部注意力增强前景信息",{"2":{"12":1}}],["并不影响本文中所说的所有配置",{"2":{"163":1}}],["并进行后处理",{"2":{"159":1}}],["并用文本定义的任意类对其进行分类",{"2":{"157":1}}],["并对两个损失项进行等权重加权",{"2":{"141":1}}],["并对编码器和解码器进行联合预训练",{"2":{"141":1}}],["并对图像进行数据增强处理",{"2":{"109":1}}],["并学习细粒度的语义特征",{"2":{"139":1}}],["并根据涂鸦监督的具体情况采用不同原型策略",{"2":{"133":1}}],["并根据需要采用多尺度或单尺度测试",{"2":{"109":1}}],["并在广泛的数据集上进行预训练",{"2":{"135":1}}],["并在nyuv2和pascal",{"2":{"109":1}}],["并在11个基准测试中设立了新的技术标准",{"2":{"72":1}}],["并从openimagesv6和objects365中采样部分数据用于训练",{"2":{"109":1}}],["并通过1×1卷积和通道注意力机制进行细化",{"2":{"96":1}}],["并通过多层感知机",{"2":{"57":1}}],["并应用相同的分割损失",{"2":{"96":1}}],["并有效继承预训练基础模型的先验知识",{"2":{"88":1}}],["并有效泛化到任意未知目标域",{"2":{"63":1}}],["并引入自蒸馏方法增强语义一致性",{"2":{"81":1}}],["并激发未来相关研究聚焦于此",{"2":{"72":1}}],["并且此文主要将",{"2":{"164":1}}],["并且页面不是很美观",{"2":{"163":1}}],["并且其标注过程也非常繁琐",{"2":{"128":1}}],["并且在使用完整的coco数据进行微调时",{"2":{"110":1}}],["并且在混合数据集上训练的模型比在单个数据集上训练的hrnet更具鲁棒性",{"2":{"110":1}}],["并且使用句子编码可以获得更好的性能",{"2":{"110":1}}],["并且计算复杂度与输入的大小成线性关系",{"2":{"101":1}}],["并且具有类别早期拒绝的功能",{"2":{"101":1}}],["并且与基于解码器的方法具有良好的兼容性",{"2":{"72":1}}],["并且能更好地重建粗掩码中缺失的部分",{"2":{"51":1}}],["并结合spt",{"2":{"72":1}}],["并仅微调解码器",{"2":{"72":1}}],["并聚合光照特征",{"2":{"70":1}}],["并将其映射到视觉",{"2":{"159":1}}],["并将其映射到掩码级别",{"2":{"57":1}}],["并将其集成到单阶段框架中",{"2":{"81":1}}],["并将该类别传播到增强的伪标签和扩展的高置信区域中",{"2":{"34":1}}],["并利用语言解析工具从中提取对象",{"2":{"57":1}}],["并提出了语义关注",{"2":{"54":1}}],["并提出",{"2":{"49":1}}],["并得出以下结论",{"2":{"47":1,"51":1,"60":1,"70":1,"124":1}}],["并使用灵活的文本定义类别对其进行分类",{"2":{"156":1}}],["并使用二元掩码损失对模型进行监督",{"2":{"150":1}}],["并使用二元交叉熵",{"2":{"44":1}}],["并使用bce损失函数进行训练监督",{"2":{"44":1}}],["并忽视类内外观的差异",{"2":{"41":1}}],["并设计了两种标签传播策略来丰富伪标签",{"2":{"38":1}}],["并挖掘更多潜在的高置信度像素",{"2":{"31":1}}],["当同步到外部查看器时",{"2":{"182":1}}],["当编译成功后",{"2":{"174":1}}],["当编译失败时",{"2":{"171":1}}],["当发现页面下方出现",{"2":{"174":1}}],["当其余编译器引用时该",{"2":{"171":1}}],["当代码被保存时自动编译文件",{"2":{"171":1}}],["当出现下图所示弹窗时",{"2":{"164":1}}],["当上面标示的时间安装完之后",{"2":{"164":1}}],["当公式比较长的时候",{"2":{"163":1}}],["当每个类别的全局原型数量增加到约5时",{"2":{"132":1}}],["当涂鸦收缩到点",{"2":{"120":1}}],["当前语义分割方法虽在预定义封闭集数据集上表现出色",{"2":{"105":1}}],["当前最先进的语义分割方法在预设的封闭数据集上表现出色",{"2":{"104":1}}],["当前主流方法主要通过对基于可见光",{"2":{"85":1}}],["当前工作虽提升了ov识别能力",{"2":{"60":1}}],["当将其扩展到跨领域",{"2":{"72":1}}],["当部分掩码数量",{"2":{"72":1}}],["当二值图与高置信区域有较大重叠时",{"2":{"34":1}}],["当使用convnext",{"2":{"101":1}}],["当使用",{"2":{"31":1}}],["利用预训练的视觉变换器",{"2":{"159":1}}],["利用多模态大语言模型",{"2":{"159":1}}],["利用图像级语义标签",{"2":{"148":1,"149":1}}],["利用大规模无标签数据进行预训练以减少对大量标注样本的依赖成为潜在解决方案",{"2":{"139":1}}],["利用这些特征计算图像",{"2":{"150":1}}],["利用这些原型增强初始特征",{"2":{"133":1}}],["利用这些策略",{"2":{"38":1}}],["利用涂鸦标签进行监督",{"2":{"130":1}}],["利用涂鸦标签的方法也存在不足",{"2":{"115":1}}],["利用从稀疏涂鸦中提取的图像级类别标签为图像监督分割提供全局线索",{"2":{"117":1}}],["利用噪声和弱标注数据集",{"2":{"111":1}}],["利用clip分类模型的知识",{"2":{"107":1}}],["利用空间金字塔网络或注意力模块等提取上下文信息",{"2":{"101":1}}],["利用自监督学习进行密集预测预训练",{"2":{"140":1}}],["利用自蒸馏知识恢复局部信息",{"2":{"81":1}}],["利用自注意力机制中固有的长距离依赖关系",{"2":{"66":1}}],["利用光照中的语义线索实现更精确预测",{"2":{"70":1}}],["利用光照组件作为线索",{"2":{"70":1}}],["利用llm的对象",{"2":{"60":1}}],["利用llm的关系先验知识",{"2":{"57":1}}],["利用连续位置信息和特征对齐",{"2":{"51":1}}],["利用相关图中隐含的形状信息来增强伪标签",{"2":{"34":1}}],["利用查询特征的内在引导",{"2":{"12":1}}],["利用查询特征的语义和空间感知",{"2":{"10":1}}],["然后按ctrl+alt+v",{"2":{"174":1}}],["然后按下该快捷键",{"2":{"174":1}}],["然后进行图示操作",{"2":{"174":1}}],["然后输入命令xelatex",{"2":{"164":1}}],["然后手动选择某一镜像网站进行下载",{"2":{"164":1}}],["然后平均得分作为最终预测结果",{"2":{"109":1}}],["然后与查询特征和先验掩码连接",{"2":{"96":1}}],["然后通过自适应融合这两部分信息来进行语义识别",{"2":{"70":1}}],["然后通过多头自注意力机制将关系先验知识编码到掩码嵌入中",{"2":{"57":1}}],["然后将结果输入到语义分割头进行最终预测",{"2":{"66":1}}],["然后将相关图传播到模型的对数似然输出中",{"2":{"34":1}}],["然而现实场景并非如此",{"2":{"105":1}}],["然而在实际应用中",{"2":{"70":1}}],["然而",{"2":{"19":1,"41":1,"49":1,"51":1,"63":1,"72":2,"76":1,"94":1,"104":1,"114":1,"127":1,"138":1,"139":1,"147":1,"148":1,"157":1,"158":1}}],["计算与目标类文本嵌入的相似度",{"2":{"160":1}}],["计算效率低",{"2":{"101":1}}],["计算每个注意力头在transformer层上的平均注意力熵",{"2":{"80":1}}],["计算每两个神经元之间的转移概率",{"2":{"22":1}}],["计算机视觉领域也在探索基础模型",{"2":{"135":1}}],["计算机视觉领域虽也对基础模型有所探索",{"2":{"135":1}}],["计算机视觉领域尝试引入可学习参数激活语义知识",{"2":{"72":1}}],["计算机视觉中领域泛化分割多聚焦驾驶场景",{"2":{"65":1}}],["计算机视觉和机器学习领域对领域泛化广泛研究",{"2":{"65":1}}],["计算成本高",{"2":{"51":1,"56":1}}],["计算查询特定的转换矩阵有助于防止过拟合",{"2":{"41":1}}],["计算高置信形状内每个唯一类别的数量",{"2":{"34":1}}],["计算特征向量对之间的相关性得到相关图",{"2":{"34":1}}],["标准字体进行替换",{"2":{"171":1}}],["标记",{"2":{"174":1}}],["标记能够编译文档",{"2":{"171":1}}],["标记使用",{"2":{"171":1}}],["标题对来弥补弱监督",{"2":{"148":1}}],["标签扩散主要依赖局部信息",{"2":{"128":1}}],["标签",{"2":{"76":1}}],["标签传播等",{"2":{"36":1}}],["标签传播策略的影响",{"2":{"36":1}}],["标签传播策略",{"2":{"34":1}}],["标注方式",{"2":{"129":1}}],["标注成本问题",{"2":{"128":1}}],["标注成本高",{"2":{"32":1}}],["标注稀疏会导致预测结果不确定",{"2":{"20":1}}],["对这些小区域做分类",{"2":{"190":1}}],["对字体的支持更好",{"2":{"171":1}}],["对应的",{"2":{"171":1}}],["对框架性能提升至关重要",{"2":{"153":1}}],["对全局平均池化",{"2":{"141":1}}],["对局部原型增强和全局原型增强方法进行消融实验",{"2":{"132":1}}],["对初始预测图和增强预测图进行约束",{"2":{"130":1}}],["对不同伪标签进行实验",{"2":{"123":1}}],["对训练语言空间外的类别泛化能力不足",{"2":{"111":1}}],["对训练集和验证集中不合理的标注进行了仔细修改",{"2":{"70":1}}],["对分割模型进行监督",{"2":{"107":1}}],["对高置信度样本施加损失",{"2":{"107":1}}],["对所有样本施加像素级损失",{"2":{"107":1}}],["对齐诱导跨模态提示器",{"2":{"88":1}}],["对辅助模态的视觉概念进行分层渐进分组",{"2":{"88":1}}],["对最大的夜间分割数据集nightcity进行细化",{"2":{"70":1}}],["对dfq的理解",{"2":{"67":1}}],["对dcam中的关键组件进行了全面分析",{"2":{"15":1}}],["对比从真实标注和mllm增强标注中提取的对象",{"2":{"161":1}}],["对比结果",{"2":{"160":1}}],["对比学习在下游视觉任务表现良好",{"2":{"140":1}}],["对比实验+消融实验",{"2":{"101":1}}],["对比实验",{"2":{"51":1}}],["对比方法",{"2":{"51":1}}],["对比rein与现有dgss和参数高效微调",{"2":{"49":1}}],["对4k或6k超高分辨率图像进行分割时",{"2":{"51":1}}],["对vfms大量可训练参数进行微调会导致泛化能力受限",{"2":{"49":1}}],["对更强的vfms在dgss中的效能探索不足",{"2":{"49":1}}],["对于反卷积而言",{"2":{"193":1}}],["对于普通的",{"2":{"193":1}}],["对于理工科",{"2":{"163":1}}],["对于文本数据的多模态表示学习",{"2":{"158":1}}],["对于标注像素级别的样本需要大量的人力和财力",{"2":{"128":1}}],["对于近期工作tel和agmm",{"2":{"119":1}}],["对于先前工作rawks和ncl采用的crf后处理",{"2":{"119":1}}],["对于低分辨率的输入",{"2":{"101":1}}],["对于前景对象",{"2":{"94":1}}],["对于少样本语义分割任务",{"2":{"93":1}}],["对于",{"2":{"51":1,"188":1}}],["对于无标签图像",{"2":{"34":1}}],["对于有标签图像",{"2":{"34":1}}],["相反",{"2":{"174":1}}],["相比当前最优方法tel",{"2":{"133":1}}],["相比像素级标注",{"2":{"128":1}}],["相比单字标签嵌入",{"2":{"107":1}}],["相较于需要完整标注的替代方案",{"2":{"114":1}}],["相较于需要像素级标注的传统语义分割方法",{"2":{"75":1}}],["相似性自匹配",{"2":{"44":1}}],["相似度测量模块",{"2":{"22":1}}],["相关方法在语义分割任务中表现良好",{"2":{"95":1}}],["相关代码已开源",{"2":{"114":1,"127":1}}],["相关代码和数据集可以在https",{"2":{"70":1}}],["相关代码可以在https",{"2":{"51":1}}],["相关代码可在",{"2":{"41":1}}],["相关性地图不仅能轻松聚类同一类别的像素",{"2":{"32":1}}],["增加到三个块时性能下降",{"2":{"143":1}}],["增加基线的训练迭代次数对性能影响不大",{"2":{"98":1}}],["增加后续解码器分割新类别的负担",{"2":{"72":1}}],["增加了成本",{"2":{"51":1}}],["增加了训练复杂度",{"2":{"32":1}}],["增强预训练模型",{"2":{"149":1}}],["增强预训练视觉",{"2":{"148":1}}],["增强网络鲁棒性",{"2":{"124":1}}],["增强特征挖掘能力",{"2":{"96":1}}],["增强前景语义一致性",{"2":{"94":1}}],["增强前景特征",{"2":{"9":1,"12":1}}],["增强不确定特征和确定特征对语义分割都很重要",{"2":{"81":1}}],["增强了全局理解",{"2":{"80":1}}],["增强了通道表示能力并减少通道冗余",{"2":{"69":1}}],["增强模型对像素对之间相似性的整体感知",{"2":{"34":1}}],["增强模型的鲁棒性",{"2":{"10":1}}],["半监督学习也难以很好地泛化到未见类别",{"2":{"72":1}}],["半监督学习受关注",{"2":{"32":1}}],["半监督和多标注场景下的分割研究也有开展",{"2":{"65":1}}],["半监督和无监督分割方法",{"2":{"32":1}}],["半监督语义分割仅需少量标注数据和大量未标注数据进行训练",{"2":{"32":1}}],["耗时长",{"2":{"32":1}}],["的测试函数",{"2":{"194":1}}],["的数量为stride−1",{"2":{"193":1}}],["的层数为",{"2":{"193":1}}],["的卷积",{"2":{"193":1}}],["的概念与目标检测中的",{"2":{"191":1}}],["的使用",{"0":{"180":1}}],["的窗口位置",{"2":{"175":1}}],["的内部查看器的快捷键绑定",{"2":{"171":1}}],["的标准字体",{"2":{"171":1}}],["的引用",{"2":{"171":1}}],["的快捷方式",{"2":{"165":1}}],["的编辑器",{"2":{"164":1}}],["的下载与安装说明",{"2":{"164":1}}],["的区别",{"2":{"164":1}}],["的性能提升",{"2":{"138":1}}],["的线性探测任务中实现了",{"2":{"138":1}}],["的统计特性来强化涂鸦监督效果",{"2":{"127":1}}],["的特征增强方法",{"2":{"127":1}}],["的coco数据",{"2":{"110":1}}],["的训练集",{"2":{"101":1}}],["的参数",{"2":{"85":1}}],["的共性特征",{"2":{"85":1}}],["的熵更高且更集中",{"2":{"80":1}}],["的消融结果",{"2":{"80":1}}],["的整体准确性",{"2":{"75":1}}],["的有效性",{"2":{"72":1}}],["的平均交并比",{"2":{"72":1,"104":1}}],["的平均iou",{"2":{"45":2}}],["的夜间语义分割范式",{"2":{"70":1}}],["的优势显著超越次优方法",{"2":{"63":1}}],["的新方法",{"2":{"60":1}}],["的配置",{"2":{"51":2,"170":1}}],["的一半",{"2":{"51":1}}],["的miou提升",{"2":{"72":3}}],["的miou",{"2":{"49":1}}],["的可训练参数",{"2":{"49":1}}],["的",{"2":{"31":1,"170":1,"171":1,"179":1,"194":1}}],["的模型参数",{"2":{"88":1,"90":1}}],["的模型",{"2":{"22":1}}],["张标注图像的",{"2":{"31":1}}],["1cityscape",{"0":{"203":1}}],["17",{"2":{"194":2}}],["171",{"2":{"101":1}}],["1代码解读",{"2":{"179":1}}],["1使用外部查看器时要执行的命令",{"2":{"179":1}}],["1用于反向同步",{"2":{"171":1}}],["1该命令的作用为设置",{"2":{"171":1}}],["1这条命令是设置什么时候对上文设置的辅助文件进行清除",{"2":{"171":1}}],["1设置pdf查看器用于在",{"2":{"179":1}}],["1设置默认的pdf查看器",{"2":{"179":1}}],["1设置为true",{"2":{"171":1}}],["1设置何时使用默认的",{"2":{"171":1}}],["1启用上下文latex菜单",{"2":{"171":1}}],["1后添加上",{"2":{"170":1}}],["1版本",{"2":{"167":1}}],["12此命令是将生成的辅助文件",{"2":{"179":1}}],["1234567",{"2":{"220":1}}],["123456789output",{"2":{"220":1}}],["12345678910",{"2":{"194":1}}],["12345678910设置当",{"2":{"179":1}}],["12345678910111213141516171819output",{"2":{"221":1}}],["123456789101112131415161718192021这串命令则是设置编译完成后要清除掉的辅助文件类型",{"2":{"171":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657pythondef",{"2":{"194":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153写在最后",{"2":{"182":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116注",{"2":{"170":1}}],["1234567891011121314151617181920212223242526272829303132333435363738394041424344此串代码是对编译链进行定义",{"2":{"171":1}}],["1234567891011121314151617181920212223242526272829303132333435363738394041这些代码是定义在下文",{"2":{"171":1}}],["123456789101112131415161718此代码仅为展示所用",{"2":{"178":1}}],["12345678910111213141516",{"2":{"0":1,"173":1}}],["123456789代码解读",{"2":{"179":1}}],["1234pythonclass",{"2":{"194":1}}],["1234此代码是设置使用外部查看器时",{"2":{"179":1}}],["123代码解读",{"2":{"179":1}}],["12",{"2":{"179":1}}],["12这两个命令是设置当文档编译错误时是否弹出显示出错和警告的弹窗",{"2":{"171":1}}],["12其中的",{"2":{"170":1}}],["12656706",{"2":{"164":1}}],["1b数据集",{"2":{"135":1}}],["1b",{"2":{"135":4}}],["13",{"2":{"122":1}}],["136",{"2":{"103":1}}],["150",{"2":{"101":2}}],["150上分别比仅使用原始注意力图的模型提高了1",{"2":{"59":1}}],["150上取得最佳性能",{"2":{"59":1}}],["150上均达到了最先进的性能",{"2":{"58":1}}],["11m",{"2":{"135":2}}],["111",{"2":{"103":1}}],["118k",{"2":{"101":1}}],["11层",{"2":{"80":1}}],["11",{"0":{"182":1},"2":{"72":1,"167":1,"174":1}}],["18",{"2":{"67":1}}],["14",{"2":{"67":1}}],["1k",{"2":{"51":1}}],["10mb",{"2":{"175":1}}],["10",{"0":{"181":1},"2":{"173":1,"194":2}}],["1080",{"2":{"109":1}}],["10k",{"2":{"51":1}}],["1000合并为训练数据集",{"2":{"51":1}}],["1000",{"2":{"45":1}}],["101",{"2":{"30":1,"31":1}}],["166523064",{"2":{"182":1}}],["16虽然分割性能较差",{"2":{"72":1}}],["16或deit",{"2":{"72":1}}],["16次之",{"2":{"72":1}}],["16骨干网络在所有设置下具有最佳的分割精度",{"2":{"72":1}}],["16",{"2":{"35":2,"68":1}}],["1",{"0":{"109":1,"119":1,"164":1,"170":1,"173":1,"176":1,"178":1,"187":1,"196":1,"197":2,"198":1,"199":1,"200":1,"202":1,"206":1,"209":1},"1":{"197":1,"198":1,"199":1,"200":1},"2":{"35":6,"49":1,"51":1,"58":1,"62":2,"70":2,"72":2,"81":1,"84":1,"85":1,"98":1,"101":2,"103":1,"109":1,"120":1,"122":1,"135":1,"138":1,"141":1,"155":1,"156":1,"163":1,"168":1,"193":6,"194":57}}],["具备综合复杂推理能力",{"2":{"56":1}}],["具有19个语义类别",{"2":{"70":1}}],["具有显著的泛化潜力",{"2":{"51":1}}],["具有显著的跨场景泛化能力",{"2":{"49":1}}],["具有较强的鲁棒性",{"2":{"28":1}}],["具体的说",{"2":{"193":1}}],["具体的安装指标已在下图标明",{"2":{"164":1}}],["具体见下图",{"2":{"181":1}}],["具体看下图",{"2":{"171":1}}],["具体安装过程与常见的软件安装过程一致",{"2":{"165":1}}],["具体通过定义可提示的分割任务",{"2":{"135":1}}],["具体结论如下",{"2":{"81":1,"111":1}}],["具体来说",{"2":{"57":1,"70":1}}],["具体如下",{"2":{"49":1}}],["具体做法是将相关图的每一行进行归一化并转换为二值图",{"2":{"34":1}}],["具体步骤为",{"2":{"34":1}}],["具体研究背景如下",{"2":{"32":1}}],["具体而言",{"2":{"19":1,"90":1}}],["具体方法",{"2":{"12":3}}],["🥇",{"0":{"27":1,"36":1,"39":1,"59":1,"68":1,"121":1,"142":1,"143":1,"152":1,"161":1},"1":{"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"122":1,"123":1}}],["带有颜色约束的伪标签再训练",{"0":{"25":1}}],["神经特征空间的自监督学习",{"0":{"24":1}}],["形成转移矩阵",{"2":{"22":1}}],["图片级",{"2":{"187":1}}],["图像编码器架构设计",{"2":{"161":1}}],["图像编码器",{"2":{"159":1}}],["图像编码器学习率多乘以一个",{"2":{"101":1}}],["图像修复等",{"2":{"140":1}}],["图像分割网络的两个模块",{"0":{"209":1}}],["图像分割的三个层次",{"0":{"200":1}}],["图像分割的前景和背景",{"0":{"199":1}}],["图像分割的应用场景",{"0":{"198":1}}],["图像分割学习笔记",{"0":{"195":1}}],["图像分割领域存在多种任务",{"2":{"135":1}}],["图像分割任务",{"2":{"135":1}}],["图像分辨率为2048x1024",{"2":{"70":1}}],["图像分辨率范围2k",{"2":{"51":1}}],["图像分辨率越来越高",{"2":{"51":1}}],["图像语义分割任务训练通常需大量高质量标注样本",{"2":{"129":1}}],["图像级弱监督语义分割",{"2":{"116":1}}],["图像级标签最为经济",{"2":{"76":1}}],["图像级监督仅为整个图像提供类别标签",{"2":{"21":1}}],["图像级监督",{"2":{"21":1}}],["图像特征提取",{"2":{"57":1}}],["图像裁剪和级联模型",{"2":{"51":1}}],["图割算法传播标注",{"2":{"21":1}}],["缺乏定位信息",{"2":{"21":1}}],["而对于输入的内部插",{"2":{"193":1}}],["而对于我们大部分人来说",{"2":{"164":1}}],["而实例分割不仅要对像素进行分类",{"2":{"187":1}}],["而never命令做不到这一点",{"2":{"171":1}}],["而有时候将",{"2":{"171":1}}],["而编译链就解决了这个问题",{"2":{"171":1}}],["而使用",{"2":{"171":1}}],["而",{"2":{"171":1}}],["而command为在该拓展中的编译方式",{"2":{"171":1}}],["而第二个选项为进行正向同步",{"2":{"171":1}}],["而每个代码块儿的最后一句是不需要加上",{"2":{"170":1}}],["而言不那么直观",{"2":{"168":1}}],["而代码设置页面虽然相对",{"2":{"168":1}}],["而visual",{"2":{"163":1}}],["而选择一个比较好的编译器是很重要的",{"2":{"163":1}}],["而数据改进则侧重于改善图像和文本数据的对齐",{"2":{"158":1}}],["而非高iou的交互式分割",{"2":{"135":1}}],["而非数据的多次采样",{"2":{"98":1}}],["而lorm可以解决这个问题",{"2":{"122":1}}],["而同时使用两者产生了更好的结果",{"2":{"122":1}}],["而该方法的性能仅下降不到1",{"2":{"120":1}}],["而这些要素对实现高质量的语义分割至关重要",{"2":{"114":1}}],["而不是",{"2":{"179":1}}],["而不是传统的transformer",{"2":{"101":1}}],["而不包含物体位置信息",{"2":{"148":1}}],["而不同跨域模式又可能共存于同一通道",{"2":{"63":1}}],["而是利用视觉基础模型",{"2":{"148":1}}],["而是学习内容和光照的纠缠表示",{"2":{"70":1}}],["而是提出了自匹配转换",{"2":{"41":1}}],["而且对非拉丁字体的支持更好",{"2":{"171":1}}],["而且",{"2":{"70":1}}],["而且无需使用任何真实的城市场景数据集",{"2":{"49":1}}],["而rdwt通过在计算协方差矩阵之前对特征进行归一化",{"2":{"66":1}}],["而处理超高分辨率细化的级联解码器方法",{"2":{"51":1}}],["而大规模vfms虽在计算机视觉挑战中表现出色",{"2":{"49":1}}],["而重复利用少量支持图像来处理每个类别",{"2":{"41":1}}],["而标注的多样性会使网络难以学习到稳定一致的分割模式",{"2":{"20":1}}],["而双语义感知注意力机制通过可学习的方式减轻背景干扰",{"2":{"15":1}}],["涂鸦标签属于弱监督学习",{"2":{"128":1}}],["涂鸦标注的稀疏性和多样性使得直接训练网络生成确定且一致的预测具有挑战性",{"2":{"19":1}}],["涂鸦收缩和丢弃实验",{"0":{"120":1}}],["涂鸦级弱监督语义分割",{"2":{"116":1}}],["涂鸦",{"2":{"76":1}}],["涂鸦监督能提供更多关键语义信息",{"2":{"129":1}}],["涂鸦监督语义分割",{"2":{"21":1,"127":1}}],["涂鸦监督是一种用户友好的弱监督形式",{"2":{"21":1}}],["涂鸦监督现存问题",{"2":{"20":1}}],["涂鸦监督因标注方式友好且能提供有效监督信息",{"2":{"20":1}}],["多粒度图像字幕生成",{"2":{"159":1}}],["多样化的分割数据集",{"2":{"135":1}}],["多级别原型设计选择",{"2":{"98":1}}],["多级原型交互",{"2":{"96":1}}],["多级原型交互模块",{"2":{"96":1}}],["多级原型处理",{"2":{"96":1}}],["多级原型生成模块",{"2":{"96":1}}],["多模态表示学习",{"2":{"158":1}}],["多模态图像分割主流方法",{"2":{"87":1}}],["多模态图像分割技术是计算机视觉领域的关键挑战",{"2":{"85":1}}],["多模态方法常采用基于rgb的预训练分割器",{"2":{"86":1}}],["多模态融合用于分割成为图像解释核心问题",{"2":{"86":1}}],["多模态融合的重要性",{"2":{"86":1}}],["多阶段方法先通过分类模型生成类激活图",{"2":{"77":1}}],["多数自监督学习方法在语义分割任务中表现欠佳",{"2":{"139":1}}],["多数语义分割数据集规模远小于分类数据集",{"2":{"139":1}}],["多数采用预训练编码器并微调解码器",{"2":{"72":1}}],["多数方法遵循元学习范式",{"2":{"72":1}}],["多数现有医学图像分割方法假定训练和测试样本遵循相同统计分布",{"2":{"64":1}}],["多数现有cd",{"2":{"42":1}}],["多尺度融合",{"2":{"143":1}}],["多尺度解码器",{"2":{"141":1}}],["多尺度评估",{"2":{"109":1}}],["多尺度方法优于单尺度方法",{"2":{"59":1}}],["多尺度自适应局部注意力",{"2":{"15":1}}],["多使用vggnet",{"2":{"49":1}}],["多种弱监督标注策略应运而生",{"2":{"20":1}}],["其插入的",{"2":{"193":1}}],["其过程如下图所示",{"2":{"193":1}}],["其操作步骤与内嵌输出",{"2":{"180":1}}],["其安装很简单",{"2":{"176":1}}],["其变量有",{"2":{"171":1}}],["其生成的",{"2":{"171":1}}],["其内部编译命令来自上文latex",{"2":{"171":1}}],["其功能是一样的",{"2":{"168":1}}],["其余如图所示",{"2":{"165":1}}],["其余均为原创内容",{"2":{"163":1}}],["其重要性自不必多说",{"2":{"163":1}}],["其核心原理是通过将已标注像素的信息传递至相邻未标注区域",{"2":{"127":1}}],["其核心难点在于域偏移问题",{"2":{"63":1}}],["其前向传播过程包含三个主要模块",{"2":{"96":1}}],["其性能表现甚至优于那些通过增加模型复杂度来提升准确率的多阶段方法",{"2":{"75":1}}],["其研究背景主要源于自然语言处理",{"2":{"135":1}}],["其研究背景主要源于当前fss方法存在的局限性以及人类视觉感知模式带来的启示",{"2":{"72":1}}],["其研究背景主要如下",{"2":{"20":1}}],["其互补数据集为bdd100k",{"2":{"70":1}}],["其dice相似系数",{"2":{"63":1}}],["其目的是为了在现实应用中识别开放类集中的对象",{"2":{"54":1}}],["其多视图一致性和光滑性有利于分割",{"2":{"51":1}}],["其在不同未知场景下展现出强大泛化能力",{"2":{"49":1}}],["其他弱监督语义分割",{"2":{"116":1}}],["其他fss方法的性能有显著提升",{"2":{"72":1}}],["其他的细粒度分类和边界框分割问题没有涉及",{"2":{"60":1}}],["其他实验",{"0":{"37":1},"1":{"38":1}}],["其他策略",{"2":{"34":1}}],["其次是对齐引导的跨模态提示器",{"2":{"85":1}}],["其次",{"2":{"31":1}}],["其中侧边栏所展现的就是上文提及的新的",{"2":{"174":1}}],["其中name是标签",{"2":{"171":1}}],["其中的name为这些命令的标签",{"2":{"171":1}}],["其中多出来的第一个选项为进行tex文件的编译",{"2":{"171":1}}],["其中风格解耦查询的贡献更大",{"2":{"68":1}}],["其中λ设置为1×10−4",{"2":{"66":1}}],["其中随机采样128个样本时性能最佳",{"2":{"36":1}}],["其中涂鸦监督因其用户友好的标注方式而逐渐流行",{"2":{"19":1}}],["其中",{"2":{"9":1,"20":1,"32":1,"76":1,"129":1,"191":1}}],["旨在有效地管理各种粒度的大量片段",{"2":{"156":1}}],["旨在为图像中每个像素识别类别标签",{"2":{"148":1}}],["旨在生成具有全局上下文信息传播的高分辨率特征",{"2":{"139":1}}],["旨在推动计算机视觉领域的基础模型研究",{"2":{"135":1}}],["旨在提高少样本任务中前景分割性能",{"2":{"94":1}}],["旨在利用多数据源增强细粒度细节和像素级语义",{"2":{"87":1}}],["旨在利用少量标注样本快速泛化到未见领域",{"2":{"72":1}}],["旨在用少量标注支持图像实现查询图像的准确分割",{"2":{"42":1}}],["旨在训练能够以少量标注图像对不同领域进行分割的通用模型",{"2":{"41":1}}],["旨在解决现有方法存在的问题",{"2":{"76":1}}],["旨在解决少样本和域差距问题",{"2":{"43":1}}],["旨在解决深度学习方法对大规模像素级标注数据集的依赖问题",{"2":{"32":1}}],["旨在解决涂鸦监督语义分割任务中预测结果的不确定性和不一致性问题",{"2":{"22":1}}],["旨在解决该领域存在的问题",{"2":{"20":1}}],["旨在通过少量的标注样本为新的类别生成一个分割模型",{"2":{"9":1}}],["迫使网络生成确定性预测结果",{"2":{"19":1}}],["使得字体变大",{"2":{"175":1}}],["使冻结的预训练基础模型适应各种下游多模态分割任务",{"2":{"90":1}}],["使冻结的预训练模型能灵活适配多种多模态分割任务",{"2":{"85":1}}],["使模型学习到通用语义概念",{"2":{"153":1}}],["使模型学习目标域风格信息",{"2":{"47":1}}],["使模型学习目标域的风格信息",{"2":{"44":1}}],["使模型适应各种下游多模态分割任务",{"2":{"88":1}}],["使特定区域的全局语义更好地聚合到提示中",{"2":{"72":1}}],["使其初步定位目标",{"2":{"72":1}}],["使编码器更精准地聚焦于目标对象",{"2":{"72":1}}],["使分割不受复杂光照干扰",{"2":{"70":1}}],["使网络在不同光照下提取一致特征",{"2":{"70":1}}],["使光不变反射率和光特定光照之间的纠缠加剧",{"2":{"70":1}}],["使神经表征在相似语义区域内均匀分布",{"2":{"19":1}}],["使用上次的recipe编译组合",{"2":{"182":1}}],["使用vscode内置pdf查看器或使用电脑默认浏览器进行pdf查看",{"2":{"179":1}}],["使用外部pdf查看器查看",{"2":{"179":1}}],["使用外部查看器时",{"2":{"182":1}}],["使用外部查看器时要执行的命令",{"2":{"182":1}}],["使用外部查看器",{"2":{"179":1}}],["使用外部",{"2":{"179":1}}],["使用电脑默认浏览器进行",{"2":{"179":1}}],["使用内置查看器已无法满足需求",{"2":{"175":1}}],["使用快捷键",{"2":{"174":1}}],["使用右键菜单",{"2":{"174":1}}],["使用侧边工具栏",{"2":{"174":1}}],["使用鼠标左键双击",{"2":{"171":1}}],["使用最近一次编译所用的编译链",{"2":{"171":1}}],["使用latex",{"2":{"171":1}}],["使用llm带来了巨大的参数量",{"2":{"60":1}}],["使用llava",{"2":{"59":1}}],["使用",{"2":{"171":1,"179":1,"182":1}}],["使用的是tex的标准字体",{"2":{"171":1}}],["使用的括号就比较多",{"2":{"163":1}}],["使用的数据集",{"2":{"70":1}}],["使用类无关掩码",{"2":{"160":1}}],["使用段",{"2":{"159":1}}],["使用图像分割模型sam生成掩码",{"2":{"159":1}}],["使用框提示生成掩码",{"2":{"159":1}}],["使用开放词汇定位模型",{"2":{"159":1}}],["使用动量编码器来提取掩码池化特征",{"2":{"150":1}}],["使用轻量级解码器进行上采样",{"2":{"150":1}}],["使用线性头时",{"2":{"142":1}}],["使用全局自注意力",{"2":{"141":1}}],["使用sumatrapdf查看的代码配置",{"0":{"177":1},"1":{"178":1,"179":1}}],["使用swin",{"2":{"141":1}}],["使用scribblesup数据集进行训练和验证",{"2":{"122":1}}],["使用稍弱的骨干网络mit",{"2":{"133":1}}],["使用mit",{"2":{"132":1}}],["使用一致性损失",{"2":{"130":1}}],["使用一致编码器的比较",{"2":{"72":1}}],["使用局部和全局原型通过原型特征增强器对初始特征进行增强",{"2":{"130":1}}],["使用基于mix",{"2":{"130":1}}],["使用基于rgb的预训练基础模型的参数初始化多模态分割模型",{"2":{"88":1}}],["使用仅25",{"2":{"110":1}}],["使用句子嵌入来表示标签可以更好地解决标签冲突问题",{"2":{"110":1}}],["使用异质损失可以持续提高所有零样本数据集的性能",{"2":{"110":1}}],["使用hrnet",{"2":{"109":1}}],["使用平均精度",{"2":{"109":1}}],["使用平均交并比",{"2":{"109":1}}],["使用无标签分支或多级别原型交互均可使性能提升约2",{"2":{"98":1}}],["使用resnet101时",{"2":{"97":1}}],["使用resnet50骨干时",{"2":{"97":1}}],["使用transformer编码器对增强的查询特征进行自注意力和交叉注意力处理",{"2":{"96":1}}],["使用先验掩码和局部平均池化",{"2":{"96":1}}],["使用相互生成的伪标签进行训练",{"2":{"96":1}}],["使用n层transformer编码器进行特征激活",{"2":{"96":1}}],["使用强大的autoaugment时",{"2":{"80":1}}],["使用在",{"2":{"78":1}}],["使用更复杂的解码器不一定能优于简单的相似度计算",{"2":{"72":1}}],["使用deeplabv3+作为分割器",{"2":{"123":1}}],["使用deit",{"2":{"72":1}}],["使用dcm实现语义对齐和空间对齐可额外提升1",{"2":{"15":1}}],["使用ctrl",{"2":{"171":1}}],["使用clip对模型进行知识蒸馏",{"2":{"110":1}}],["使用clip",{"2":{"107":1}}],["使用clip提取的语言信息作为初始提示具有最优的分割结果",{"2":{"72":1}}],["使用cascadepsp提出的高分辨率图像分割数据集big",{"2":{"51":1}}],["使用随机初始化的提示来调整编码器不会带来性能提升",{"2":{"72":1}}],["使用其子集bdd100k",{"2":{"70":1}}],["使用风格解耦的键和值",{"2":{"68":1}}],["使用特征查询使dsc提高了0",{"2":{"68":1}}],["使用对象先验能持续提高开放词汇分割性能",{"2":{"59":1}}],["使用多模态大语言模型",{"2":{"57":1}}],["使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题",{"2":{"51":1}}],["使用超高清图像进行训练和测试仍耗资源",{"2":{"51":1}}],["使用adam优化器",{"2":{"51":1}}],["使用去掉conv5",{"2":{"51":1}}],["使用pytorch实现模型",{"2":{"51":1}}],["使用所有三个模块时模型性能最佳",{"2":{"46":1}}],["使用标准涂鸦重新实现它们",{"2":{"119":1}}],["使用标准的交叉熵损失",{"2":{"34":1}}],["使用标签传播策略后",{"2":{"37":1}}],["使用与训练过程相关的动态阈值策略",{"2":{"34":1}}],["使用掩码注意力减轻背景噪声干扰对性能提升影响不大",{"2":{"15":1}}],["提取图像块嵌入",{"2":{"159":1}}],["提取初始特征图",{"2":{"130":1}}],["提供最终的分割结果",{"2":{"96":1}}],["提示词分割",{"2":{"135":1}}],["提示编码器和掩码解码器组成",{"2":{"135":1}}],["提示生成",{"2":{"88":1}}],["提示调优作为新范式",{"2":{"87":1}}],["提示增强次数",{"2":{"72":1}}],["提示增强消融实验",{"2":{"72":1}}],["提示初始化消融实验",{"2":{"72":1}}],["提示学习",{"2":{"72":1}}],["提示与迁移",{"2":{"72":1}}],["提高涂鸦监督语义分割的性能",{"2":{"128":1}}],["提高效率",{"2":{"128":1}}],["提高到73",{"2":{"122":1}}],["提高了前景的类内泛化能力",{"2":{"99":1}}],["提高空间信息的适应性",{"2":{"88":1}}],["提高wsss性能",{"2":{"76":1}}],["提高分割精度",{"2":{"72":1}}],["提高训练效率",{"2":{"49":1}}],["提高未标注数据的使用效率",{"2":{"32":1}}],["提升预测性能",{"2":{"133":1}}],["提升分割性能",{"2":{"76":1}}],["提升分割效果",{"2":{"47":1}}],["提升",{"2":{"72":1}}],["提升模型的识别能力",{"2":{"72":1}}],["提升其在夜间的分割效果",{"2":{"70":1}}],["提升涂鸦监督语义分割确定性与一致性的方法",{"2":{"19":1}}],["提出多种预训练任务",{"2":{"140":1}}],["提出多级原型生成与交互模块",{"2":{"99":1}}],["提出可提示分割任务",{"2":{"135":1}}],["提出异质损失",{"2":{"111":1}}],["提出异质损失来监督合并的数据集",{"2":{"110":1}}],["提出的全局语义聚类和可学习类提示方法",{"2":{"153":1}}],["提出的特征自强化",{"2":{"79":1}}],["提出的模型",{"0":{"12":1,"22":1,"34":1,"44":1,"57":1,"66":1,"78":1,"88":1,"96":1,"107":1,"117":1,"130":1,"141":1,"150":1,"159":1},"1":{"23":1,"24":1,"25":1},"2":{"72":1}}],["提出nightcity",{"2":{"70":2}}],["提出了一个系统的自监督预训练框架",{"2":{"139":1}}],["提出了一种经济高效的训练方法",{"2":{"127":1}}],["提出了一种新颖的动态类别感知提示范式",{"2":{"72":1}}],["提出了一种新颖且高效的基于提示的方案",{"2":{"72":1}}],["提出了异构损失函数",{"2":{"107":1}}],["提出了放松的深度白化变换",{"2":{"69":1}}],["提出了两种策略",{"2":{"28":1}}],["提出松弛深度白化变换",{"2":{"66":1}}],["提出语义导向的解纠缠",{"2":{"70":1}}],["提出语义",{"2":{"60":1}}],["提出连续细化模型",{"2":{"51":1}}],["提出问题",{"2":{"49":1}}],["提出",{"2":{"49":2,"70":1,"72":1}}],["为第二个卷积块",{"2":{"194":1}}],["为第一个卷积块",{"2":{"194":1}}],["为w×d×n",{"2":{"188":1}}],["为下文解读之用",{"2":{"178":1}}],["为默认选项",{"2":{"171":1}}],["为您添加的其余代码",{"2":{"170":1}}],["为其专属定制编辑器",{"2":{"163":1}}],["为模型训练提供强大支撑",{"2":{"135":1}}],["为预测分配不同置信度",{"2":{"124":1}}],["为确保公平性",{"2":{"119":1}}],["为二进制分类提供细粒度信息",{"2":{"96":1}}],["为更好的类间区分提供多粒度证据",{"2":{"96":1}}],["为增强类间区分度",{"2":{"93":1}}],["为增强一致性",{"2":{"19":1}}],["为不同个体生成不同但互补的部分提示",{"2":{"72":1}}],["为夜间分割提供更可靠基准",{"2":{"70":1}}],["为应对不同领域间的分布差异",{"2":{"66":1}}],["为每个掩码选择合适的视觉特征图",{"2":{"57":1}}],["为证明模型的通用性",{"2":{"51":1}}],["为该领域建立重要基准",{"2":{"49":1}}],["为支持和查询特征分别构建专门的变换矩阵",{"2":{"44":1}}],["为查询图像生成粗略的分割掩码",{"2":{"44":1}}],["为解决合并数据集中标注质量不平衡的问题",{"2":{"107":1}}],["为解决上述问题",{"2":{"76":1}}],["为解决这一问题",{"2":{"75":1}}],["为解决这些问题",{"2":{"19":1,"94":1}}],["为解决跨领域医学图像的特征不对齐问题",{"2":{"69":1}}],["为解决超高分辨率图像分割中精度与计算成本的平衡问题",{"2":{"51":1}}],["为解决fss模型在跨领域场景下性能显著下降的问题",{"2":{"42":1}}],["为了出现和内嵌输出具有相同的效果",{"2":{"180":1}}],["为了测试",{"2":{"173":1}}],["为了后面不必要的麻烦",{"2":{"164":1}}],["为了让更多人能够有一个比较清晰的了解",{"2":{"163":1}}],["为了稳定训练过程",{"2":{"150":1}}],["为了缓解相似度图和掩码之间的分辨率差距",{"2":{"150":1}}],["为了融合全局和局部上下文信息",{"2":{"141":1}}],["为了获得高分辨率的细粒度特征",{"2":{"141":1}}],["为了深度融合两种监督方式的优势",{"2":{"114":1}}],["为了提高推理速度",{"2":{"101":1}}],["为了提高效率",{"2":{"34":1}}],["为了更方便进行编译",{"2":{"174":1}}],["为了更有效地泛化到未见类别",{"2":{"72":1}}],["为了更高效地在未知领域",{"2":{"72":1}}],["为了增强提示效果",{"2":{"72":1}}],["为了解决dino和sam生成的掩码过度分割问题",{"2":{"150":1}}],["为了解决在没有语义标签的情况下利用掩码的挑战",{"2":{"147":1}}],["为了解决这个问题",{"2":{"70":1}}],["为了解决上述问题",{"2":{"41":1}}],["为了减少这种需求",{"2":{"19":1}}],["为减少对大规模准确标注数据的需求",{"2":{"32":1}}],["为主干的",{"2":{"31":1}}],["为此开发了互补式自蒸馏方法",{"2":{"75":1}}],["为此",{"2":{"28":1,"93":1,"99":1,"104":1}}],["为缓解数据标注压力",{"2":{"20":1}}],["为进一步提升性能",{"2":{"19":1}}],["为降低不确定性",{"2":{"19":1}}],["但其只能测试一部分功能",{"2":{"173":1}}],["但其在零样本",{"2":{"104":1}}],["但笔者不建议这么做",{"2":{"171":1}}],["但却可以对自己想要的功能直接进行代码编写",{"2":{"168":1}}],["但一些设置需要去寻找",{"2":{"168":1}}],["但建议在不明白各个选项的作用时",{"2":{"164":1}}],["但texstudio的代码高亮功能实在是",{"2":{"163":1}}],["但掩码无语义标签",{"2":{"149":1}}],["但图像标题通常只提供图像中有什么",{"2":{"148":1}}],["但传统分割数据集获取密集标注的语义标签需大量人力",{"2":{"148":1}}],["但标注成本高昂",{"2":{"139":1}}],["但由于语义分割标注的获取成本极高",{"2":{"138":1}}],["但缺乏大规模",{"2":{"135":1}}],["但计算机视觉问题广泛",{"2":{"135":1}}],["但计算复杂度高",{"2":{"95":1}}],["但该方法的miou仍提高了0",{"2":{"131":1}}],["但未充分发挥其特征增强和引导作用",{"2":{"129":1}}],["但以上的方法都存在不足",{"2":{"115":1}}],["但以往研究忽略了其在建模位置对关系中的作用",{"2":{"32":1}}],["但是这个思想一个很重要的问题就是效率低",{"2":{"190":1}}],["但是其编译速度比较慢",{"2":{"163":1}}],["但是上述两种方法都存在不足",{"2":{"158":1}}],["但是",{"2":{"128":1}}],["但是也面临着一些挑战",{"2":{"115":1}}],["但是大多数还是利用预训练的视觉语言模型提取embeding",{"2":{"55":1}}],["但增加数据类别和改进语言模型有望解决",{"2":{"111":1}}],["但泛化能力受限",{"2":{"106":1}}],["但直接合并不同领域的数据集会导致标签分类体系冲突",{"2":{"105":1}}],["但直接插值预测结果存在边缘锯齿和细节缺失问题",{"2":{"51":1}}],["但都需额外无标签数据",{"2":{"95":1}}],["但获取像素级标注需大量人力和成本",{"2":{"94":1}}],["但效率低",{"2":{"86":1}}],["但面临诸多挑战",{"2":{"86":1}}],["但这种方法存在明显局限",{"2":{"85":1}}],["但这些方法存在一定局限性",{"2":{"129":1}}],["但这些方法存在一定缺陷",{"2":{"128":1}}],["但这些方法未明确估计光照对语义的影响",{"2":{"70":1}}],["但这些数据驱动的技术在标注数据不足时表现不佳",{"2":{"72":1}}],["但性能常不如多阶段方法",{"2":{"77":1}}],["但推理速度具有竞争力",{"2":{"72":1}}],["但综合效率和性能考虑",{"2":{"72":1}}],["但存在信息缺失问题",{"2":{"149":1}}],["但存在明显局限",{"2":{"105":1}}],["但存在标注错误",{"2":{"70":1}}],["但存在图形模型依赖低层次颜色边界",{"2":{"51":1}}],["但它们通常基于光照纠缠的表示进行场景解析",{"2":{"70":1}}],["但因夜间缺乏对应标签",{"2":{"70":1}}],["但松弛白化变换损失无法保证不同领域的医学图像在同一通道上显示相似的特征响应",{"2":{"66":1}}],["但难以应对稀疏训练数据",{"2":{"95":1}}],["但难以应对不同成像条件下任意未见领域的特征分布变化",{"2":{"64":1}}],["但难以利用",{"2":{"76":1}}],["但难以利用大预训练模型知识",{"2":{"56":1}}],["但llm参数多致速度慢",{"2":{"60":1}}],["但实际中",{"2":{"64":1}}],["但实际场景中收集大量训练数据耗时且成本高",{"2":{"42":1}}],["但实验中主要使用简单提示",{"2":{"59":1}}],["但主要用于通用表示和预测",{"2":{"56":1}}],["但大多聚焦白天场景",{"2":{"70":1}}],["但大多仅从视觉",{"2":{"56":1}}],["但大多不是为dgss设计",{"2":{"49":1}}],["但不能够很好的处理真实世界中各种新的目标",{"2":{"55":1}}],["但本文通过利用大型语言模型",{"2":{"54":1}}],["但更注重细节",{"2":{"51":1}}],["但结构复杂",{"2":{"51":1}}],["但多采用vggnet",{"2":{"49":1}}],["但在训练监督和人工成本间难以平衡",{"2":{"116":1}}],["但在推理时需要下采样和裁剪补丁",{"2":{"51":1}}],["但在dgss中的性能及利用方式尚不明确",{"2":{"49":1}}],["但在dgss任务中的表现缺乏专门研究",{"2":{"49":1}}],["但在源域和目标域差距大时",{"2":{"43":1}}],["但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征",{"2":{"42":1}}],["但在实际应用中",{"2":{"42":1}}],["但准确标注分割数据集成本高",{"2":{"32":1}}],["但忽视了关联图在建模像素位置关系中的重要作用",{"2":{"31":1}}],["但仍存在预测不确定和不一致问题",{"2":{"21":1}}],["但仍存在预测结果不确定和不一致的问题",{"2":{"20":1}}],["但需大量细粒度标注的训练数据",{"2":{"20":1}}],["但通常需要大量带有细粒度标注的训练数据",{"2":{"19":1}}],["但受背景干扰",{"2":{"10":1}}],["代码解读",{"0":{"179":1}}],["代码展示",{"0":{"178":1}}],["代码进行编译",{"2":{"170":1}}],["代码不算哈哈哈",{"2":{"163":1}}],["代码已发布",{"2":{"49":1}}],["代码已开源",{"2":{"31":1,"104":1}}],["代码",{"2":{"18":1}}],["代替多层感知器",{"2":{"15":1}}],["http",{"2":{"164":1}}],["https",{"2":{"18":1,"30":1,"31":1,"40":1,"41":1,"49":2,"51":1,"62":1,"63":1,"70":1,"101":1,"103":1,"104":1,"113":1,"114":1,"126":1,"127":1,"135":1,"146":1,"164":1}}],["html",{"2":{"164":2}}],["hin−1",{"2":{"193":1}}],["hinder",{"2":{"103":1}}],["hides",{"0":{"112":1},"1":{"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1}}],["hierarchical",{"2":{"101":5,"137":1}}],["highlighted",{"2":{"220":2}}],["highlighting",{"0":{"220":1},"2":{"220":2}}],["highly",{"2":{"126":1}}],["high",{"0":{"50":2},"2":{"30":1,"51":3,"126":1,"137":1}}],["human",{"2":{"103":1}}],["humans",{"2":{"51":1,"72":1}}],["hunan",{"2":{"52":1}}],["hundreds",{"2":{"40":1}}],["hard",{"2":{"90":1}}],["harness",{"2":{"49":2}}],["harnessing",{"0":{"48":1},"1":{"49":1},"2":{"53":1}}],["has",{"2":{"53":1,"101":1,"103":1,"126":1,"137":2}}],["have",{"2":{"8":1,"18":2,"30":1,"40":1,"53":1,"70":1,"103":1,"146":1,"155":1}}],["hypercorrelations",{"2":{"40":1}}],["hypercorrelation",{"2":{"40":1,"44":1}}],["hello",{"2":{"173":1}}],["help",{"2":{"155":1}}],["helps",{"2":{"18":1}}],["heiti",{"2":{"173":1}}],["here",{"2":{"165":1,"171":1,"173":1,"176":1}}],["heterogeneous",{"2":{"18":1}}],["hout=",{"2":{"193":1}}],["hohai",{"2":{"125":1}}],["how",{"2":{"92":1}}],["however",{"2":{"18":1,"40":1,"62":1,"72":1,"92":1,"103":1,"113":1,"126":1,"137":1,"146":1}}],["hospitals",{"2":{"62":1}}],["holistic",{"2":{"18":1}}],["gz",{"2":{"179":2}}],["geometry",{"2":{"173":1}}],["genome",{"2":{"160":1}}],["general",{"2":{"146":1}}],["generalize",{"2":{"62":1}}],["generalized",{"0":{"48":1,"61":1},"1":{"49":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"40":1,"49":1,"62":4}}],["generalization",{"2":{"51":1,"72":1,"103":2,"135":1}}],["generaliz",{"2":{"49":1}}],["generating",{"2":{"137":1,"155":1}}],["generation",{"2":{"92":1,"101":2}}],["generator",{"2":{"72":1}}],["generated",{"2":{"40":1,"126":1,"146":1}}],["generate",{"2":{"8":1,"72":1,"113":1}}],["gls",{"2":{"170":1,"171":1,"182":1}}],["glo",{"2":{"170":1,"171":1,"182":1}}],["global",{"2":{"74":1,"92":1,"113":2,"137":2}}],["glg",{"2":{"170":1,"171":1,"182":1}}],["gaining",{"2":{"113":1}}],["gap性能优于gmp",{"2":{"80":1}}],["gap",{"2":{"51":1,"80":1,"141":1}}],["gpu",{"2":{"101":1}}],["gpu内存使用和模型存储要求",{"2":{"49":1}}],["gt",{"2":{"101":3,"174":2,"193":1}}],["google",{"2":{"145":1}}],["good",{"2":{"30":1}}],["gopt在准确性和效率上达到了最佳平衡",{"2":{"90":1}}],["gopt",{"2":{"84":2,"85":2,"88":1}}],["gmp",{"2":{"80":1}}],["g",{"2":{"72":1,"74":2,"137":1,"155":1,"178":1,"179":1,"182":1}}],["guiding",{"2":{"146":1}}],["guides",{"2":{"62":1}}],["guided",{"2":{"57":1}}],["guangxi",{"2":{"61":1}}],["git下载",{"2":{"70":1}}],["git访问",{"2":{"49":1}}],["git",{"2":{"49":2,"70":1}}],["github",{"2":{"18":1,"30":1,"31":1,"40":1,"41":1,"49":3,"51":2,"62":1,"63":1,"70":2,"101":2,"103":1,"104":1,"113":1,"114":1,"126":1,"127":1,"146":1,"147":1,"171":1,"173":1}}],["groups=1",{"2":{"193":1}}],["groups",{"2":{"101":1,"193":1}}],["group",{"2":{"91":1}}],["grouping",{"0":{"82":1},"1":{"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1},"2":{"84":3,"88":1,"90":1}}],["great",{"2":{"30":1}}],["granularities",{"2":{"155":1}}],["gradual",{"2":{"101":3}}],["graphnet",{"2":{"26":1}}],["grained",{"2":{"18":1,"92":1,"137":2}}],["ui界面设置",{"2":{"181":1}}],["ui",{"2":{"168":4}}],["uti",{"2":{"113":1}}],["utilization",{"2":{"92":1}}],["utilizes",{"2":{"126":1}}],["utilized",{"2":{"103":1}}],["utilize",{"2":{"53":1}}],["utilizing",{"2":{"40":1,"101":1}}],["up",{"0":{"102":1},"1":{"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1},"2":{"53":1,"62":1,"103":1}}],["upon",{"2":{"49":1}}],["uous",{"2":{"51":1}}],["ultra",{"0":{"50":1},"2":{"51":3}}],["urban",{"2":{"49":1}}],["urss",{"2":{"26":1}}],["u2pl",{"2":{"33":1}}],["uestc",{"2":{"29":1}}],["unet",{"0":{"218":1}}],["unmerging",{"2":{"141":1}}],["unc",{"2":{"80":3}}],["uncertain",{"2":{"74":2}}],["uncertainty",{"2":{"18":1,"113":1}}],["unlike",{"2":{"53":1}}],["unlabeled",{"2":{"30":2,"92":3,"113":1,"126":1,"146":1}}],["unprecedented",{"2":{"53":1}}],["unseen",{"2":{"40":1,"62":1,"72":1,"103":3}}],["understanding",{"2":{"137":2,"146":2}}],["underperforming",{"2":{"70":1}}],["under",{"2":{"18":1,"70":1}}],["universal",{"0":{"154":1},"1":{"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"155":2,"157":1}}],["universit",{"2":{"17":1}}],["university",{"0":{"83":1},"2":{"17":1,"29":1,"39":2,"52":2,"61":2,"70":1,"73":3,"91":1,"112":1,"125":2,"145":1}}],["uni",{"2":{"84":1,"88":1}}],["uniformly",{"2":{"18":1}}],["usepackage",{"2":{"173":3}}],["use框架在ade20k",{"2":{"162":1}}],["use方法在所有数据集上大幅优于最先进的两阶段方法",{"2":{"160":1}}],["use模型不仅可以帮助开放词汇表图像分割",{"2":{"156":1}}],["use",{"0":{"154":1},"1":{"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"30":1,"155":3,"156":1,"157":1,"159":2}}],["uses",{"2":{"18":1}}],["user",{"2":{"18":1}}],["used",{"2":{"0":1,"40":1,"70":1}}],["usedata",{"2":{"0":3}}],["using",{"2":{"8":1,"101":1,"103":1,"113":1,"135":1,"137":1,"146":2}}],["usage",{"2":{"0":1}}],["缓解空间感知偏差",{"2":{"16":1}}],["缓解骨干网络的固有偏差",{"2":{"12":1}}],["中",{"2":{"171":1}}],["中文语言环境配置",{"0":{"166":1}}],["中的范围",{"2":{"194":1}}],["中的",{"2":{"171":1}}],["中的轻量级卷积解码器",{"2":{"78":1}}],["中的视觉编码器",{"2":{"57":1}}],["中进行泛化",{"2":{"72":1}}],["中科院",{"0":{"72":1}}],["中评估多种vfms",{"2":{"49":1}}],["中利用视觉基础模型",{"2":{"49":1}}],["中国科学技术大学",{"0":{"49":1}}],["中国科学院",{"0":{"7":1}}],["中施加一致性损失",{"2":{"19":1}}],["中固有偏差",{"2":{"16":1}}],["结合clip和dinov2的信息",{"2":{"159":1}}],["结合unc",{"2":{"80":1}}],["结合部分掩码生成器",{"2":{"72":1}}],["结合语义提示转移",{"2":{"72":1}}],["结合编码器",{"2":{"70":1}}],["结合伪标签再训练",{"2":{"28":1}}],["结论与不足",{"2":{"101":1}}],["结论",{"0":{"16":1,"28":1,"38":1,"47":1,"60":1,"69":1,"81":1,"99":1,"111":1,"124":1,"133":1,"144":1,"153":1,"162":1},"2":{"51":1,"70":1,"90":1}}],["结果略有下降",{"2":{"80":1}}],["结果显示包含cls令牌可提高miou",{"2":{"161":1}}],["结果显示corrmatch始终优于现有最佳方法",{"2":{"35":1}}],["结果显示",{"2":{"15":1,"68":1}}],["结果表明结合clip和dinov2可获得性能提升",{"2":{"161":1}}],["结果表明该方法比mseg和joem更具鲁棒性",{"2":{"110":1}}],["结果表明该方法在camvid",{"2":{"110":1}}],["结果表明gopt仅训练不到1",{"2":{"88":1}}],["结果表明crm性能更好",{"2":{"51":1}}],["结果表明使用标签传播策略后",{"2":{"37":1}}],["结果表明",{"2":{"15":1,"54":1,"110":2,"123":1,"132":1,"160":1}}],["这时候需要对边缘进行裁切或者补零",{"2":{"188":1}}],["这时可以使用外部查看器进行查看",{"2":{"175":1}}],["这要我们就可以把每一个像素的预测看成是一个分类任务",{"2":{"188":1}}],["这就意味着",{"2":{"171":1}}],["这里的快捷键为默认设置",{"2":{"174":1}}],["这里就不作赘述",{"2":{"165":1}}],["这里是重点compared",{"2":{"74":1}}],["这个时候可能就是由于辅助文件没有进行及时更新的缘故",{"2":{"171":1}}],["这个选项",{"2":{"165":1}}],["这个选项一定要选中",{"2":{"164":1}}],["这个模型被特别设计并训练为能够接受简单提示",{"2":{"135":1}}],["这还需要理解物体的位置",{"2":{"147":1}}],["这表明该方法直接受益于图像级弱监督语义分割方法",{"2":{"123":1}}],["这项技术的核心难点在于如何有效融合不同模态",{"2":{"85":1}}],["这类方法通常会过度关注最具区分度的区域",{"2":{"75":1}}],["这是使用从视觉基础模型",{"2":{"147":1}}],["这是由于模型对伪标签中的噪声标签过拟合",{"2":{"122":1}}],["这是一个自动驾驶数据集",{"2":{"70":1}}],["这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域",{"2":{"15":1}}],["这也是未来工作的研究方法",{"2":{"60":1}}],["这限制了模型在实际应用中对超高分辨率数据的处理能力",{"2":{"51":1}}],["这在人像照片后期处理",{"2":{"51":1}}],["这样的操作称为上采样",{"2":{"193":1}}],["这样可以帮助网络在复杂多变的光照条件下持续准确地识别语义",{"2":{"70":1}}],["这样",{"2":{"49":1}}],["这些模型在进行语义分割时仍需像素级语义标签",{"2":{"148":1}}],["这些数据集涵盖了不同的标注风格和图像领域",{"2":{"109":1}}],["这些场景光照充足且均匀",{"2":{"70":1}}],["这些方法假设测试集中的所有类别都在训练时出现",{"2":{"105":1}}],["这些方法完全依赖于支持图像进行特征转换",{"2":{"41":1}}],["这些方法在一些公开数据集",{"2":{"33":1}}],["这些策略使模型能更高效地利用未标记数据",{"2":{"38":1}}],["这两个比例显著高于未使用时",{"2":{"37":1}}],["这种扩散过程未能有效利用全局语义信息和类别特异性特征线索",{"2":{"114":1}}],["这种方法能显著降低标注成本",{"2":{"114":1}}],["这种方式能保留标签之间的语义关系",{"2":{"107":1}}],["这种方式仅提供有限的语义信息",{"2":{"55":1}}],["这种向量化的语义表示可以融合不同领域",{"2":{"104":1}}],["这种现象的典型表现是物体边界区域的识别精度下降",{"2":{"75":1}}],["这种固定参数的特征编码器往往对类别不敏感",{"2":{"72":1}}],["这种固定的编码器通常是类别无关的",{"2":{"72":1}}],["这种任务不仅在技术上具有挑战性",{"2":{"63":1}}],["这种自监督机制既继承了神经特征空间的类别级判别能力",{"2":{"19":1}}],["这种关系不足以准确匹配",{"2":{"12":1}}],["512",{"2":{"194":2}}],["5k",{"2":{"101":2}}],["500步时将学习率降至十分之一",{"2":{"51":1}}],["500和37",{"2":{"51":1}}],["50作为编码器eθ",{"2":{"51":1}}],["572张具有超过1000个语义类别的图像",{"2":{"51":1}}],["59上分别提高了7",{"2":{"58":1}}],["59",{"2":{"45":1,"101":1}}],["5分钟",{"2":{"20":1}}],["5",{"0":{"168":1,"191":1},"2":{"15":1,"35":1,"36":2,"79":2,"96":1,"97":2,"109":1,"132":1,"141":1,"143":1,"168":1,"194":3}}],["5i数据集",{"2":{"97":1}}],["5i和coco基准测试中超越了现有技术水平",{"2":{"99":1}}],["5i和coco数据集上的表现优于现有方法",{"2":{"96":1}}],["5i和coco",{"2":{"9":1,"12":1,"16":1,"72":4}}],["5i",{"2":{"8":1,"14":1,"93":1,"97":1}}],["和输入内部插",{"2":{"193":1}}],["和区域的交并比",{"2":{"191":1}}],["和微调miou",{"2":{"143":1}}],["和局部注意力阶段",{"2":{"141":1}}],["和计算机视觉领域的发展现状与需求",{"2":{"135":1}}],["和对应的数据集",{"2":{"135":1}}],["和距离熵损失",{"2":{"117":1}}],["和满足特定条件的像素百分比",{"2":{"109":1}}],["和全局最大池化",{"2":{"80":1}}],["和确定区域",{"2":{"80":1}}],["和75",{"2":{"79":1}}],["和3",{"2":{"72":1}}],["和2",{"2":{"59":1}}],["和关系注意力",{"2":{"57":1}}],["和多尺度可变形注意力",{"2":{"57":1}}],["和pascal",{"2":{"54":1}}],["和平均精度",{"2":{"51":1}}],["和",{"2":{"49":1,"75":1,"93":1,"101":4,"104":2,"138":1,"164":1,"171":1,"180":1}}],["和4",{"2":{"45":1,"58":2,"72":1}}],["和域泛化语义分割",{"2":{"43":1}}],["和81",{"2":{"36":1}}],["和1",{"2":{"35":1,"36":1,"63":1,"69":1,"72":1,"79":1}}],["和5",{"2":{"35":1,"58":2,"72":1}}],["和0",{"2":{"35":2,"36":2,"67":1}}],["和类内差异表示",{"2":{"15":1}}],["和双分类模块",{"2":{"15":1,"16":1}}],["和双重分类模块",{"2":{"9":1}}],["如需写入到",{"2":{"178":1}}],["如上图所示",{"2":{"164":1}}],["如下图所示",{"2":{"166":1}}],["如下图",{"2":{"164":1,"174":1,"180":1}}],["如下采样",{"2":{"51":1}}],["如果bias=true",{"2":{"193":1}}],["如果出现",{"2":{"174":1}}],["如果使用onbuilt命令",{"2":{"171":1}}],["如果说论文中有很多图片或者其他元素没有嵌入字体的话",{"2":{"171":1}}],["如果您对此不感兴趣",{"2":{"171":1}}],["如果您日后需要在上述代码之后再添加其他代码",{"2":{"170":1}}],["如果您需要个性化程度高的话",{"2":{"164":1}}],["如果您想了解",{"2":{"164":1}}],["如果下载速度过慢",{"2":{"164":1}}],["如grounding",{"2":{"159":1}}],["如dino和sam等视觉基础模型可生成细粒度掩码",{"2":{"149":1}}],["如dino",{"2":{"148":1}}],["如deeplab系列引入空洞空间金字塔池化",{"2":{"70":1}}],["如sam",{"2":{"156":1}}],["如sam和dino",{"2":{"147":1}}],["如sg",{"2":{"95":1}}],["如色彩化",{"2":{"140":1}}],["如交互式分割",{"2":{"135":1}}],["如使用涂鸦",{"2":{"129":1}}],["如使用预训练或低分辨率训练测试",{"2":{"51":1}}],["如引入类激活图",{"2":{"116":1}}],["如objects365",{"2":{"107":1}}],["如openimages",{"2":{"107":1}}],["如bucher用word2vec编码标签",{"2":{"106":1}}],["如ros合并六个驾驶数据集",{"2":{"106":1}}],["如fcn开启全卷积方法",{"2":{"106":1}}],["如图像字幕",{"2":{"149":1}}],["如图像标题",{"2":{"148":1}}],["如图像",{"2":{"85":1}}],["如图像级监督",{"2":{"20":1}}],["如第7",{"2":{"80":1}}],["如比beco分别高2",{"2":{"79":1}}],["如比unimatch在各分割比例上分别高出1",{"2":{"35":1}}],["如物体边界和易混淆类别",{"2":{"75":1}}],["如高置信度的前景和背景",{"2":{"75":1}}],["如计算机视觉",{"2":{"72":1}}],["如在gan框架中学习跨域不变表示",{"2":{"70":1}}],["如在眼底和前列腺基准测试中",{"2":{"69":1}}],["如egnet",{"2":{"70":1}}],["如",{"2":{"51":1,"57":1}}],["如针对1k",{"2":{"51":1}}],["如clip和align",{"2":{"149":1}}],["如clip和align利用对比学习训练文本和图像编码器",{"2":{"135":1}}],["如clip和align利用对比学习训练文本和图像编码器实现零样本泛化",{"2":{"135":1}}],["如clip",{"2":{"49":1,"148":2}}],["如ppnet和soopil的方法",{"2":{"95":1}}],["如pixda",{"2":{"43":1}}],["如pascal",{"2":{"33":1}}],["如采用mean",{"2":{"33":1}}],["如mean",{"2":{"32":1}}],["如原始的普通注意力",{"2":{"15":1}}],["如动态调整分类器权重",{"2":{"11":1}}],["且同样支持双向同步",{"2":{"180":1}}],["且根据需要关闭标签",{"2":{"180":1}}],["且根据笔者使用来看",{"2":{"175":1}}],["且侧面带有书签",{"2":{"180":1}}],["且支持双向同步功能",{"2":{"175":1}}],["且需要为英文路径",{"2":{"171":1}}],["且弹窗弹出比较烦人",{"2":{"171":1}}],["且代码设置可以直接克隆别人的代码到自己的编辑器中",{"2":{"168":1}}],["且所有引用在文中或文末注明了来源",{"2":{"163":1}}],["且",{"2":{"163":1}}],["且优于使用图像级语义标签的方法",{"2":{"153":1}}],["且要通过密集的自监督信号实现上述两点",{"2":{"139":1}}],["且生成的边界不如一些计算密集型的",{"2":{"135":1}}],["且现有模型在泛化能力和处理模糊提示方面存在不足",{"2":{"135":1}}],["且现有方法难以实现强大的泛化能力",{"2":{"135":1}}],["且现有的参数高效微调策略大多不适用于dgss",{"2":{"49":1}}],["且性能随模型规模",{"2":{"135":1}}],["且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用",{"2":{"128":1}}],["且比点",{"2":{"128":1}}],["且不能在现实世界中进行语义分割",{"2":{"115":1}}],["且模型受限于训练数据集的图像领域",{"2":{"105":1}}],["且因样本标注有限",{"2":{"86":1}}],["且当基于cam的选择不那么严格时",{"2":{"80":1}}],["且可扩展到跨领域",{"2":{"72":1}}],["且这一问题未得到实质性解决",{"2":{"72":1}}],["且弱监督",{"2":{"65":1}}],["且存在细粒度分类和边界分割问题",{"2":{"60":1}}],["且只能识别固定集对象",{"2":{"56":1}}],["且flops和参数更少",{"2":{"51":1}}],["且对gpu内存需求大",{"2":{"51":1}}],["且依赖复杂数据增强和领域不变特征提取策略",{"2":{"49":1}}],["且推理过程无额外计算负担",{"2":{"38":1}}],["且在各分割比例上均优于现有方法",{"2":{"35":1}}],["且整个过程无需额外信息或注释准备要求",{"2":{"28":1}}],["且前馈网络在略微增加计算成本的情况下保留了更多特征细节",{"2":{"15":1}}],["且参数数量较少",{"2":{"12":1}}],["作为外部查看器",{"2":{"175":1}}],["作为",{"2":{"164":1}}],["作为一种强大的排版系统",{"2":{"163":1}}],["作为伪标签",{"2":{"77":1}}],["作为种子区域",{"2":{"76":1}}],["作为前馈网络",{"2":{"15":1}}],["作者通过掩码池化将无标签掩码集成到clip图像特征图中",{"2":{"150":1}}],["作者通过研究得出以下结论",{"2":{"28":1}}],["作者希望该简单框架能推动无标签或少量标签语义分割的广泛应用",{"2":{"144":1}}],["作者希望这项工作能为小样本场景下的编码器设计提供新视角",{"2":{"72":1}}],["作者在摒弃以往冻结编码器以泛化到未见类别的小样本分割",{"2":{"72":1}}],["作者认为理想的fss特征编码器应具有类别感知能力",{"2":{"72":1}}],["作者引入rein微调方法",{"2":{"49":1}}],["作者评估并利用vfms进行dgss研究",{"2":{"49":1}}],["作者提出用于开放词汇图像分割的use框架",{"2":{"162":1}}],["作者提出用于跨域少样本语义分割的dmtnet",{"2":{"47":1}}],["作者提出使用可学习的类提示将语义相似的掩码聚类成全局共享的簇",{"2":{"150":1}}],["作者提出基于原型的特征增强方法",{"2":{"128":1}}],["作者提出一种语义分割方法",{"2":{"111":1}}],["作者提出一种基于transformer的自适应原型匹配网络",{"2":{"10":1}}],["作者提出评估vfms在dgss中的性能以及如何有效利用vfms的问题",{"2":{"49":1}}],["作者提出了pixelclip框架用于开放词汇语义分割",{"2":{"153":1}}],["作者提出了相关内在特征增强网络",{"2":{"99":1}}],["作者提出了用于",{"2":{"124":1}}],["作者提出了用于开放词汇语义分割的sed方法",{"2":{"101":1}}],["作者提出了用于多模态图像分割的参数高效视觉调优框架gopt",{"2":{"90":1}}],["作者提出了用于超高清图像分割细化的连续细化模型",{"2":{"51":1}}],["作者提出了nightcity",{"2":{"70":1}}],["作者提出了llmformer这一利用大语言模型",{"2":{"60":1}}],["作者提出了连续细化模型",{"2":{"51":1}}],["作者提出了一种自监督预训练方法",{"2":{"144":1}}],["作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割",{"2":{"133":1}}],["作者提出了一种基于语义不确定性引导的弱监督语义分割方法",{"2":{"81":1}}],["作者提出了一种基于transformer的自适应原型匹配网络",{"2":{"16":1}}],["作者提出了一种新颖的夜间语义分割范式",{"2":{"70":1}}],["作者提出了一种名为corrmatch的半监督语义分割方法",{"2":{"38":1}}],["作者提出了corrmatch方法",{"2":{"32":1}}],["作者提出了cc4s方法",{"2":{"20":1}}],["+outputpadding",{"2":{"193":2}}],["+kernelsize",{"2":{"193":2}}],["+点击",{"2":{"182":1}}],["+2",{"2":{"143":1}}],["+4",{"2":{"143":1}}],["+1",{"2":{"137":1}}],["+",{"2":{"15":2,"30":1,"31":1,"79":1,"143":1,"164":1,"168":1,"194":1}}],["实时计算且能处理歧义",{"2":{"135":1}}],["实例分割模型",{"2":{"189":1}}],["实例分割等",{"2":{"135":1}}],["实例分割等下游任务中带来显著性能提升",{"2":{"104":1}}],["实例分割",{"2":{"109":1,"110":1,"187":1}}],["实例分割在coco数据集上进行测试",{"2":{"109":1}}],["实现",{"2":{"193":1}}],["实现零样本泛化",{"2":{"135":1}}],["实现最多4",{"2":{"101":1}}],["实现更精确的分割",{"2":{"72":1}}],["实现更精准预测",{"2":{"70":1}}],["实现类别感知增强",{"2":{"72":1}}],["实现细节",{"2":{"51":1,"109":1}}],["实现了精确分割",{"2":{"15":1,"16":1}}],["实验发现",{"2":{"132":1}}],["实验内容",{"0":{"110":1}}],["实验结果",{"2":{"101":1,"119":1,"122":1}}],["实验结果表明",{"2":{"12":1,"72":1,"96":1,"110":4,"120":1,"124":1,"133":1}}],["实验设定",{"2":{"101":1}}],["实验证明了为无标签分支添加引导等操作的合理性和可靠性",{"2":{"98":1}}],["实验证明了我们的cormatch算法优于其他方法",{"2":{"38":1}}],["实验过程",{"0":{"89":1,"90":1,"131":1,"132":1}}],["实验验证",{"2":{"88":1}}],["实验数据显示",{"2":{"85":1}}],["实验步骤",{"2":{"51":1}}],["实验表明其在超高清图像上的分割效果最佳",{"2":{"51":1}}],["实验表明使用骨干网络特征的性能始终优于其他位置",{"2":{"36":1}}],["实验表明",{"2":{"16":1,"49":1,"51":1,"68":1,"93":1,"99":1,"127":1,"138":1}}],["实验",{"0":{"13":1,"26":1,"27":1,"35":1,"36":1,"45":1,"46":1,"58":1,"59":1,"67":1,"68":1,"79":1,"80":1,"97":1,"98":1,"108":1,"118":1,"121":1,"142":1,"143":1,"151":1,"152":1,"160":1,"161":1},"1":{"14":1,"15":1,"109":1,"110":1,"119":1,"120":1,"122":1,"123":1},"2":{"70":1,"72":1}}],["减少对大量高质量标注数据的依赖",{"2":{"144":1}}],["减少通道冗余",{"2":{"66":1,"67":1}}],["减少参数冗余",{"2":{"49":1}}],["减少神经表示的不确定性",{"0":{"23":1}}],["减少标注稀疏和多样性带来的影响",{"2":{"20":1}}],["减少了背景干扰",{"2":{"15":1}}],["减轻背景干扰",{"2":{"12":1}}],["减轻fss中的背景干扰",{"2":{"10":1}}],["33",{"2":{"122":1}}],["36",{"2":{"72":1}}],["366和1464分割比例上分别提高1",{"2":{"36":1}}],["38",{"2":{"67":1,"72":1}}],["31",{"2":{"62":1,"63":1,"69":1,"101":1,"194":1}}],["3",{"0":{"122":1,"166":1,"189":1,"199":1,"204":1,"205":1,"206":1,"207":1,"211":1},"1":{"206":1,"207":1},"2":{"15":2,"35":1,"58":1,"59":1,"72":3,"78":1,"81":1,"137":1,"138":1,"141":2,"143":1,"163":1,"193":4,"194":31}}],["6的输出处分别应用自蒸馏损失",{"2":{"141":1}}],["64",{"2":{"122":1,"194":1}}],["65",{"2":{"103":1,"104":1}}],["60",{"2":{"103":1,"104":1}}],["61",{"2":{"72":1}}],["63",{"2":{"67":1}}],["6k",{"2":{"51":3}}],["68",{"2":{"45":1,"68":1}}],["6",{"0":{"169":1,"170":1,"171":1},"1":{"170":1,"171":1},"2":{"15":1,"35":3,"36":1,"90":1,"101":2,"119":2,"122":1,"131":1,"133":1,"141":1,"142":1,"170":1,"194":1}}],["24",{"2":{"194":2}}],["2编译链",{"2":{"174":1}}],["2和6",{"2":{"141":1}}],["25",{"2":{"109":1}}],["25×10⁻⁴",{"2":{"51":1}}],["2k",{"2":{"101":1}}],["2k分辨率图像的方法",{"2":{"51":1}}],["27",{"2":{"72":1}}],["26",{"2":{"67":1}}],["2分割比例上分别高出0",{"2":{"35":1}}],["2020",{"2":{"173":1}}],["200",{"2":{"104":1}}],["20",{"2":{"101":1}}],["20k",{"2":{"101":1}}],["2015",{"2":{"103":1,"104":1}}],["2014基准上验证了有效性",{"2":{"81":1}}],["2014",{"2":{"74":1,"75":1,"79":2}}],["2012验证集上进行比较",{"2":{"131":1}}],["2012和ms",{"2":{"81":1}}],["2012和cityscapes等数据集上",{"2":{"38":1}}],["2012数据集",{"2":{"58":1}}],["2012数据集上达到了最先进的性能",{"2":{"133":1}}],["2012数据集上",{"2":{"51":1}}],["2012数据集上进行评估",{"2":{"51":1}}],["2012",{"2":{"26":1,"30":1,"31":1,"33":1,"35":3,"45":1,"74":1,"75":1,"79":2,"103":1,"104":1,"126":1,"127":1}}],["20i数据集上分别实现了4",{"2":{"72":1}}],["20i数据集上分别实现了3",{"2":{"72":1}}],["20i数据集上分别实现了2",{"2":{"72":1}}],["20i数据集上分别实现了0",{"2":{"72":1}}],["20i数据集上以最少的参数达到了最先进的性能",{"2":{"16":1}}],["20i数据集上进行了大量实验",{"2":{"9":1}}],["20i两个基准数据集上取得了优于现有方法的性能",{"2":{"12":1}}],["20i",{"2":{"8":1,"14":1}}],["2",{"0":{"110":1,"120":1,"165":1,"171":1,"174":1,"177":1,"178":1,"179":2,"188":1,"198":1,"201":1,"202":1,"203":1,"204":1,"207":1,"210":1},"1":{"178":1,"179":1,"202":1,"203":1,"204":1},"2":{"15":1,"35":2,"53":1,"54":1,"58":2,"59":1,"60":1,"70":2,"72":2,"79":1,"81":1,"103":1,"143":1,"151":1,"153":1,"155":1,"156":1,"163":1,"170":2,"171":2,"173":2,"182":2,"193":2,"194":1}}],["91",{"2":{"72":1}}],["96",{"2":{"72":1}}],["94",{"2":{"68":1}}],["98",{"2":{"62":1,"63":1,"69":1,"103":1}}],["92",{"2":{"30":1,"31":1}}],["9",{"0":{"180":1},"2":{"15":2,"35":1,"143":2,"171":1,"194":1}}],["三通道的图像",{"2":{"188":1}}],["三种deit变体在11个块时效果最佳",{"2":{"72":1}}],["三个关键组件组成",{"2":{"68":1}}],["三个主要模块",{"2":{"15":1}}],["三是分类阶段",{"2":{"10":1}}],["该软件的优点在于在具有",{"2":{"175":1}}],["该快捷键为默认设置",{"2":{"174":1}}],["该框架结合精心设计的数据管道和轻量级嵌入模型",{"2":{"162":1}}],["该框架由两个关键的组件",{"2":{"157":1}}],["该框架由两个关键组件组成",{"2":{"156":1}}],["该框架在网络架构和自监督目标方面都有创新",{"2":{"141":1}}],["该框架包含两大创新模块",{"2":{"85":1}}],["该数据集包括10亿个掩码和1100万张图像",{"2":{"135":1}}],["该范式让模型利用少量标注数据学习分割",{"2":{"94":1}}],["该范式模仿人类视觉感知模式",{"2":{"72":1}}],["该无标注分支在测试阶段可完全移除",{"2":{"93":1}}],["该网络通过协同利用涂鸦标注和基于图像级类别及全局语义引导的伪标签实现监督",{"2":{"114":1}}],["该网络引入无标签分支",{"2":{"99":1}}],["该网络创新性地引入无标注分支训练策略",{"2":{"93":1}}],["该网络包含目标增强模块",{"2":{"16":1}}],["该方案构建了一种动态的类别感知提示机制",{"2":{"72":1}}],["该方法因能在显著降低标注成本的同时实现高性能表现",{"2":{"127":1}}],["该方法优于现有方法",{"2":{"124":1}}],["该方法优于所有先前方法",{"2":{"119":1}}],["该方法可以达到与基线方法相当的性能",{"2":{"110":1}}],["该方法的性能也随之提高",{"2":{"123":1}}],["该方法的分割结果更优",{"2":{"110":1}}],["该方法的预训练权重可以显著提升性能",{"2":{"110":1}}],["该方法取得了最先进的性能",{"2":{"110":1}}],["该方法通过计算语义相似度实现未见过标签的分割",{"2":{"104":1}}],["该方法同样显著优于现有最优方法",{"2":{"67":1}}],["该方法能够以参数高效的方式利用vfm来解决dgss任务",{"2":{"49":1}}],["该方法达到了最先进的性能",{"2":{"28":1}}],["该方法在神经网络架构和自监督目标方面均有创新",{"2":{"144":1}}],["该方法在前列腺和眼底图像分割基准测试中显著优于现有最先进的方法",{"2":{"69":1}}],["该方法在涂鸦随机丢弃或按比例缩小的困难情况下也能表现良好",{"2":{"28":1}}],["该方法在pascal",{"2":{"16":1,"133":1}}],["该方法在降低计算复杂度的同时保持了较高的准确性",{"2":{"15":1}}],["该方法包含目标增强模块",{"2":{"15":1}}],["该子集包含314张夜间训练图像和31张验证图像",{"2":{"70":1}}],["该模块能够学习语义与光照之间的关系",{"2":{"70":1}}],["该模块通过建立全局原型",{"2":{"93":1}}],["该模块通过生成对象关系图",{"2":{"57":1}}],["该模块通过多头交叉注意力机制",{"2":{"57":1}}],["该模块根据属性先验生成掩码的属性嵌入",{"2":{"57":1}}],["该模块与软熵损失函数共同作用",{"2":{"19":1}}],["该模型显著提升单目深度估计和实例分割等下游应用的性能",{"2":{"111":1}}],["该模型在pascal",{"2":{"12":1}}],["该模型主要包含以下三个模块",{"2":{"12":1}}],["该模型包含三个模块",{"2":{"9":1}}],["导致路径已经是中文了",{"2":{"171":1}}],["导致特征模糊",{"2":{"98":1}}],["导致特征冗余",{"2":{"64":1,"66":1}}],["导致前景",{"2":{"93":1}}],["导致前景对象与背景的边界区域以及多语义不同对象内的误分类区域存在高度不确定性",{"2":{"76":1}}],["导致模型只能隐式学习物体位置",{"2":{"148":1}}],["导致模型泛化能力要求高",{"2":{"64":1}}],["导致模态间知识共享和模态内信息处理失衡",{"2":{"86":1}}],["导致弱监督与全监督方法间存在显著性能差异",{"2":{"75":1}}],["导致物体外观随光照变化",{"2":{"70":1}}],["导致速度减小",{"2":{"60":1}}],["导致fss模型对未见领域的泛化能力较差",{"2":{"42":1}}],["导致对未知类别的分割失败",{"2":{"15":1}}],["导致难以准确定位目标类别",{"2":{"12":1}}],["导致注意力偏差",{"2":{"12":1}}],["忽略噪声较大的部分",{"2":{"107":1}}],["忽略背景区域",{"2":{"42":1}}],["忽略了目标对象的空间一致性",{"2":{"12":1}}],["忽略空间信息",{"2":{"10":1}}],["由内转外操作相同",{"2":{"181":1}}],["由编辑器根据情况自动设置",{"2":{"179":1}}],["由图像编码器",{"2":{"135":1}}],["由图像级类标签监督",{"2":{"78":1}}],["由",{"2":{"78":1,"194":1}}],["由类激活映射",{"2":{"78":1}}],["由类内差异表示和双语义感知注意力机制两个关键部分组成",{"2":{"12":1}}],["由不同水平的标注者标注",{"2":{"64":1}}],["由于想要看到",{"2":{"175":1}}],["由于进行测试的文件中涉及参考文献的引用",{"2":{"174":1}}],["由于",{"2":{"164":1,"171":1}}],["由于没有语义标签",{"2":{"150":1}}],["由于基于涂鸦的注释具有灵活性",{"2":{"120":1}}],["由于大规模数据标注既困难又昂贵",{"2":{"104":1}}],["由于其提供了对场景的综合理解能力",{"2":{"55":1}}],["由于深度学习在计算机视觉领域的快速发展",{"2":{"10":1}}],["由此引出开放词汇语义分割",{"2":{"55":1}}],["双超相关构建模块",{"2":{"44":1}}],["双语义感知注意力",{"2":{"15":1}}],["双语义感知注意力机制通过两层约束",{"2":{"12":1}}],["双分类模块",{"2":{"12":1,"15":1}}],["双约束聚合模块",{"2":{"12":1,"15":2,"16":1}}],["双重约束聚合模块",{"2":{"9":1}}],["用一句话来解释",{"2":{"193":1}}],["用作下文",{"2":{"171":1}}],["用模型获取掩码嵌入",{"2":{"160":1}}],["用户注释的涂鸦长度可能不同",{"2":{"120":1}}],["用文本嵌入",{"2":{"104":1}}],["用渐进式融合解码器生成高分辨率特征图进行分割",{"2":{"101":1}}],["用于自动驾驶场景",{"0":{"203":1}}],["用于反向同步的内部查看器的键绑定",{"2":{"182":1}}],["用于配置编译链",{"2":{"182":1}}],["用于促进下游语义分割任务",{"2":{"144":1}}],["用于在特征空间中修正前景目标表示",{"2":{"114":1}}],["用于训练condinst模型",{"2":{"110":1}}],["用于提升实例分割和单目深度估计的性能",{"2":{"109":1}}],["用于提取图像的特征",{"2":{"22":1}}],["用于开放词汇语义分割",{"2":{"101":1}}],["用于解决少样本语义分割任务中存在的语义模糊和类间相似性问题",{"2":{"96":1}}],["用于多模态图像分割任务",{"2":{"88":1}}],["用于调整编码器以聚焦不同fss任务中的特定对象",{"2":{"72":1}}],["用于更有效的训练和验证评估",{"2":{"70":1}}],["用于跨域少样本语义分割",{"2":{"44":1}}],["用于半监督语义分割",{"2":{"38":1}}],["用于少样本语义分割",{"2":{"12":1}}],["用可逆神经网络",{"2":{"12":1}}],["倾向于提取与当前任务无关的特征",{"2":{"12":1}}],["存在显著的领域偏移问题",{"2":{"64":1}}],["存在图形模型依赖低级别颜色边界",{"2":{"51":1}}],["存在固有偏差",{"2":{"12":1}}],["存在三方面问题",{"2":{"10":1}}],["任务创新",{"2":{"135":1}}],["任务中",{"2":{"49":1}}],["任务",{"2":{"12":1}}],["研究意义",{"2":{"162":1}}],["研究cls令牌对性能的影响",{"2":{"161":1}}],["研究了聚类数量",{"2":{"152":1}}],["研究了不同骨干网络对方法的影响",{"2":{"132":1}}],["研究了解耦查询和风格不变的键与值的影响",{"2":{"68":1}}],["研究者越来越关注利用涂鸦标签进行监督学习的方法",{"2":{"128":1}}],["研究者提出弱监督",{"2":{"32":1}}],["研究者提出了多种弱监督标注策略",{"2":{"19":1}}],["研究结论",{"2":{"72":1}}],["研究方法",{"2":{"51":1,"101":1}}],["研究思路清晰",{"2":{"49":1}}],["研究思路",{"2":{"49":1}}],["研究动机",{"2":{"49":1}}],["研究现状",{"0":{"11":1,"21":1,"33":1,"43":1,"56":1,"65":1,"77":1,"87":1,"95":1,"106":1,"116":1,"129":1,"140":1,"149":1,"158":1},"2":{"49":1,"51":1,"70":1,"72":1,"101":1,"135":1}}],["研究背景如下",{"2":{"76":1}}],["研究背景",{"0":{"10":1,"20":1,"32":1,"42":1,"55":1,"64":1,"76":1,"86":1,"94":1,"105":1,"115":1,"128":1,"139":1,"148":1,"157":1},"2":{"51":1,"70":1,"72":1,"101":1,"135":1}}],["以池化层为区分",{"2":{"194":1}}],["以下展示由外部查看转为内部查看的操作",{"2":{"181":1}}],["以下为具体操作",{"2":{"180":1}}],["以下是详细介绍",{"2":{"141":1}}],["以下是该模型的详细介绍",{"2":{"88":1,"96":1}}],["以此来进行更多更好的文字编写",{"2":{"182":1}}],["以此来选中",{"2":{"174":1}}],["以此测试其是否支持中英文",{"2":{"173":1}}],["以此可以随时对自己的配置代码进行更改",{"2":{"163":1}}],["以免后期使用产生奇怪的问题",{"2":{"164":1}}],["以平均交并比",{"2":{"160":1}}],["以获取更丰富的语义信息",{"2":{"159":1}}],["以微调clip图像编码器",{"2":{"150":1}}],["以实现开放词汇语义分割",{"2":{"148":1}}],["以实现高效",{"2":{"51":1}}],["以确保编码器和解码器网络得到充分的预训练",{"2":{"141":1}}],["以提升语义分割任务的性能",{"2":{"139":1}}],["以提高涂鸦监督语义分割的确定性和一致性",{"2":{"20":1}}],["以挖掘其巨大潜力和应用价值",{"2":{"133":1}}],["以仅使用部分交叉熵损失作为基线",{"2":{"132":1}}],["以评估其影响",{"2":{"123":1}}],["以生成最佳结果",{"2":{"119":1}}],["以生成类别感知特征",{"2":{"72":1}}],["以验证模型的鲁棒性和有效性",{"2":{"110":1}}],["以捕获高维类别级别的类别信息",{"2":{"96":1}}],["以捕获多尺度的图像特征",{"2":{"57":1}}],["以进行准确的识别",{"2":{"96":1}}],["以少量提示参数促进模型快速收敛",{"2":{"88":1}}],["以深度多模态融合为主",{"2":{"87":1}}],["以继承基础模型的强大特征提取能力",{"2":{"85":1}}],["以改进模型训练",{"2":{"78":1}}],["以约束最后一层补丁令牌之间的亲和性",{"2":{"78":1}}],["以明确不确定区域的视觉语义",{"2":{"76":1}}],["以动态驱动编码器关注特定对象",{"2":{"72":1}}],["以极少额外参数助力现有方法提升性能",{"2":{"70":1}}],["以更灵活的方式实现通道解耦特征学习",{"2":{"63":1}}],["以更好地分割不同大小的对象",{"2":{"57":1}}],["以更准确地在未知领域自我调整查询预测",{"2":{"41":1}}],["以增强开放词汇对象的发现",{"2":{"57":1}}],["以利用llm的先验知识进行开放词汇语义分割",{"2":{"57":1}}],["以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机",{"2":{"49":1}}],["以较少可训练参数有效利用vfms",{"2":{"49":1}}],["以有效利用vfms的强大能力",{"2":{"49":1}}],["以高效利用vfms解决dgss问题",{"2":{"49":1}}],["以解决新数据分布下的一系列下游分割问题",{"2":{"135":1}}],["以解决现有方法的不足",{"2":{"128":1}}],["以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战",{"2":{"88":1}}],["以解决现有fss模型存在的固有偏差",{"2":{"12":1}}],["以解决现有fss模型存在的问题",{"2":{"10":1}}],["以解决特征变换过度依赖支持图像",{"2":{"42":1}}],["以及专门设计的自监督训练策略用于学习细粒度语义特征",{"2":{"138":1}}],["以及构建数据引擎收集大规模数据集",{"2":{"135":1}}],["以及风格解耦的查询",{"2":{"68":1}}],["以及训练速度",{"2":{"49":1}}],["以及近期的unimatch等单阶段框架",{"2":{"33":1}}],["以及在神经特征空间进行自监督以保证输出的一致性",{"2":{"28":1}}],["以及用可逆神经网络",{"2":{"15":1}}],["以cityscapes为例",{"2":{"20":1}}],["以应对少样本语义分割",{"2":{"16":1}}],["以往少样本语义分割方法存在语义模糊和类间相似性问题",{"2":{"94":1}}],["以往dgss方法着重提升模型在多未见领域的预测准确性",{"2":{"49":1}}],["以往的分割细化方法",{"2":{"51":1}}],["以往的研究已经证明了特征转换在解决cd",{"2":{"41":1}}],["以往的fss方法由于固有偏差",{"2":{"9":1}}],["以往工作依赖预训练骨干网络直接提取的特征",{"2":{"12":1}}],["匹配和分类三个阶段",{"2":{"10":1}}],["执行过程分特征提取",{"2":{"10":1}}],["在分割任务中",{"2":{"191":1}}],["在语义分割而言",{"2":{"188":1}}],["在语义分割和部件分割任务的实验中",{"2":{"162":1}}],["在构建失败后清除辅助文件",{"2":{"182":1}}],["在编译生成的",{"2":{"174":1}}],["在编译链中定义的命令出现在了vscode右侧的工具栏中",{"2":{"171":1}}],["在编辑器页面上端进行编译链选择",{"2":{"174":1}}],["在检测任何依赖项中的文件更改",{"2":{"171":1}}],["在打开方式中选择",{"2":{"164":1}}],["在将视觉概念与文本描述联系起来方面显示出了很好的结果",{"2":{"158":1}}],["在生成与类别无关的图像片段方面表现出了优异的性能",{"2":{"156":1}}],["在线聚类",{"2":{"150":1}}],["在线性探测和微调上分别高出2",{"2":{"143":1}}],["在开放词汇语义分割任务中显著提升了clip的性能",{"2":{"153":1}}],["在开放词汇语义分割中",{"2":{"147":1}}],["在开放集场景下也存在局限性",{"2":{"105":1}}],["在这项工作中",{"2":{"147":1}}],["在这种情况下",{"2":{"10":1}}],["在识别存在的对象方面表现出色",{"2":{"147":1}}],["在具有挑战性的cityscapes数据集上",{"2":{"142":1}}],["在coco和vg图像上训练时性能进一步提升",{"2":{"160":1}}],["在coco",{"2":{"142":1,"144":1}}],["在imagenet上进行300个epoch的预训练",{"2":{"142":1}}],["在定性结果中",{"2":{"141":1}}],["在stage",{"2":{"141":1}}],["在scribblesup数据集上采用不同质量涂鸦标注的实验表明",{"2":{"114":1}}],["在深层",{"2":{"141":1}}],["在浅层",{"2":{"141":1}}],["在特征提取方面展现出显著成效",{"2":{"138":1}}],["在特征提取阶段",{"2":{"12":1}}],["在特定领域",{"2":{"135":1}}],["在提供多个提示点时",{"2":{"135":1}}],["在提示调优过程中",{"2":{"88":1}}],["在图像分割方面",{"2":{"135":1}}],["在nlp中",{"2":{"135":1}}],["在nyuv2和pascal",{"2":{"110":1}}],["在原型提取时",{"2":{"132":1}}],["在训练时",{"2":{"159":1}}],["在训练过程中",{"2":{"159":1}}],["在训练迭代中",{"2":{"130":1}}],["在训练早期所有阈值都会快速趋近相似值",{"2":{"36":1}}],["在比较中进行了考虑",{"2":{"119":1}}],["在7个跨域数据集上",{"2":{"111":1}}],["在采样的objects365上创建伪实例掩码",{"2":{"110":1}}],["在细化标签集上",{"2":{"110":1}}],["在聚合openimages和objects365时",{"2":{"110":1}}],["在wilddash2基准测试中",{"2":{"110":1}}],["在评估语义分割性能时",{"2":{"109":1}}],["在零样本数据集上创建伪语义标签",{"2":{"109":1}}],["在零样本任务上表现出色",{"2":{"101":1}}],["在8个零样本数据集",{"2":{"109":1}}],["在自动驾驶等现实世界视觉任务需求日益增长的驱动下",{"2":{"158":1}}],["在自动驾驶",{"2":{"105":1,"139":1}}],["在自然语言处理中表现出色",{"2":{"87":1}}],["在自然语言处理领域取得成功",{"2":{"49":1}}],["在标准数据集微调后",{"2":{"104":1}}],["在对语义相近的类别进行分类和分割时存在困难",{"2":{"101":1}}],["在解码器中引入类别早期拒绝方案",{"2":{"101":1}}],["在解码高层特征时",{"2":{"66":1}}],["在不增加额外数据的情况下",{"2":{"99":1}}],["在不同比例标记的ade20k图像上",{"2":{"142":1}}],["在不同质量涂鸦的实验中表现出卓越的鲁棒性",{"2":{"124":1}}],["在不同训练尺寸和分割比例下进行实验",{"2":{"35":1}}],["在不同分割比例下与其他最先进方法比较",{"2":{"35":1}}],["在少样本分割任务中",{"2":{"99":1}}],["在单样本元训练过程中",{"2":{"98":1}}],["在单样本设置下",{"2":{"98":1}}],["在单样本设置下比cyctr高约3",{"2":{"97":1}}],["在处理前景与背景相似性问题上表现更好",{"2":{"97":1}}],["在支持集和查询集中前景对象姿态",{"2":{"97":1}}],["在该数据集的复杂场景下",{"2":{"97":1}}],["在传统的支持",{"2":{"96":1}}],["在区分前景和背景方面",{"2":{"94":1}}],["在医疗图像理解",{"2":{"94":1}}],["在保留模态内独特空间信息的同时",{"2":{"85":1}}],["在两个基准测试中",{"2":{"81":1}}],["在验证集上miou达到45",{"2":{"79":1}}],["在",{"2":{"75":1,"93":1,"138":2}}],["在使用一致的特征编码器设置下",{"2":{"72":1}}],["在变压器编码器的最后l个块中进行提示增强",{"2":{"72":1}}],["在没有spt和pmg持续增强其类别感知能力的情况下",{"2":{"72":1}}],["在白天场景下从50个不同城市采集",{"2":{"70":1}}],["在各种夜间分割任务的实验中",{"2":{"70":1}}],["在多个语义分割数据集",{"2":{"141":1}}],["在多个零样本深度数据集上创建伪语义标签",{"2":{"110":1}}],["在多个数据集",{"2":{"101":1}}],["在多个数据集和三种泛化设置下进行实验",{"2":{"49":1}}],["在多个下游多模态图像分割任务",{"2":{"88":1}}],["在多个任务和基准上取得新的最优性能",{"2":{"72":1}}],["在多个指标上取得了最佳性能",{"2":{"69":1}}],["在第一",{"2":{"67":1}}],["在学习解相关表示时存在问题",{"2":{"66":1}}],["在眼底和前列腺的泛化性基准测试中",{"2":{"63":1}}],["在open",{"2":{"58":1}}],["在ade20k数据集和开放词汇语义分割任务上研究",{"2":{"161":1}}],["在ade20k数据集上",{"2":{"142":1}}],["在ade20k",{"2":{"60":1}}],["在a",{"2":{"58":1,"59":1}}],["在pc",{"2":{"58":1}}],["在pascal",{"2":{"38":1,"72":4,"81":1,"141":1}}],["在许多领域取得成功",{"2":{"56":1}}],["在内的常见基准测试上进行了广泛的实验",{"2":{"54":1}}],["在包括ade20k",{"2":{"54":1}}],["在性能和速度方面表现出色",{"2":{"51":1}}],["在重新标注的pascal",{"2":{"51":1}}],["在全景分割和实体分割实验中",{"2":{"51":1}}],["在高分辨率图像上运行速度更快",{"2":{"51":1}}],["在big数据集上对比crm",{"2":{"51":1}}],["在实验中从连续范围中选择4个缩放比例进行细化",{"2":{"51":1}}],["在22",{"2":{"51":1}}],["在神经网络中用于表示对象或场景",{"2":{"51":1}}],["在骨干网络层间嵌入该机制",{"2":{"49":1}}],["在本文中",{"2":{"49":1,"156":1}}],["在本研究中",{"2":{"9":1}}],["在5",{"2":{"45":1}}],["在15个数据集上进行评估",{"2":{"110":1}}],["在1",{"2":{"45":2}}],["在元测试阶段",{"2":{"44":1,"47":1}}],["在四个流行数据集上的大量实验表明",{"2":{"41":1}}],["在获得领域无关特征后",{"2":{"41":1}}],["在321×321训练尺寸下",{"2":{"35":1}}],["在半监督语义分割领域",{"2":{"33":1}}],["在仅含",{"2":{"31":1}}],["在主流分割基准测试中表现优异",{"2":{"31":1}}],["在损失函数引入分割正则化等",{"2":{"21":1}}],["在极端监督条件下也展现出良好的鲁棒性",{"2":{"19":1}}],["在随机游走模块的概率转移矩阵特征空间",{"2":{"19":1}}],["在目标类别存在显著类内差异时",{"2":{"12":1}}],["在前馈过程中保留更细粒度的特征",{"2":{"12":1}}],["在fss中得到应用",{"2":{"11":1}}],["我们可以发现",{"2":{"193":1}}],["我们假设模型的分类数量为",{"2":{"188":1}}],["我们先要对模型有一个大的了解",{"2":{"188":1}}],["我们证明了use框架优于最先进的开放词汇切分方法",{"2":{"156":1}}],["我们将在segment",{"2":{"135":1}}],["我们将大语言模型知识划分为对象先验",{"2":{"54":1}}],["我们成功构建了迄今为止最大的图像分割数据集",{"2":{"135":1}}],["我们推出了segment",{"2":{"135":1}}],["我们还开发了距离熵损失函数",{"2":{"114":1}}],["我们还提出了一种测试时自我微调",{"2":{"41":1}}],["我们特别设计了定位校正模块",{"2":{"114":1}}],["我们通过语言模型生成短文本描述来表征类别语义",{"2":{"104":1}}],["我们通过定量的性能评估和可视化结果",{"2":{"51":1}}],["我们训练的模型在",{"2":{"104":1}}],["我们在多个任务中测试了该模型",{"2":{"135":1}}],["我们在多个开放词汇语义分割数据集上进行了实验",{"2":{"101":1}}],["我们在解码器中引入了类别早期拒绝方案",{"2":{"101":1}}],["我们在pascal",{"2":{"9":1}}],["我们研发了分组提示调优框架",{"2":{"85":1}}],["我们设计了一种使用可学习类名来获取一般语义概念的在线聚类算法",{"2":{"147":1}}],["我们设计了一种自适应掩码策略",{"2":{"75":1}}],["我们设计了多层次原型生成与交互模块",{"2":{"93":1}}],["我们设计了两种创新的标签传播策略来提升未标注数据的使用效率",{"2":{"31":1}}],["我们引入了通用段嵌入",{"2":{"156":1}}],["我们引入了",{"2":{"70":1}}],["我们开发了系统的自监督预训练框架",{"2":{"138":1}}],["我们开发了一种自适应深度白化变换",{"2":{"63":1}}],["我们开发了区域传播策略",{"2":{"31":1}}],["我们的框架相比其他主流自监督预训练方法展现出竞争优势",{"2":{"138":1}}],["我们的方法在",{"2":{"127":1}}],["我们的方法在性能表现和鲁棒性方面均优于现有所有方法",{"2":{"114":1}}],["我们的方法甚至可以在没有目标候选类的情况下预测开放词汇分割结果",{"2":{"54":1}}],["我们的源代码可以通过https",{"2":{"101":1}}],["我们的逐渐融合解码器采用自上而下的结构",{"2":{"101":1}}],["我们的模型比以前的最先进的",{"2":{"54":1}}],["我们首先在领域泛化语义分割",{"2":{"49":1}}],["我们利用双重超相关构建",{"2":{"41":1}}],["我们并不完全依赖支持图像",{"2":{"41":1}}],["我们提出相关本质特征增强网络",{"2":{"93":1}}],["我们提出将图像内容自适应划分为确定区域",{"2":{"75":1}}],["我们提出基于通道解耦深度特征的查询机制",{"2":{"63":1}}],["我们提出的单阶段",{"2":{"75":1}}],["我们提出的",{"2":{"70":1}}],["我们提出的方法利用大语言模型中嵌入的大量知识进行开放词汇语义分割",{"2":{"54":1}}],["我们提出的动机是",{"2":{"49":1}}],["我们提出了",{"2":{"51":1}}],["我们提出了一种新的方法",{"2":{"147":1}}],["我们提出了一种高效扩展多领域语义分割数据集的方法",{"2":{"104":1}}],["我们提出了一种高效的微调方法",{"2":{"49":1}}],["我们提出了一种基于transformer的自适应原型匹配网络",{"2":{"9":1}}],["我们提出了corrmatch",{"2":{"38":1}}],["我们提出像素传播策略",{"2":{"31":1}}],["我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性",{"2":{"127":1}}],["我们发现",{"2":{"31":1}}],["强化了约束效果",{"2":{"9":1}}],["建立更为稳定的匹配关系",{"2":{"9":1}}],["翻译",{"0":{"9":1,"19":1,"31":1,"41":1,"54":1,"63":1,"75":1,"85":1,"93":1,"104":1,"114":1,"127":1,"138":1,"147":1,"156":1},"2":{"49":1,"51":1,"70":1,"72":1,"101":1,"135":1}}],["lln",{"2":{"159":1}}],["llm先验引导的分割",{"2":{"57":1}}],["llm先验提取",{"2":{"57":1}}],["llm",{"2":{"53":2,"57":2,"59":1,"60":1}}],["llms",{"2":{"53":3}}],["llmformer由三个主要部分组成",{"2":{"57":1}}],["llmformer",{"0":{"52":1},"1":{"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1}}],["lseg用语言嵌入监督类别标签",{"2":{"106":1}}],["lt",{"2":{"84":1}}],["l",{"2":{"72":1,"101":1,"178":1,"179":1,"182":1}}],["l骨干的san分别为2",{"2":{"58":1}}],["lastused",{"2":{"170":1,"171":2,"182":1}}],["latex配置代码解读",{"0":{"171":1}}],["latex配置代码展示",{"0":{"170":1}}],["latexmk",{"2":{"170":5,"171":6,"182":5}}],["latex环境的代码配置",{"0":{"169":1},"1":{"170":1,"171":1}}],["latex的支持插件",{"0":{"167":1}}],["latex",{"0":{"167":1},"2":{"163":1,"164":1,"167":1,"170":22,"171":23,"173":1,"174":1,"178":6,"179":8,"182":26}}],["lambert提出统一分类法合并多领域数据集",{"2":{"106":1}}],["language",{"2":{"53":3,"84":1,"101":1,"103":1,"146":1,"166":1}}],["languagemodel",{"0":{"52":1},"1":{"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1}}],["layer",{"2":{"49":2,"101":1,"194":2}}],["lab",{"2":{"73":1}}],["laboratory",{"2":{"39":1,"70":1}}],["labeling",{"2":{"103":1}}],["labeled",{"2":{"40":1,"92":1,"113":1,"126":1,"137":1}}],["label",{"0":{"29":1,"112":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1},"2":{"30":1,"72":1,"103":2,"113":1}}],["labels",{"0":{"145":1},"1":{"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1},"2":{"18":1,"30":1,"74":2,"103":3,"113":3,"146":1}}],["largest",{"2":{"135":1}}],["large",{"0":{"52":1},"1":{"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1},"2":{"18":1,"53":1,"72":1,"103":1,"137":1,"146":1,"155":1}}],["less",{"2":{"103":1}}],["learnable",{"2":{"146":1}}],["learn",{"2":{"62":2,"70":1,"84":1}}],["learning",{"0":{"61":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"18":1,"62":2,"84":1,"137":2}}],["leading",{"2":{"74":1}}],["lead",{"2":{"40":1}}],["leveraging",{"2":{"49":1,"146":1}}],["leverages",{"2":{"126":1}}],["leverage",{"2":{"30":1,"62":1}}],["levels",{"2":{"51":1,"101":1}}],["level",{"2":{"18":1,"74":2,"92":2,"101":3,"113":1,"137":1,"146":3}}],["loner",{"2":{"173":1}}],["long",{"2":{"62":1}}],["lot",{"2":{"170":1,"171":1,"182":1}}],["lof",{"2":{"170":1,"171":1,"182":1}}],["log",{"2":{"167":1,"170":1,"171":1,"182":1}}],["located",{"2":{"146":1}}],["localization",{"2":{"113":1,"117":1}}],["local",{"2":{"8":1,"74":1,"92":1,"101":1,"113":1}}],["loop",{"2":{"135":1}}],["lorm",{"2":{"117":1,"124":1}}],["low",{"2":{"51":1}}],["loss",{"2":{"18":3,"113":1,"117":1,"130":2}}],["lipsum",{"2":{"173":2}}],["liuliang1999",{"2":{"164":1}}],["live",{"0":{"164":1},"2":{"164":5}}],["like",{"2":{"146":2,"220":1}}],["licensed",{"2":{"135":1}}],["lizes",{"2":{"113":1}}],["limitations",{"2":{"126":1}}],["limit",{"2":{"92":1}}],["limited",{"2":{"8":1,"92":1,"103":1}}],["lies",{"2":{"84":1,"155":1}}],["linguistic",{"2":{"72":1}}],["linear",{"2":{"101":1,"137":1}}],["line",{"2":{"72":1,"170":3,"171":3,"178":1,"179":2,"182":5,"220":1}}],["linked",{"2":{"49":1}}],["light",{"2":{"70":2}}],["lighting",{"2":{"70":5}}],["list",{"2":{"5":1,"164":1,"222":1}}],["error",{"2":{"170":4,"171":4,"182":4}}],["era",{"2":{"72":1}}],["e7",{"2":{"119":1}}],["e2",{"2":{"119":1}}],["european",{"2":{"103":1}}],["et",{"2":{"103":2,"104":2}}],["equal",{"2":{"103":1}}],["elaborated",{"2":{"72":1}}],["either",{"2":{"62":1}}],["eigenspace",{"2":{"18":3}}],["e",{"2":{"62":1,"70":1,"72":1,"74":2,"137":1,"155":1}}],["everingham",{"2":{"103":1,"104":1}}],["even",{"2":{"53":1,"135":1}}],["evaluate",{"2":{"135":1}}],["evaluation",{"2":{"51":1}}],["eva02和dinov2等五种不同训练策略和数据集的vfms进行评估",{"2":{"49":1}}],["ec",{"2":{"51":1}}],["estimation",{"2":{"103":1}}],["establish",{"2":{"8":1}}],["especially",{"2":{"40":1,"72":1}}],["early",{"2":{"101":4}}],["each",{"2":{"40":1,"49":2,"72":1,"84":1,"103":2}}],["easily",{"2":{"30":1,"40":1}}],["effort",{"2":{"103":1}}],["effortlessly",{"2":{"72":1}}],["efficacy",{"2":{"101":1}}],["efficient",{"2":{"72":1,"92":1,"135":1}}],["efficiently",{"2":{"49":2,"155":1}}],["efficiency",{"2":{"30":1}}],["effective",{"2":{"51":1,"84":1,"126":1}}],["effectiveness",{"2":{"8":1,"40":1,"90":1,"137":1}}],["embedding",{"2":{"103":3,"104":1,"155":2,"157":1}}],["embeddings",{"0":{"102":1,"154":1},"1":{"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"103":1}}],["embedded",{"2":{"53":1}}],["embeds",{"2":{"18":1}}],["ema",{"2":{"34":1,"141":1}}],["employs",{"2":{"101":2}}],["employ",{"2":{"30":1}}],["emerging",{"2":{"18":1}}],["enumerate",{"2":{"194":1}}],["engine",{"2":{"135":1}}],["enabling",{"2":{"126":1}}],["enabled",{"2":{"170":1,"171":1,"182":1}}],["enables",{"2":{"70":1,"103":3,"155":1}}],["enable",{"2":{"30":1,"135":1}}],["enough",{"2":{"74":1}}],["encoder",{"0":{"100":1},"1":{"101":1},"2":{"72":1,"101":4,"137":1,"141":1,"146":1}}],["encoders",{"2":{"72":2}}],["encourage",{"2":{"18":1}}],["encouraging",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"2":{"22":1}}],["end",{"2":{"62":2,"173":2,"194":2}}],["entire",{"2":{"126":1}}],["entity",{"2":{"51":2}}],["entangled",{"2":{"70":1}}],["entropy",{"2":{"18":1,"113":1,"117":1,"130":1}}],["enhance",{"2":{"8":1,"30":1,"40":1,"72":1}}],["enhancement",{"0":{"71":1,"91":1},"1":{"72":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"8":1,"12":1,"92":1,"93":1,"96":1}}],["exe文件所在位置",{"2":{"179":1}}],["exe",{"2":{"178":2,"179":3,"182":2}}],["excelling",{"2":{"146":1}}],["excluded",{"2":{"92":1}}],["existing",{"2":{"62":1,"70":1,"84":1,"101":2,"113":1,"126":1}}],["external",{"2":{"178":5,"179":10,"182":7}}],["extensions",{"2":{"219":1,"222":1}}],["extension",{"0":{"219":1},"1":{"220":1,"221":1,"222":1}}],["extensive",{"2":{"8":1,"40":1,"49":1,"53":1,"62":1,"70":1,"74":1,"84":1}}],["extend",{"2":{"92":1}}],["extremely",{"2":{"137":1}}],["extreme",{"2":{"18":1}}],["extracting",{"2":{"137":1}}],["extraction",{"2":{"57":2,"70":1}}],["extract",{"2":{"92":2}}],["extracted",{"2":{"30":1}}],["extra",{"2":{"18":1,"49":1,"51":1,"92":1}}],["export",{"2":{"220":1}}],["experimental",{"2":{"126":1,"155":1}}],["experiment",{"0":{"108":1},"1":{"109":1,"110":1}}],["experiments",{"0":{"13":1,"15":1,"27":1,"36":1,"59":1,"68":1,"80":1,"98":1,"121":1,"132":1,"143":1,"152":1,"161":1},"1":{"14":1,"15":1,"122":1,"123":1},"2":{"8":1,"18":1,"40":1,"49":1,"53":1,"62":1,"70":1,"72":2,"74":1,"84":1,"101":1,"113":1,"135":1}}],["expensive",{"2":{"103":1,"137":1}}],["explicit",{"2":{"84":1}}],["explicitly",{"2":{"70":2}}],["explore",{"2":{"40":1,"101":1}}],["exploring",{"2":{"8":1}}],["exploit",{"2":{"40":1,"72":1,"113":1}}],["examples",{"0":{"0":1,"219":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"220":1,"221":1,"222":1}}],["www",{"2":{"164":1}}],["w",{"2":{"143":1}}],["w48和segformer两种网络架构进行实验",{"2":{"109":1}}],["wsss使用如边界框",{"2":{"76":1}}],["wsss的优势与挑战",{"2":{"76":1}}],["wsss",{"2":{"74":3,"75":3,"76":1,"114":1,"124":1}}],["wout=",{"2":{"193":1}}],["would",{"2":{"72":1}}],["work",{"2":{"70":1,"146":1}}],["workshop插件",{"2":{"167":1}}],["workshop",{"2":{"167":1,"170":13,"171":14,"178":6,"179":8,"182":19}}],["workshop安装",{"0":{"167":1}}],["works",{"2":{"0":1,"18":1,"30":1,"40":1,"53":1,"70":1,"137":1}}],["world",{"2":{"53":1,"173":1}}],["win−1",{"2":{"193":1}}],["windows",{"2":{"164":2}}],["wiki",{"2":{"171":1}}],["widely",{"2":{"137":1}}],["will",{"2":{"135":1}}],["wilddash2评估",{"2":{"110":1}}],["wilddash2",{"2":{"109":1}}],["wilddash1",{"2":{"109":1}}],["wise",{"2":{"62":2}}],["withspt",{"2":{"72":1}}],["without",{"0":{"145":1},"1":{"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1},"2":{"49":1,"53":1,"70":1,"92":1,"101":1,"146":1}}],["within",{"2":{"18":1,"49":3,"53":1,"62":1,"72":1}}],["with",{"0":{"13":1,"14":1,"26":1,"35":1,"45":1,"58":1,"67":1,"70":1,"79":1,"82":1,"97":1,"102":1,"108":1,"112":1,"118":1,"125":1,"131":1,"142":1,"151":1,"160":1},"1":{"14":1,"15":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":2,"110":2,"111":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":2,"120":2,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"18":3,"30":3,"40":2,"49":2,"51":1,"62":3,"70":2,"72":2,"74":4,"101":3,"103":4,"113":2,"135":2,"137":4,"146":1,"155":1,"220":1}}],["wuhan",{"2":{"61":1}}],["w1oves",{"2":{"49":3,"70":2}}],["what",{"2":{"135":3,"146":1}}],["when",{"2":{"101":1,"113":1}}],["where",{"2":{"40":1,"146":2}}],["whu",{"2":{"89":1}}],["whitening",{"2":{"62":1}}],["while",{"2":{"53":1,"62":1,"70":1,"84":1,"126":1}}],["which",{"2":{"18":2,"30":1,"40":1,"53":1,"72":1,"84":2,"92":1,"101":3,"103":2,"113":3,"126":1,"137":1,"146":2}}],["warning",{"2":{"170":1,"171":1,"182":1,"221":6}}],["walk",{"2":{"18":1}}],["walkmodule",{"2":{"18":1}}],["way",{"2":{"18":1,"62":1}}],["weights",{"2":{"113":1}}],["weak",{"2":{"72":1,"84":1,"103":1}}],["weakly",{"0":{"73":1,"112":1},"1":{"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1},"2":{"18":1,"74":2,"75":1,"113":1,"114":1}}],["well",{"2":{"51":1,"62":1}}],["we",{"2":{"8":1,"18":2,"30":5,"40":4,"49":2,"51":2,"53":1,"62":1,"70":2,"74":3,"84":1,"92":3,"101":2,"103":5,"113":3,"135":4,"137":1,"146":2,"155":2}}],["nn",{"2":{"193":1,"194":36}}],["nij表示像素点属于分类i被预测为分类j的像素点数量",{"2":{"191":1}}],["night进行补充实验",{"2":{"70":1}}],["nightcity",{"2":{"70":3}}],["nightlab等",{"2":{"70":1}}],["night",{"0":{"70":1},"2":{"70":6}}],["n",{"2":{"188":4}}],["nms",{"2":{"159":1}}],["nlp",{"2":{"135":2}}],["numpy",{"2":{"194":1}}],["numerous",{"2":{"135":1}}],["number",{"2":{"8":1,"90":1,"137":1}}],["nyud",{"2":{"103":1,"104":1}}],["nyudv2",{"2":{"89":1}}],["np",{"2":{"72":1,"194":1}}],["nw",{"2":{"43":1}}],["nkiari",{"2":{"29":1}}],["naive",{"2":{"103":1}}],["name",{"2":{"170":10,"171":10,"182":10}}],["names",{"2":{"146":1}}],["namely",{"2":{"49":1}}],["named",{"2":{"18":2,"101":1}}],["nanjing",{"2":{"39":1,"125":1}}],["nankai",{"2":{"29":1}}],["ncl",{"2":{"26":1}}],["never",{"2":{"170":1,"171":3,"182":1}}],["nevertheless",{"2":{"137":1}}],["need",{"2":{"137":2}}],["needs",{"2":{"51":1}}],["new",{"2":{"72":1,"135":2}}],["negligible",{"2":{"70":1}}],["neglect",{"2":{"18":1}}],["net及其变体占主导",{"2":{"65":1}}],["networks",{"2":{"39":1}}],["network",{"0":{"6":1,"91":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"8":1,"12":1,"18":7,"40":1,"44":1,"62":1,"70":1,"92":1,"93":1,"96":1,"113":2,"114":1}}],["nerf",{"2":{"51":1}}],["next",{"2":{"49":1}}],["neural",{"2":{"18":3}}],["now",{"2":{"155":1,"166":1}}],["north",{"2":{"154":1}}],["noisy",{"2":{"103":1}}],["noise",{"2":{"8":1}}],["no",{"2":{"101":1}}],["notably",{"2":{"62":1,"92":1}}],["not",{"2":{"30":1,"74":1,"84":1,"103":1,"155":1}}],["nontrivial",{"2":{"18":1}}],["novel",{"2":{"8":1,"30":1,"53":2,"70":1,"72":1,"92":1,"146":1}}],["摘要",{"0":{"8":1,"18":1,"30":1,"40":1,"53":1,"62":1,"74":1,"84":1,"92":1,"103":1,"113":1,"126":1,"137":1,"146":1,"155":1},"2":{"49":1,"51":1,"70":1,"72":1,"101":1,"135":1}}],["💯",{"0":{"7":1},"2":{"185":1}}],["澳门大学",{"0":{"7":1}}],["青海师范大学",{"0":{"7":1}}],["南京信息工程大学",{"0":{"7":1}}],["otherdownstream",{"2":{"155":1}}],["other",{"2":{"137":2}}],["os",{"2":{"89":1}}],["optional",{"2":{"193":6}}],["optimally",{"2":{"137":1}}],["optimal",{"2":{"84":1}}],["open",{"0":{"52":1,"100":1,"145":1,"154":1},"1":{"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"101":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"53":2,"101":4,"146":2,"155":4}}],["ov",{"2":{"53":4,"60":1}}],["over",{"2":{"40":1,"103":2,"135":1,"146":1}}],["overfitting",{"2":{"40":2}}],["overlooking",{"2":{"40":1}}],["overlook",{"2":{"30":1}}],["omron",{"2":{"51":1}}],["omitted",{"2":{"30":1}}],["observation",{"2":{"70":1}}],["observe",{"2":{"30":1}}],["object",{"2":{"53":1,"72":1,"74":2}}],["objects",{"2":{"51":1,"53":1,"72":2,"146":2}}],["obtaining",{"2":{"40":1}}],["org",{"2":{"164":1}}],["oriented",{"2":{"70":1}}],["or",{"2":{"40":2,"51":1,"62":1,"135":1,"193":5}}],["ohem",{"2":{"35":1}}],["ously",{"2":{"51":1}}],["our",{"2":{"8":1,"30":1,"51":2,"53":3,"70":1,"74":1,"84":1,"101":3,"103":1,"113":1,"126":1,"135":1,"137":2}}],["output",{"2":{"193":2,"194":6}}],["outperform",{"2":{"113":1}}],["outperforming",{"2":{"62":1}}],["outperforms",{"2":{"49":1,"53":1,"70":1,"74":1,"155":1}}],["outdir",{"2":{"170":1,"171":1,"182":1}}],["outdir=",{"2":{"170":1,"171":1,"182":1}}],["out",{"2":{"5":1,"30":1,"170":1,"171":1,"178":1,"179":1,"182":2,"193":2,"194":13,"222":1}}],["onbuilt",{"2":{"171":1}}],["onsave",{"2":{"171":1}}],["onfilechange",{"2":{"171":1}}],["onfailed",{"2":{"170":1,"171":2,"182":1}}],["online",{"2":{"146":1}}],["only",{"2":{"30":2,"40":1,"72":1,"74":1,"84":1,"155":1}}],["ones",{"2":{"40":1,"51":1,"113":1}}],["one",{"2":{"18":1,"40":1,"84":1,"95":1}}],["on",{"2":{"8":1,"18":2,"30":2,"40":5,"49":1,"51":1,"53":3,"62":2,"70":3,"72":4,"74":3,"84":2,"92":1,"101":3,"103":7,"113":1,"126":1,"135":2,"137":3,"146":1,"155":1,"179":1,"182":1}}],["ofour",{"2":{"101":1}}],["ofdecoder",{"2":{"101":1}}],["ofplain",{"2":{"101":1}}],["ofhumanbeings",{"2":{"72":1}}],["of78",{"2":{"49":1}}],["oftext",{"2":{"155":1}}],["often",{"2":{"18":1,"126":1,"135":1}}],["ofthe",{"2":{"30":1}}],["oflocations",{"2":{"30":1}}],["ofcorrelation",{"2":{"30":1}}],["of",{"0":{"13":1,"14":1,"83":2},"1":{"14":1,"15":1},"2":{"0":2,"5":1,"8":4,"18":5,"30":2,"39":2,"40":5,"49":4,"53":2,"62":3,"70":6,"72":5,"73":1,"74":2,"84":5,"90":3,"91":3,"92":5,"101":3,"103":10,"113":3,"126":2,"135":1,"137":8,"146":1,"155":3,"219":1,"222":1}}],["rcnn",{"2":{"189":1}}],["r",{"2":{"164":1,"178":1,"179":1,"182":1}}],["rmit",{"2":{"125":1}}],["rifenet是一种有效的少样本语义分割模型",{"2":{"99":1}}],["rifenet性能提升显著",{"2":{"99":1}}],["rifenet在基线基础上提高3",{"2":{"98":1}}],["rifenet在保持前景语义一致性方面有显著改进",{"2":{"97":1}}],["rifenet在单样本设置的几乎所有分割中仍比当前最佳的dcama高出0",{"2":{"97":1}}],["rifenet在大多数实验场景下优于最佳方法",{"2":{"97":1}}],["rifenet在pascal",{"2":{"96":1,"99":1}}],["rifenet的损失函数是主损失和自监督损失的加权和",{"2":{"96":1}}],["rifenet由三个共享主干网络的分支组成",{"2":{"96":1}}],["rifenet",{"2":{"92":2,"93":2,"94":1,"96":1,"99":1}}],["richer",{"2":{"53":1}}],["rgb",{"2":{"84":1,"85":1,"88":3,"89":1,"188":1}}],["rdwt进一步使dsc提高了1",{"2":{"68":1}}],["rdwt",{"2":{"66":1,"68":1,"69":1}}],["run",{"2":{"170":2,"171":2,"182":2}}],["runner",{"2":{"62":1}}],["runtime",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1,"5":1}}],["rtd",{"2":{"43":1}}],["row",{"2":{"90":2}}],["role",{"2":{"30":1}}],["robustness",{"2":{"18":1,"113":1}}],["robust",{"2":{"8":1,"49":1,"74":1,"92":1,"103":1}}],["randn",{"2":{"194":1}}],["random",{"2":{"18":2}}],["ranking",{"2":{"155":1}}],["range",{"2":{"62":1,"155":1,"194":3}}],["ray",{"2":{"45":1}}],["rawks",{"2":{"26":1}}],["reuse",{"2":{"178":1,"179":1,"182":1}}],["reduction",{"2":{"113":1}}],["reduces",{"2":{"113":1}}],["reduce",{"2":{"18":1}}],["rejects",{"2":{"101":1}}],["rejection",{"2":{"101":3}}],["return",{"2":{"194":1,"220":2}}],["retinanet",{"2":{"189":1}}],["retaining",{"2":{"84":1}}],["retrains",{"2":{"18":1}}],["remote",{"2":{"72":1}}],["remarkably",{"2":{"49":1}}],["re",{"2":{"70":1}}],["ref上的",{"2":{"182":1}}],["ref",{"2":{"178":1,"179":3,"182":1}}],["reflectance",{"2":{"70":1}}],["refinenet",{"2":{"189":1}}],["refinement",{"2":{"51":4}}],["refinements",{"2":{"49":1}}],["refines",{"2":{"49":1}}],["reaching",{"2":{"62":1}}],["real",{"2":{"49":1,"53":1}}],["recipe",{"2":{"170":1,"171":1,"182":1}}],["recipes中的第一条编译链",{"2":{"171":1}}],["recipes中内容",{"2":{"171":1}}],["recipes",{"2":{"170":1,"171":3,"182":1}}],["rectification",{"2":{"113":1,"117":1}}],["recognition",{"2":{"146":1}}],["recognizing",{"2":{"146":1}}],["recognizes",{"2":{"70":1}}],["recognize",{"2":{"53":1,"70":1}}],["recover",{"2":{"74":1}}],["reconstruct",{"2":{"51":1}}],["recently",{"2":{"137":1}}],["recent",{"2":{"53":2,"84":1,"155":1}}],["receive",{"2":{"30":1}}],["reinforcement",{"0":{"73":1},"1":{"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1}}],["rein以更少可训练参数显著增强vfms的泛化能力",{"2":{"49":1}}],["rein便在cityscapes数据集上达到了78",{"2":{"49":1}}],["rein显著超越了现有的最先进方法",{"2":{"49":1}}],["rein在微调vfm时",{"2":{"49":1}}],["rein能够在单张图像中为不同的类别生成多样化的细化结果",{"2":{"49":1}}],["rein方法依赖于一组可训练的标记",{"2":{"49":1}}],["rein",{"2":{"49":12}}],["replace",{"2":{"103":1}}],["representation",{"2":{"62":3,"84":1}}],["representations",{"2":{"18":1,"84":1,"113":1,"137":1}}],["repeatedly",{"2":{"40":1}}],["resources",{"2":{"178":1,"179":1,"182":1}}],["resolu",{"2":{"51":1}}],["resolution",{"0":{"50":1},"2":{"51":4,"137":1}}],["restart",{"2":{"166":1}}],["rest",{"2":{"62":1}}],["responsible",{"2":{"135":1}}],["responses",{"2":{"62":1}}],["responding",{"2":{"135":1}}],["respecting",{"2":{"135":1}}],["respectively",{"2":{"40":1,"62":1,"103":1}}],["respect",{"2":{"101":1}}],["researchers",{"2":{"126":1}}],["research",{"2":{"51":2,"61":1,"135":1,"145":1,"154":1}}],["resulting",{"2":{"74":1,"101":1}}],["result",{"2":{"40":1}}],["results",{"0":{"1":1},"1":{"2":1,"3":1,"4":1},"2":{"0":1,"53":2,"126":1,"135":1,"146":1}}],["resnet",{"2":{"30":1,"31":1}}],["resnet骨干网络",{"2":{"22":1}}],["regarded",{"2":{"126":1}}],["region",{"2":{"30":1,"113":1}}],["regions",{"2":{"18":1,"70":1,"74":5}}],["regularizer",{"2":{"18":1}}],["requisition",{"2":{"18":1}}],["requires",{"2":{"62":1}}],["require",{"2":{"18":1,"146":1}}],["relu",{"2":{"194":13}}],["release",{"2":{"135":1}}],["releasing",{"2":{"135":1}}],["relevant",{"0":{"91":1},"1":{"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"92":1,"93":1,"96":1}}],["relevance",{"2":{"8":1}}],["reliable",{"2":{"113":1}}],["relied",{"2":{"53":1}}],["related",{"2":{"84":1}}],["relation",{"2":{"53":2,"57":2}}],["relationships",{"2":{"8":1,"30":1}}],["relaxed",{"2":{"62":1}}],["relying",{"2":{"40":1}}],["rely",{"2":{"40":1}}],["=",{"2":{"0":1,"72":1,"80":1,"101":3,"119":2,"120":1,"122":2,"194":8}}],["f",{"2":{"178":4,"179":4,"182":4,"194":1}}],["fdb",{"2":{"170":1,"171":1,"182":1}}],["fls",{"2":{"170":1,"171":1,"182":1}}],["flexible",{"2":{"155":1}}],["flectance",{"2":{"70":1}}],["fg",{"2":{"72":1}}],["fsr通过提高深层的上下文程度有利于语义分割",{"2":{"80":1}}],["fsr时",{"2":{"80":1}}],["fsr分析",{"2":{"80":2}}],["fsr互补",{"2":{"80":1}}],["fsr可以进一步提高伪标签和预测标签的质量",{"2":{"80":1}}],["fsr和cer",{"2":{"80":1}}],["fsr在伪标签和预测标签上都有显著提升",{"2":{"80":1}}],["fsr",{"2":{"79":1,"80":2}}],["fsl",{"2":{"72":2}}],["fss方法在分割过程中只关注前景目标区域",{"2":{"42":1}}],["fss方法patnet通过将特定领域特征转换为领域无关特征来消除领域差距",{"2":{"42":1}}],["fss问题中的有效性",{"2":{"41":1}}],["fss旨在用少量标注样本为新类别生成分割模型",{"2":{"11":1}}],["fss遵循元学习框架",{"2":{"10":1}}],["fss",{"2":{"8":2,"9":1,"10":1,"11":1,"12":1,"16":1,"40":2,"41":1,"42":2,"43":2,"44":1,"45":1,"72":9}}],["f4",{"2":{"68":1}}],["f3",{"2":{"68":1}}],["f2",{"2":{"68":1}}],["f1",{"2":{"68":1}}],["fcn结果",{"0":{"215":1}}],["fcn细节",{"0":{"214":1}}],["fcn基本原理",{"0":{"213":1}}],["fcn模型讲解",{"0":{"194":1}}],["fcn",{"0":{"212":1},"1":{"213":1,"214":1,"215":1},"2":{"51":1,"70":1,"95":1,"189":1,"194":4}}],["false",{"2":{"170":2,"171":2,"182":2}}],["facilitate",{"2":{"155":1}}],["factors",{"2":{"137":1}}],["fact",{"2":{"51":1}}],["far",{"2":{"135":1}}],["fails",{"2":{"113":1}}],["fast",{"2":{"51":1}}],["foster",{"2":{"135":1}}],["foundation",{"0":{"48":1},"1":{"49":1},"2":{"49":1,"84":2,"135":2,"146":1,"155":1}}],["four",{"2":{"40":1}}],["focuses",{"2":{"74":1}}],["focused",{"2":{"62":1}}],["focusing",{"2":{"72":1}}],["focus",{"2":{"18":1,"72":1}}],["forward",{"2":{"178":1,"179":1,"182":1,"194":1}}],["forwards",{"2":{"49":1}}],["foraccuracy",{"2":{"74":1}}],["formance",{"2":{"51":1}}],["force",{"2":{"18":1}}],["foreground",{"2":{"8":1,"40":2,"74":1,"92":3,"113":1}}],["for",{"0":{"6":1,"29":1,"48":1,"50":1,"52":1,"71":1,"73":1,"91":1,"100":1,"136":1,"154":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"49":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"72":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"101":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"0":1,"5":1,"8":1,"18":2,"40":3,"49":4,"51":1,"53":2,"62":1,"70":3,"72":4,"74":3,"84":1,"92":2,"101":3,"113":4,"126":1,"135":2,"137":5,"146":2,"154":1,"166":1,"171":1,"173":2,"194":3,"222":1}}],["fusion",{"2":{"70":1,"101":3}}],["fundamental",{"2":{"137":1}}],["fundus",{"2":{"62":1}}],["functional",{"2":{"194":1}}],["function",{"2":{"18":1}}],["futian",{"2":{"29":1}}],["furthermore",{"2":{"70":1,"84":1,"92":1}}],["further",{"2":{"18":1,"70":1,"74":1,"113":1}}],["fully",{"2":{"18":1,"62":1,"74":1,"113":1,"135":1}}],["full",{"2":{"5":1,"49":2,"84":1,"222":1}}],["framework",{"2":{"70":1,"84":1,"88":1,"137":2,"155":3}}],["freeze",{"2":{"49":1}}],["friendly",{"2":{"18":1}}],["frozen",{"2":{"49":1,"84":1}}],["frontmatter",{"0":{"4":1},"2":{"0":3,"4":1}}],["from",{"0":{"61":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"0":1,"18":3,"30":1,"40":1,"49":1,"51":1,"53":1,"62":2,"92":1,"101":1,"103":1,"126":1,"146":1}}],["figure",{"2":{"90":2}}],["fixed",{"2":{"72":1}}],["filetypes",{"2":{"170":1,"171":1,"182":1}}],["file",{"2":{"170":3,"171":3,"182":3}}],["files",{"2":{"0":1}}],["fill",{"2":{"51":1}}],["first",{"2":{"30":1,"49":1,"171":1}}],["find",{"2":{"135":1}}],["finely",{"2":{"103":1}}],["fine一起为夜间分割提供了更优的基准",{"2":{"70":1}}],["finetuning",{"2":{"40":1,"44":1}}],["fine",{"2":{"18":1,"49":3,"70":5,"72":1,"84":1,"103":1,"137":2}}],["finally",{"2":{"8":1,"18":1}}],["feasible",{"2":{"62":1}}],["fea",{"2":{"51":1}}],["feature",{"0":{"61":1,"73":1,"91":1,"125":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"40":2,"49":1,"51":1,"57":1,"62":1,"72":1,"92":1,"93":1,"96":1,"101":1,"113":1,"126":3,"127":1,"193":2}}],["features",{"2":{"8":2,"40":2,"62":3,"70":2,"92":1,"126":1,"137":2,"194":3,"220":1}}],["fewer",{"0":{"48":1},"1":{"49":1},"2":{"49":2}}],["few",{"0":{"6":1,"39":1,"71":1,"91":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"72":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"8":1,"9":1,"10":1,"11":1,"12":1,"40":3,"44":1,"72":2,"92":1}}],["v",{"2":{"164":1}}],["vgg",{"2":{"194":4}}],["vgg16",{"2":{"194":4}}],["vg",{"2":{"160":1}}],["v2",{"2":{"103":1,"104":1}}],["vector",{"2":{"103":2}}],["vscode的中文环境需要下载插件来进行支持",{"2":{"166":1}}],["vscode下载与安装",{"0":{"165":1}}],["vscode",{"2":{"163":3,"164":1,"165":2,"166":1,"167":1,"171":2,"173":3,"175":1,"179":1,"180":1}}],["vscode配置latex环境",{"0":{"163":1},"1":{"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1,"176":1,"177":1,"178":1,"179":1,"180":1,"181":1,"182":1}}],["vs",{"2":{"90":1,"178":1,"179":2,"182":1}}],["vl",{"2":{"53":2,"54":1}}],["vfm",{"2":{"49":1}}],["vfms的潜力与挑战",{"2":{"49":1}}],["vfms",{"2":{"49":7}}],["viewer",{"2":{"178":4,"179":4,"182":4}}],["view",{"2":{"74":1,"170":3,"171":1,"178":6,"179":10,"182":11}}],["vits",{"2":{"159":1}}],["vit块",{"2":{"141":1}}],["vit将transformer应用于图像识别",{"2":{"140":1}}],["vit语言模型将这些描述编码为向量值的句子嵌入",{"2":{"107":1}}],["vit",{"2":{"57":1,"78":1}}],["vitepress",{"2":{"0":2,"219":1,"220":1}}],["visi",{"2":{"103":1}}],["vision",{"0":{"48":1,"136":1},"1":{"49":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1},"2":{"49":1,"53":2,"84":1,"101":1,"103":1,"135":1,"137":2,"141":1,"146":2,"155":1}}],["visual",{"2":{"53":1,"57":2,"72":1,"160":1,"165":1,"166":1}}],["visualization",{"2":{"51":1}}],["via",{"0":{"29":1,"39":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"62":1}}],["voc上",{"2":{"141":1}}],["voc和cityscapes四个常用数据集的多种语义分割和低样本评估指标上达到了最优性能",{"2":{"144":1}}],["voc和cityscapes",{"2":{"141":1}}],["voc2012网站获取",{"2":{"119":1}}],["voc2012",{"2":{"118":1}}],["voc等数据集上显著优于现有方法",{"2":{"60":1}}],["vocabulary",{"0":{"52":1,"100":1,"145":1,"154":1},"1":{"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"101":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"53":1,"101":3,"146":2,"155":4}}],["voc",{"2":{"26":1,"30":1,"31":1,"33":1,"35":3,"38":1,"45":1,"51":2,"58":2,"74":1,"75":1,"79":2,"81":1,"101":3,"109":1,"126":1,"127":1,"131":1,"133":1,"137":2,"138":2}}],["valued",{"2":{"103":2,"126":1}}],["validation",{"2":{"101":2}}],["varying",{"2":{"70":1,"103":1}}],["variability",{"2":{"92":1}}],["various",{"2":{"49":2,"70":1,"84":2,"155":1}}],["variety",{"2":{"18":1,"137":1}}],["vast",{"2":{"53":1,"155":1}}],["va",{"2":{"15":1}}],["vue",{"2":{"0":1}}],["much",{"2":{"103":1}}],["mul",{"2":{"62":1}}],["multiple",{"2":{"62":1,"101":1,"103":1}}],["multi",{"0":{"82":1,"102":1},"1":{"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1},"2":{"8":1,"74":1,"84":3,"90":1,"92":1,"103":2,"141":1}}],["mfnet",{"2":{"89":1}}],["mca大幅优于gap",{"2":{"80":1}}],["mca",{"2":{"80":1}}],["msg",{"2":{"220":2}}],["ms",{"2":{"74":1,"75":1,"79":2,"101":2}}],["msda",{"2":{"57":1}}],["msla",{"2":{"15":2}}],["mllm增强标注能捕获更细粒度的对象和部件",{"2":{"161":1}}],["mllm",{"2":{"57":1,"159":1}}],["mlp",{"2":{"12":1,"15":2,"57":1}}],["mba提高",{"2":{"51":1}}],["mba",{"2":{"51":1}}],["mt等",{"2":{"33":1}}],["microsoft",{"2":{"178":1,"179":2,"182":1}}],["mirror",{"2":{"164":1}}],["miktex",{"2":{"164":1}}],["minimizing",{"2":{"126":1}}],["mined",{"2":{"62":1}}],["misguide",{"2":{"113":1}}],["misclassified",{"2":{"74":1}}],["might",{"2":{"113":1}}],["million",{"2":{"103":1}}],["millisecond",{"2":{"101":1}}],["mimics",{"2":{"72":1}}],["miou=1ncl∑inii",{"2":{"191":1}}],["miou和2",{"2":{"143":1}}],["miou达到81",{"2":{"132":1}}],["miou的增加趋于饱和",{"2":{"132":1}}],["miou比基线提高了10",{"2":{"132":1}}],["miou精度反而下降",{"2":{"72":1}}],["miou精度随之增加",{"2":{"72":1}}],["miou",{"2":{"30":1,"31":1,"49":1,"72":1,"101":1,"103":1,"104":1,"109":1,"137":1,"138":1,"141":1,"142":3,"143":1,"160":1}}],["mitigates",{"2":{"8":1}}],["message",{"2":{"167":1,"170":2,"171":2,"182":2}}],["mean",{"2":{"191":2}}],["meaningful",{"2":{"155":1}}],["meanwhile",{"2":{"18":1}}],["mevt比之前最好的方法ibot高出2",{"2":{"142":1}}],["mevt比基于transformer的dino方法高出8",{"2":{"142":1}}],["mevt在低分辨率阶段使用全局自注意力进行多尺度信息融合",{"2":{"143":1}}],["mevt在各种监督水平下均优于现有方法",{"2":{"142":1}}],["mevt在各种设置",{"2":{"141":1}}],["mevt在大多数数据集上优于所有基线方法",{"2":{"142":1}}],["mevt在复杂环境中识别小物体的能力优于其他基线方法",{"2":{"141":1}}],["mevt在线性探测中比其他方法提高了+1",{"2":{"141":1}}],["mevt在预训练架构中引入了多尺度解码器",{"2":{"141":1}}],["mevt使用图像级自蒸馏损失",{"2":{"141":1}}],["mevt采用了一种简单而有效的混合注意力策略",{"2":{"141":1}}],["mevt",{"2":{"137":2,"138":2,"141":1}}],["melbourne",{"2":{"125":1}}],["merged",{"2":{"103":1}}],["merging",{"2":{"103":2}}],["medical",{"0":{"61":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"61":1,"62":1,"72":1}}],["metric",{"2":{"62":1}}],["methodology",{"2":{"53":1}}],["method",{"2":{"51":1,"53":1,"74":1,"92":1,"101":2,"103":1,"113":1,"126":2,"146":1}}],["methods",{"2":{"8":1,"18":1,"49":1,"53":1,"62":1,"70":3,"74":1,"84":1,"92":2,"101":1,"103":2,"113":2,"126":1,"137":1,"146":1,"155":1}}],["meta",{"0":{"135":1},"2":{"40":1}}],["mechanism",{"2":{"8":1,"62":1}}],["most",{"2":{"70":1,"72":1,"74":1,"101":2,"103":1,"137":1}}],["mostly",{"2":{"30":1}}],["monash",{"2":{"52":1}}],["mobilenetv2等旧骨干网络",{"2":{"49":1}}],["mobilenetv2和resnet等经典骨干网络",{"2":{"49":1}}],["motivation",{"2":{"49":1}}],["motivated",{"2":{"30":1,"51":1}}],["mode=false",{"2":{"194":5}}],["modeling",{"2":{"30":2,"84":1}}],["models",{"0":{"13":1,"48":1},"1":{"14":1,"15":1,"49":1},"2":{"40":1,"49":2,"53":1,"62":1,"72":2,"84":1,"101":1,"135":2,"146":2,"155":1}}],["model",{"2":{"8":2,"30":1,"51":2,"53":2,"62":1,"84":3,"92":1,"101":1,"103":3,"113":1,"135":10,"146":1,"155":3,"159":1}}],["modality",{"2":{"84":2}}],["modalities",{"2":{"84":2}}],["modal",{"0":{"82":1},"1":{"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1},"2":{"72":1,"84":7,"88":2,"90":1}}],["module",{"2":{"8":4,"12":3,"18":1,"40":1,"92":1,"113":1,"117":1,"194":1}}],["modules",{"2":{"8":1,"53":1}}],["moreover",{"2":{"53":1}}],["more",{"0":{"5":1,"222":1},"2":{"30":1,"40":1,"70":1,"72":1,"171":1}}],["markdown",{"0":{"219":1},"1":{"220":1,"221":1,"222":1},"2":{"219":1,"222":1}}],["margin=1in",{"2":{"173":1}}],["maxpool2d",{"2":{"194":5}}],["macc=1nc1∑iniiti计算每一类分类正确的像素点数和该类的所有像素点数的比例然后求平均",{"2":{"191":1}}],["made",{"2":{"126":1}}],["manually",{"2":{"103":1}}],["many",{"2":{"101":1,"137":1}}],["manifestation",{"2":{"74":1}}],["manchester",{"2":{"73":1}}],["manner",{"2":{"62":1}}],["masking",{"2":{"74":1}}],["mask",{"2":{"72":1,"189":1}}],["masks",{"2":{"30":1,"135":2,"146":2}}],["mae",{"2":{"49":3}}],["may",{"2":{"40":1}}],["mapillary",{"2":{"109":1}}],["map",{"2":{"51":1,"101":5,"193":1}}],["map的潜力",{"2":{"32":1}}],["maps",{"2":{"30":3,"40":1,"49":1,"101":1}}],["matrices",{"2":{"40":1}}],["matri",{"2":{"40":1}}],["matrix",{"2":{"18":1}}],["matching",{"0":{"6":1,"29":1,"39":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"8":2,"12":1,"40":2,"44":2}}],["maketitle",{"2":{"173":1}}],["make",{"2":{"18":2}}],["ma",{"2":{"15":1}}],["main",{"2":{"0":1,"51":2,"84":1,"137":1,"155":1}}],["md```js",{"2":{"220":1}}],["md",{"2":{"0":2,"221":1}}],["io",{"2":{"146":1,"147":1}}],["iou指标下也有显著提升",{"2":{"58":1}}],["iou",{"2":{"51":1,"191":4}}],["iii",{"2":{"137":1,"138":1}}],["ii",{"2":{"137":1,"138":1}}],["ignore",{"2":{"126":1,"137":1}}],["irrelevant",{"2":{"72":1}}],["iaparser",{"2":{"70":4}}],["illumination",{"0":{"70":1},"2":{"70":4}}],["id",{"2":{"187":1}}],["idx",{"2":{"170":1,"171":1,"182":1,"194":2}}],["idd",{"2":{"109":1}}],["ideal",{"2":{"62":1}}],["idr",{"2":{"15":1}}],["i",{"2":{"62":1,"70":1,"137":1,"138":1}}],["imagenet",{"2":{"78":1}}],["image",{"0":{"61":1,"82":1,"154":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"40":1,"49":1,"51":3,"57":1,"62":1,"74":2,"84":2,"101":3,"103":1,"113":1,"126":1,"135":2,"146":2,"155":4}}],["images",{"0":{"50":1},"2":{"30":1,"40":8,"51":2,"62":2,"70":1,"72":1,"103":2,"135":2,"146":1,"155":1}}],["im",{"2":{"40":1}}],["impact",{"2":{"90":1}}],["impeded",{"2":{"70":1}}],["imposes",{"2":{"18":1}}],["important",{"2":{"113":1}}],["import",{"2":{"0":1,"194":3}}],["improvements",{"2":{"103":1,"146":1}}],["improvement",{"2":{"103":1}}],["improve",{"2":{"18":2,"30":1,"92":1}}],["improving",{"2":{"8":1,"84":1}}],["impressive",{"2":{"18":1,"103":2,"135":1,"146":1}}],["ist",{"2":{"170":1,"171":1,"182":1}}],["iso",{"2":{"164":2}}],["isic2018",{"2":{"45":1}}],["issue",{"2":{"40":1,"51":1,"62":1,"74":1}}],["issues",{"2":{"18":1,"84":1,"92":1}}],["is",{"2":{"18":1,"30":1,"40":1,"49":1,"51":2,"62":4,"72":1,"74":1,"84":4,"92":2,"101":2,"103":4,"113":2,"126":2,"135":3,"137":3,"146":2,"155":1,"173":2,"221":10}}],["inplace=true",{"2":{"194":13}}],["input",{"2":{"101":1,"194":6,"220":1,"221":1}}],["init",{"2":{"194":2}}],["initialize",{"2":{"72":1}}],["info",{"2":{"221":4}}],["informed",{"2":{"113":1}}],["information",{"2":{"30":1,"72":1,"74":1,"84":2,"90":1,"92":1,"101":1,"126":1,"137":1}}],["inference",{"2":{"101":1}}],["ind",{"2":{"170":1,"171":1,"182":1}}],["individual",{"2":{"103":1}}],["individuals",{"2":{"72":1}}],["induced",{"2":{"84":1,"88":1}}],["inevitably",{"2":{"72":1}}],["inverse",{"2":{"178":1,"179":1,"182":1}}],["involves",{"2":{"155":1}}],["involving",{"2":{"18":1}}],["invariant",{"2":{"62":1,"70":1}}],["insufficient",{"2":{"70":1}}],["inspired",{"2":{"53":1,"84":1}}],["install",{"2":{"164":1,"166":1,"167":1}}],["instance",{"2":{"103":1,"178":1,"179":1,"182":1}}],["instances",{"2":{"49":1,"92":1}}],["instead",{"2":{"40":1,"70":1,"101":1,"103":1}}],["inconsistent",{"2":{"103":1}}],["including",{"2":{"53":1,"72":1}}],["includes",{"2":{"8":1}}],["increasing",{"2":{"53":1}}],["int",{"2":{"103":1,"193":8}}],["intellisense",{"2":{"170":1,"171":1,"182":1}}],["intelligence",{"2":{"154":1}}],["internal",{"2":{"170":3,"171":1,"182":1}}],["interaction=nonstopmode",{"2":{"170":3,"171":3,"182":3}}],["interaction",{"2":{"92":1}}],["inter",{"2":{"84":1,"92":2}}],["interested",{"2":{"72":1}}],["integrating",{"2":{"84":1}}],["integrate",{"2":{"62":1}}],["integrated",{"2":{"39":1}}],["intrinsic",{"0":{"91":1},"1":{"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"92":3,"93":1,"96":1}}],["introduced",{"2":{"72":1,"84":1}}],["introduces",{"2":{"53":1,"84":1}}],["introduce",{"2":{"49":1,"70":1,"74":1,"101":1,"113":1,"135":1,"137":1,"155":1}}],["intra",{"2":{"40":1,"84":1,"92":1}}],["into",{"2":{"8":1,"18":1,"40":1,"53":1,"62":1,"70":1,"74":1,"101":1,"135":1,"155":3}}],["inherit",{"2":{"84":1}}],["inherits",{"2":{"18":1}}],["inherent",{"2":{"8":2}}],["inn",{"2":{"12":1,"15":2}}],["in",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"2":{"0":1,"8":2,"18":3,"22":1,"30":1,"40":4,"49":2,"51":1,"53":3,"62":3,"70":3,"72":5,"74":1,"84":4,"101":4,"103":3,"113":2,"126":2,"135":1,"137":5,"146":3,"155":4,"193":2,"194":16,"219":1}}],["its",{"0":{"112":1},"1":{"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1},"2":{"18":1,"51":1,"62":1,"135":2}}],["it",{"2":{"0":1,"18":1,"74":1,"103":1,"113":1,"126":2,"135":1,"137":1}}],["cmd",{"2":{"182":1}}],["cmd+鼠标左键单击",{"2":{"171":1}}],["c",{"2":{"174":1}}],["ct照片等",{"2":{"198":1}}],["ctex",{"2":{"173":1}}],["ctrl",{"2":{"171":1,"182":1}}],["cnblogs",{"2":{"164":1}}],["cvlab",{"2":{"146":1,"147":1}}],["cv",{"2":{"72":1}}],["custom",{"0":{"221":1}}],["curate",{"2":{"155":1}}],["current",{"2":{"0":1,"72":2,"103":1}}],["cup",{"2":{"88":1,"90":1}}],["cues",{"2":{"74":1,"113":2}}],["cue",{"2":{"70":1}}],["chinese",{"0":{"83":1},"2":{"91":2,"166":1}}],["china",{"2":{"70":1,"112":1,"125":1}}],["change",{"2":{"167":1}}],["channels=512",{"2":{"194":11}}],["channels=256",{"2":{"194":6}}],["channels=128",{"2":{"194":4}}],["channels=64",{"2":{"194":4}}],["channels=3",{"2":{"194":1}}],["channels",{"2":{"62":1,"193":4}}],["channel",{"2":{"62":4}}],["challenges",{"2":{"146":1}}],["challenge",{"2":{"70":1,"74":1,"84":1,"155":2}}],["challenging",{"2":{"62":1,"103":1}}],["chest",{"2":{"45":1}}],["chenjiayi68",{"2":{"40":1,"41":1}}],["check",{"2":{"5":1,"222":1}}],["crm在低分辨率下能细化出较好的通用掩码",{"2":{"51":1}}],["crm在超高分辨率图像上取得最佳分割结果",{"2":{"51":1}}],["crm和推理分辨率",{"2":{"51":1}}],["crm的细化结果包含更多细节",{"2":{"51":1}}],["crm的总推理时间仍不到cascadepsp的一半",{"2":{"51":1}}],["crm比segfix表现更好",{"2":{"51":1}}],["crm采用多分辨率推理策略",{"2":{"51":1}}],["crm找到",{"2":{"51":1}}],["crm展现出了很强的泛化能力",{"2":{"51":1}}],["crm通过不断地对齐特征图和细化目标",{"2":{"51":1}}],["crm",{"2":{"51":10}}],["crop",{"2":{"51":1}}],["cross",{"0":{"39":1},"1":{"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"40":1,"44":1,"62":3,"72":2,"84":1,"88":1,"130":1}}],["ceil",{"2":{"194":5}}],["cer",{"2":{"80":2}}],["certain",{"2":{"74":1}}],["certainty",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"2":{"18":1,"22":1}}],["center",{"2":{"61":1,"154":1}}],["ces",{"2":{"40":1}}],["cdsp",{"2":{"124":1}}],["cd",{"2":{"40":2,"41":1,"42":1,"43":1,"44":1}}],["cdl",{"2":{"26":1}}],["cityscapes和pascal等数据集上",{"2":{"142":1}}],["cityscapes",{"2":{"33":1,"35":2,"49":1,"70":1,"109":1,"137":1,"138":1}}],["clean",{"2":{"170":1,"171":1,"182":1}}],["closeness",{"2":{"103":1}}],["close",{"2":{"103":1}}],["cli",{"2":{"178":1,"179":1,"182":1}}],["click",{"2":{"165":1,"170":3,"171":5,"173":1,"176":1,"182":1}}],["clinically",{"2":{"62":1}}],["clip和dinov2保持冻结",{"2":{"159":1}}],["clip成功后",{"2":{"101":1}}],["clip分别为1",{"2":{"58":1}}],["clip相比",{"2":{"58":1}}],["clip",{"2":{"49":1,"53":1,"58":1,"101":1,"146":3}}],["clustering",{"2":{"30":1,"146":1}}],["classifying",{"2":{"155":2}}],["classified",{"2":{"126":1}}],["classification",{"2":{"8":1,"12":1,"92":1,"155":1}}],["class",{"0":{"71":1,"112":2},"1":{"72":1,"113":2,"114":2,"115":2,"116":2,"117":2,"118":2,"119":2,"120":2,"121":2,"122":2,"123":2,"124":2},"2":{"30":1,"40":2,"53":1,"72":5,"74":1,"84":3,"88":1,"92":4,"103":3,"113":3,"114":1,"146":1,"155":1}}],["classes",{"2":{"8":1,"40":1,"53":3,"72":1,"103":1,"113":1}}],["ccl",{"2":{"26":1}}],["cc4s取得了与全监督方法相媲美的性能",{"2":{"19":1}}],["cc4s将网络预测结果作为伪标签",{"2":{"19":1}}],["cc4s采用自监督训练策略",{"2":{"19":1}}],["cc4s在网络架构中嵌入随机游走模块",{"2":{"19":1}}],["cc4s",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"2":{"18":6,"19":1}}],["cat",{"2":{"101":1}}],["categories",{"2":{"49":1,"74":1,"92":1,"101":3,"103":1,"155":3}}],["category",{"2":{"18":1,"30":1,"101":3}}],["cam",{"2":{"116":1}}],["camvid",{"2":{"109":1}}],["cams本质上存在缺陷",{"2":{"76":1}}],["cams",{"2":{"76":1,"77":1,"78":1}}],["cam和隐式函数",{"2":{"51":1}}],["caption",{"2":{"146":1}}],["captures",{"2":{"101":1}}],["capture",{"2":{"62":1}}],["capitalizes",{"2":{"53":1}}],["capabilities",{"2":{"53":1,"135":1,"146":1}}],["cascadepsp",{"2":{"51":2}}],["cascade",{"2":{"51":1}}],["cases",{"2":{"18":1}}],["calculating",{"2":{"40":1}}],["called",{"2":{"30":1,"72":1}}],["candidate",{"2":{"53":1}}],["cannot",{"2":{"51":1}}],["can",{"2":{"0":1,"40":2,"53":1,"62":2,"70":2,"72":1,"103":1,"135":2,"155":1}}],["cor",{"2":{"135":1}}],["core",{"2":{"84":1}}],["corresponding",{"2":{"135":1}}],["correct",{"2":{"113":1}}],["correlation",{"0":{"29":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1},"2":{"30":2,"32":1,"70":1}}],["corrmatch显著扩展了高置信度区域",{"2":{"38":1}}],["corrmatch的主要贡献是重新考虑相关映射的使用",{"2":{"38":1}}],["corrmatch始终优于其他现有方法",{"2":{"38":1}}],["corrmatch在所有分割比例下均优于其他方法",{"2":{"35":1}}],["corrmatch在全分割比例下miou达到81",{"2":{"35":1}}],["corrmatch是一个单阶段框架",{"2":{"34":1}}],["corrmatch",{"0":{"29":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1},"2":{"30":3,"31":3}}],["counterparts",{"2":{"74":1}}],["cover",{"2":{"70":1}}],["collect",{"2":{"137":1}}],["collection",{"2":{"135":1}}],["collected",{"2":{"62":1}}],["color",{"2":{"18":1}}],["coarse",{"2":{"51":1}}],["costs",{"2":{"113":1,"126":1}}],["cost",{"2":{"51":1,"101":5,"126":1}}],["code插件",{"2":{"166":1}}],["code呢",{"2":{"163":1}}],["code",{"2":{"30":1,"40":1,"49":1,"51":1,"62":1,"70":1,"101":1,"103":1,"113":1,"126":1,"165":1,"178":2,"179":3,"182":2}}],["command的参数",{"2":{"179":1,"182":1}}],["command",{"2":{"170":4,"171":4,"178":2,"179":2,"182":6}}],["commonly",{"2":{"74":1}}],["common",{"2":{"51":1,"53":1,"84":2}}],["com网站发布segment",{"2":{"135":1}}],["combine",{"2":{"113":1}}],["com",{"2":{"18":1,"30":1,"31":1,"40":1,"41":1,"49":3,"51":2,"62":1,"63":1,"70":2,"101":3,"103":1,"104":1,"113":1,"114":1,"126":1,"127":1,"135":2,"164":1,"182":1}}],["comput",{"2":{"103":1}}],["computer",{"2":{"84":1,"103":1,"135":1,"137":1}}],["computational",{"2":{"101":1}}],["computation",{"2":{"51":2,"92":1}}],["competitive",{"2":{"72":1,"135":1,"137":1,"146":1}}],["component",{"2":{"70":2}}],["components",{"2":{"70":2,"155":1}}],["comprised",{"2":{"155":1}}],["comprises",{"2":{"70":1,"101":1}}],["comprehensive",{"2":{"18":1,"155":1}}],["complementarity",{"2":{"92":1}}],["complementary",{"2":{"72":1,"74":1}}],["complexity",{"2":{"74":1,"101":1}}],["completely",{"2":{"40":2}}],["complicated",{"2":{"30":1,"70":2}}],["comparable",{"2":{"18":1}}],["comparison",{"0":{"14":1},"2":{"72":1}}],["compared",{"0":{"13":1,"26":1,"35":1,"45":1,"58":1,"67":1,"79":1,"97":1,"108":1,"118":1,"131":1,"142":1,"151":1,"160":1},"1":{"14":1,"15":1,"109":1,"110":1,"119":1,"120":1},"2":{"53":1,"72":1,"101":1,"113":1,"137":1,"146":1}}],["coco数据集",{"0":{"204":1},"2":{"97":1}}],["coco",{"2":{"8":1,"14":1,"58":1,"74":1,"75":1,"79":2,"81":1,"92":1,"93":1,"97":1,"101":1,"109":1,"137":1,"138":1,"141":1,"160":1}}],["concepts",{"2":{"146":1}}],["concretely",{"2":{"70":1}}],["connections",{"2":{"141":1}}],["conv2d",{"2":{"194":13}}],["convtranspose2d",{"2":{"193":1}}],["convnext",{"2":{"101":2}}],["conventional",{"2":{"74":1}}],["conjunction",{"2":{"72":1}}],["conference",{"2":{"103":1}}],["confused",{"2":{"70":1}}],["confident",{"2":{"74":3}}],["confidence",{"2":{"30":1,"113":1}}],["confirm",{"2":{"8":1}}],["conditions",{"2":{"70":2}}],["conducted",{"2":{"53":1,"74":1}}],["conduct",{"2":{"30":1}}],["content",{"2":{"74":1}}],["context和partimagenet等数据集上",{"2":{"162":1}}],["context和pascal",{"2":{"60":1}}],["contextual",{"2":{"137":1}}],["context上微调模型",{"2":{"110":1}}],["context上微调模型以评估性能",{"2":{"109":1}}],["context数据集上对比",{"2":{"160":1}}],["context数据集",{"2":{"58":1}}],["context",{"2":{"8":1,"26":1,"49":1,"53":1,"54":1,"58":1,"101":2,"103":1,"104":1,"109":1,"137":1,"160":1}}],["contrast",{"2":{"72":1}}],["contin",{"2":{"51":1}}],["continuously",{"2":{"51":1}}],["continu",{"2":{"51":1}}],["containers",{"0":{"221":1}}],["contain",{"2":{"30":1}}],["consensus",{"2":{"62":1}}],["consists",{"2":{"137":1}}],["consistently",{"2":{"70":1}}],["consistent",{"2":{"18":2,"103":1}}],["consistency",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"2":{"18":3,"22":1,"74":1,"92":1,"130":1}}],["consideration",{"2":{"51":1}}],["constrains",{"2":{"74":1}}],["constraints",{"2":{"8":1}}],["constraint",{"2":{"8":1,"12":1,"18":1}}],["constructs",{"2":{"72":1}}],["construction",{"2":{"40":1,"44":1}}],["construct",{"2":{"40":1}}],["const",{"2":{"0":1}}],["d",{"2":{"88":1,"89":1,"194":1}}],["dynamic",{"0":{"71":1},"1":{"72":1},"2":{"72":1}}],["dtp显著优于现有技术",{"2":{"70":1}}],["dtp显著超越了现有的先进方法",{"2":{"70":1}}],["dtp可作为即插即用范式",{"2":{"70":1}}],["dtp几乎不增加额外的参数",{"2":{"70":1}}],["dtp包含两个关键技术",{"2":{"70":1}}],["dtp方法首先将夜间图像分解为不受光照影响的反射成分和与光照相关的光照成分",{"2":{"70":1}}],["dtp",{"2":{"70":10}}],["dwt",{"2":{"66":1}}],["dsir方法相比",{"2":{"67":1}}],["dsc指标分别至少高出1",{"2":{"69":1}}],["dsc指标在六个领域中的五个领域超过了所有现有方法",{"2":{"67":1}}],["dsc",{"2":{"62":1,"63":1}}],["dsaa",{"2":{"15":1}}],["dfq框架由分割骨干网络",{"2":{"68":1}}],["dfq",{"2":{"62":2,"63":2}}],["dvlab",{"2":{"51":2}}],["drawn",{"2":{"103":1}}],["driven",{"2":{"49":1,"72":1,"113":2,"114":1}}],["dr",{"2":{"43":1}}],["dressing",{"2":{"40":1}}],["dgss通过归一化和白化",{"2":{"43":1}}],["dgss",{"0":{"48":1},"1":{"49":1},"2":{"43":1,"49":8}}],["dangerous",{"2":{"221":2}}],["danger",{"2":{"221":2}}],["date",{"2":{"135":1,"173":1}}],["datasets",{"2":{"40":1,"49":1,"101":1,"103":4,"137":1}}],["dataset",{"2":{"30":1,"70":1,"103":2,"113":1,"126":1,"135":4}}],["data",{"0":{"2":1,"3":1},"2":{"0":3,"18":1,"30":2,"84":1,"92":3,"103":1,"135":4,"155":1,"159":1,"220":2}}],["day",{"2":{"70":3}}],["dass通过联合使用源域和目标域数据训练模型",{"2":{"43":1}}],["dass",{"2":{"43":1}}],["dhc模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性",{"2":{"47":1}}],["dhc和tsf三个关键模块的有效性",{"2":{"46":1}}],["dhc",{"2":{"40":1,"41":1,"44":1}}],["dmtnet利用smt模块基于自身原型为支持和查询图像计算变换矩阵",{"2":{"47":1}}],["dmtnet在四个具有不同领域差距的数据集上有效",{"2":{"47":1}}],["dmtnet在四个数据集的平均结果上表现优异",{"2":{"45":1}}],["dmtnet在性能上优于现有的最先进方法",{"2":{"41":1}}],["dmtnet",{"2":{"40":3,"41":2,"42":1,"44":1}}],["double",{"2":{"170":3,"171":2,"182":1}}],["doubly",{"0":{"39":1},"1":{"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"40":1,"44":1}}],["document",{"2":{"173":2}}],["documentclass",{"2":{"173":1}}],["documentation",{"2":{"5":1,"222":1}}],["doc表明编译器访问的是没有扩展名的根文件完整路径",{"2":{"171":1}}],["doc",{"2":{"171":1}}],["docfile仅是因为之前使用",{"2":{"171":1}}],["docfile可以将文件所在路径设置为中文",{"2":{"171":1}}],["docfile表明编译器访问没有扩展名的根文件名",{"2":{"171":1}}],["docfile更改为",{"2":{"171":1}}],["docfile",{"2":{"170":4,"171":4,"182":4}}],["download",{"2":{"165":1,"173":1,"176":1}}],["downstream",{"2":{"84":3,"137":1}}],["down",{"2":{"51":1,"101":1}}],["domains",{"2":{"40":2,"62":2,"72":2,"103":4}}],["domain",{"0":{"39":1,"48":1,"102":1},"1":{"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"49":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1},"2":{"40":4,"44":1,"49":1,"62":5,"72":1,"103":1}}],["dbfnet",{"2":{"26":1}}],["dilation",{"2":{"193":1}}],["dilation=1",{"2":{"193":1,"194":5}}],["dino",{"2":{"146":1,"159":1}}],["diffusion",{"2":{"113":1}}],["diffusing",{"2":{"113":1}}],["differences",{"2":{"40":1,"92":1}}],["different",{"2":{"40":1,"49":1,"62":2,"72":3,"84":1,"92":1,"101":2,"103":1,"113":1}}],["diode",{"2":{"109":1}}],["diminished",{"2":{"74":1}}],["diverse",{"2":{"49":1}}],["diversity",{"2":{"18":1}}],["dig",{"2":{"30":1}}],["disable",{"2":{"167":1}}],["disparity",{"2":{"74":1}}],["disentangles",{"2":{"70":1}}],["disentanglement",{"0":{"70":1},"2":{"70":1}}],["disentangle",{"0":{"70":1},"2":{"70":2}}],["discriminative",{"2":{"74":1}}],["discriminability",{"2":{"18":1}}],["discrepancy",{"2":{"62":1}}],["distance",{"2":{"113":1,"117":1}}],["distillation",{"2":{"74":1}}],["distilled",{"2":{"74":1,"75":1}}],["distinguish",{"2":{"51":1,"101":1}}],["distinction",{"2":{"92":1}}],["distinct",{"2":{"49":1}}],["distributions",{"2":{"135":1}}],["distribution",{"2":{"62":1}}],["distributed",{"2":{"18":1}}],["directly",{"2":{"18":1,"70":1,"72":1,"113":1}}],["def",{"2":{"194":2}}],["default",{"2":{"170":1,"171":1,"182":1,"220":2}}],["defined",{"2":{"155":3}}],["devise",{"2":{"146":1}}],["developed",{"2":{"70":1}}],["dec",{"2":{"143":1}}],["decoder",{"0":{"100":1},"1":{"101":1},"2":{"72":1,"101":5,"137":1,"141":1}}],["decoupled",{"0":{"61":1},"1":{"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1},"2":{"62":3}}],["decouples",{"2":{"8":1}}],["dense",{"2":{"137":1}}],["del",{"2":{"117":1,"124":1}}],["demonstrating",{"2":{"113":1}}],["demonstrated",{"2":{"146":1}}],["demonstrate",{"2":{"49":1,"70":1,"74":1,"126":1,"155":1}}],["demonstrates",{"2":{"0":1,"101":1,"219":1}}],["depth",{"2":{"103":1}}],["dependency",{"2":{"62":1}}],["describe",{"2":{"103":1}}],["describing",{"2":{"103":1}}],["despite",{"2":{"103":1}}],["designed",{"2":{"84":1,"135":1,"155":1}}],["designing",{"2":{"30":1}}],["design",{"2":{"18":1,"113":1}}],["degradation",{"2":{"101":1}}],["deit",{"2":{"72":1}}],["deal",{"2":{"62":1}}],["derived",{"2":{"53":1}}],["determined",{"2":{"113":1}}],["deterministic",{"2":{"18":2}}],["deteriorated",{"2":{"74":1}}],["details",{"2":{"51":1,"171":1,"221":4}}],["deepmask等等",{"2":{"189":1}}],["deeplabv2作为分割器",{"2":{"122":1}}],["deeplabv3+作为分割器",{"2":{"119":1}}],["deeplabv3+",{"2":{"30":1,"31":1}}],["deeplab",{"2":{"51":1,"78":1,"189":1}}],["deepglobe",{"2":{"45":1}}],["deep",{"2":{"18":1,"62":3}}],["dcm将分割任务解耦为语义对齐和空间对齐",{"2":{"16":1}}],["dcm模块将分割任务拆解为语义对齐和空间对齐",{"2":{"9":1}}],["dcm",{"2":{"8":2,"9":1,"12":1,"15":2,"16":1}}],["dcam利用双语义感知注意力机制加强约束",{"2":{"16":1}}],["dcam通过双重语义感知注意力机制解决了注意力偏差问题",{"2":{"9":1}}],["dcam",{"2":{"8":2,"9":1,"12":1,"15":2,"16":1}}],["during",{"2":{"92":1,"126":1}}],["dut",{"2":{"51":1}}],["dual",{"2":{"8":3,"12":2,"40":1,"44":1}}],["due",{"2":{"8":1,"18":1,"62":1,"70":1,"84":1}}],["syntax",{"0":{"220":1},"2":{"220":1}}],["synctex的参数",{"2":{"179":1,"182":1}}],["synctex",{"2":{"170":3,"171":1,"178":2,"179":4,"182":3}}],["synctex=1",{"2":{"170":3,"171":3,"182":3}}],["systematic",{"2":{"137":1}}],["skip",{"2":{"141":1}}],["swin",{"2":{"140":1}}],["sbd",{"2":{"118":1}}],["ssl",{"2":{"137":1,"138":1,"139":1}}],["ssiw",{"2":{"103":1,"104":1}}],["ssd和fss",{"2":{"51":1}}],["s",{"2":{"72":2,"113":1}}],["sne可视化特征空间",{"2":{"67":1}}],["smt",{"2":{"40":1,"41":1,"44":1}}],["smm",{"2":{"22":1}}],["silberman",{"2":{"103":1,"104":1}}],["size=2",{"2":{"194":5}}],["size=",{"2":{"194":13}}],["size",{"2":{"101":1,"193":2,"194":1}}],["signal",{"2":{"137":1}}],["significant",{"2":{"51":1,"62":1,"103":1,"146":1}}],["significantly",{"2":{"49":1,"70":1}}],["sight",{"2":{"72":1}}],["single",{"2":{"49":1,"74":1,"101":1}}],["simplified",{"2":{"166":1}}],["simplebase等",{"2":{"33":1}}],["simple",{"0":{"100":1},"1":{"101":1},"2":{"30":1,"101":1,"103":1}}],["similarity",{"2":{"92":1}}],["similarities",{"2":{"30":1}}],["similar",{"2":{"18":1,"62":2,"92":1}}],["sice",{"2":{"29":1}}],["site",{"2":{"0":1}}],["springer",{"2":{"103":1}}],["spread",{"2":{"30":1}}],["speed",{"2":{"101":1}}],["specific",{"2":{"40":3,"70":1,"72":2,"84":2,"92":1,"113":1,"137":1}}],["specifically",{"2":{"18":1,"84":1}}],["spt中的高斯抑制通过调整注意力分布",{"2":{"72":1}}],["spt",{"2":{"72":5}}],["spml",{"2":{"26":1}}],["sparse",{"2":{"113":1}}],["sparsity",{"2":{"18":1}}],["space",{"2":{"103":1,"113":1}}],["spatial",{"2":{"8":4,"84":1,"101":1}}],["sumatrapdf作为自己的",{"2":{"180":1}}],["sumatrapdf下载与安装",{"0":{"176":1}}],["sumatrapdf",{"0":{"175":1,"180":1},"1":{"176":1,"177":1,"178":1,"179":1},"2":{"175":1,"176":1,"178":4,"179":5,"180":2,"182":5}}],["sub",{"2":{"137":1}}],["surrounding",{"2":{"126":1}}],["surpasses",{"2":{"74":1,"92":1}}],["surpassing",{"2":{"49":1}}],["surprisingly",{"2":{"49":1,"72":1}}],["sunrgbd",{"2":{"109":1}}],["sun",{"2":{"89":1}}],["success",{"2":{"84":1}}],["such",{"2":{"18":2,"51":1,"53":1,"62":1,"72":1,"84":1,"103":1,"146":1,"155":1}}],["supposed",{"2":{"62":1}}],["support",{"2":{"40":5}}],["super",{"2":{"194":1}}],["superiority",{"2":{"84":1,"113":1}}],["superior",{"0":{"48":1},"1":{"49":1},"2":{"40":1,"49":1,"135":1,"155":1}}],["supervisory",{"2":{"137":1}}],["supervisions",{"2":{"113":1}}],["supervision",{"2":{"18":4,"74":2,"113":3,"126":2}}],["supervised",{"0":{"17":1,"29":1,"73":1,"112":1,"125":1,"136":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1},"2":{"18":3,"22":1,"30":1,"40":1,"74":1,"75":1,"103":2,"113":1,"114":1,"126":2,"127":1,"135":1,"137":4,"146":1}}],["songti",{"2":{"173":1}}],["so",{"2":{"135":1,"137":1}}],["sod",{"2":{"70":4}}],["source",{"2":{"62":3,"101":1}}],["solution",{"2":{"103":1}}],["solutions",{"2":{"18":2}}],["solve",{"2":{"40":1}}],["sota",{"0":{"26":1,"35":1,"45":1,"58":1,"67":1,"79":1,"97":1,"108":1,"118":1,"131":1,"142":1,"151":1,"160":1},"1":{"109":1,"110":1,"119":1,"120":1},"2":{"53":1,"54":1,"84":1}}],["soft",{"2":{"18":1,"90":1}}],["some",{"2":{"0":1,"70":1,"103":1,"219":1}}],["shiki",{"2":{"220":1}}],["shift",{"2":{"62":1}}],["share",{"2":{"84":1}}],["shanghai",{"2":{"70":1}}],["shandong",{"2":{"17":1}}],["shallow",{"2":{"62":1}}],["shape",{"2":{"30":1,"62":1,"188":1}}],["shenzhen",{"2":{"29":1}}],["short",{"2":{"103":1}}],["should",{"2":{"74":1}}],["showcontextmenu",{"2":{"170":1,"171":1,"182":1}}],["showing",{"2":{"103":1}}],["show",{"2":{"40":1,"51":1,"53":1,"62":2,"84":1,"170":2,"171":2,"182":2}}],["shows",{"2":{"18":1,"51":1,"137":1,"146":1}}],["shown",{"2":{"8":1,"137":1,"155":1}}],["shot分割",{"2":{"72":1}}],["shot设置下分别提高了3",{"2":{"45":1}}],["shot设置下达到66",{"2":{"45":1}}],["shot设置下达到",{"2":{"45":1}}],["shot和5",{"2":{"45":1}}],["shot语义分割",{"2":{"9":1}}],["shot",{"0":{"6":1,"39":1,"71":1,"91":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"72":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1},"2":{"8":1,"10":1,"11":1,"12":1,"40":1,"44":1,"72":3,"92":1,"103":1,"104":1,"135":3}}],["sar分割",{"2":{"88":1}}],["salient",{"2":{"18":1}}],["sam可能会遗漏图像中的精细结构",{"2":{"135":1}}],["sampling",{"2":{"51":1}}],["samples",{"2":{"8":1,"137":1}}],["sam等在计算机视觉挑战中表现出色",{"2":{"49":1}}],["sam等大规模vfms显著提升了计算机视觉任务的性能",{"2":{"49":1}}],["same",{"2":{"30":1,"62":2,"74":1}}],["sam",{"2":{"15":1,"49":1,"135":4,"146":1,"148":1,"155":1}}],["sa",{"2":{"15":1,"135":5}}],["studio",{"2":{"163":1,"165":1,"166":1}}],["studies",{"2":{"155":1}}],["study",{"0":{"46":1},"2":{"8":1,"113":1,"137":1}}],["stuff",{"2":{"58":1,"101":1,"137":1,"138":1,"141":1,"142":1,"144":1}}],["start",{"2":{"194":2}}],["statistics",{"2":{"84":1}}],["state",{"0":{"13":1,"14":1},"1":{"14":1,"15":1},"2":{"39":1,"40":1,"49":1,"53":1,"62":1,"70":1,"72":3,"74":1,"92":1,"103":3,"126":1,"155":1}}],["standard",{"2":{"72":1,"103":1}}],["stage",{"2":{"40":1,"74":2,"141":4}}],["st++",{"2":{"33":1}}],["stride=2",{"2":{"194":5}}],["stride=",{"2":{"194":13}}],["stride=1",{"2":{"193":1}}],["strides=",{"2":{"193":2}}],["stride",{"2":{"193":2}}],["strives",{"2":{"101":1}}],["struggle",{"2":{"146":1}}],["structure",{"2":{"18":2,"90":1,"101":1}}],["strong",{"2":{"103":1,"137":1}}],["stronger",{"0":{"48":1},"1":{"49":1},"2":{"49":1}}],["strategy",{"2":{"40":1,"74":1,"137":1}}],["strategies",{"2":{"18":1,"30":2,"51":1}}],["strengthen",{"2":{"8":1}}],["scenarios",{"2":{"74":1}}],["scenes",{"2":{"70":2}}],["scene",{"2":{"49":1}}],["sciences",{"0":{"83":1},"2":{"91":2}}],["science",{"2":{"70":1}}],["scheme",{"2":{"62":1,"72":1,"101":1}}],["scannet和wilddash1上达到了最先进的性能",{"2":{"110":1}}],["scannet和sintel等数据集上进行评估",{"2":{"109":1}}],["scannet",{"2":{"109":1}}],["scanners",{"2":{"62":1}}],["scaling",{"0":{"102":1},"1":{"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1}}],["scaled",{"2":{"53":1,"57":2}}],["scale",{"2":{"8":1,"103":2,"141":1,"146":1}}],["scarce",{"2":{"84":1}}],["score",{"2":{"30":1,"101":1}}],["scribbling",{"2":{"126":1}}],["scribblesup数据集上的比较",{"0":{"119":1}}],["scribblesup",{"2":{"26":1,"113":1}}],["scribble",{"0":{"17":1,"112":2,"125":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"113":2,"114":2,"115":2,"116":2,"117":2,"118":2,"119":2,"120":2,"121":2,"122":2,"123":2,"124":2,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"18":3,"22":1,"113":7,"114":1,"124":1,"126":4,"127":1,"131":1}}],["script>",{"2":{"0":1}}],["script",{"2":{"0":1}}],["sequential",{"2":{"194":1}}],["section",{"2":{"173":2}}],["search",{"2":{"178":2,"179":2,"182":2}}],["searchpath",{"2":{"164":1}}],["seamlessly",{"2":{"62":1}}],["sentence",{"0":{"102":1},"1":{"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1},"2":{"103":2}}],["sensing",{"2":{"72":1}}],["sensitivity",{"2":{"8":1}}],["sed在准确性和速度方面均有效",{"2":{"101":1}}],["sed由基于分层编码器的代价图生成和带有类别早期拒绝的渐进式融合解码器组成",{"2":{"101":1}}],["sed下载",{"2":{"101":1}}],["sed方法在ade20k数据集上",{"2":{"101":1}}],["sed包含两部分",{"2":{"101":1}}],["sed",{"0":{"100":1},"1":{"101":1},"2":{"101":4}}],["seek",{"2":{"74":1}}],["separate",{"2":{"74":1}}],["serve",{"2":{"70":1}}],["services",{"2":{"39":1}}],["segnet的基本原理",{"0":{"217":1}}],["segnet",{"0":{"216":1},"1":{"217":1}}],["segmen",{"2":{"155":1}}],["segments",{"2":{"155":3}}],["segmentor作为全景和实体分割的基准方法",{"2":{"51":1}}],["segment",{"0":{"134":1,"154":1},"1":{"135":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"40":2,"51":1,"103":1,"135":5,"155":5,"157":1}}],["segmenting",{"2":{"8":1}}],["segmentation",{"0":{"6":1,"17":1,"29":1,"39":1,"48":1,"50":1,"52":1,"61":1,"70":1,"71":1,"73":1,"82":1,"91":1,"100":1,"102":1,"112":1,"125":1,"136":1,"145":1,"154":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"49":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1,"72":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"101":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1},"2":{"8":2,"10":1,"11":1,"12":1,"18":2,"22":1,"30":3,"40":2,"44":1,"49":1,"51":3,"53":4,"57":1,"62":2,"70":4,"72":3,"74":2,"75":1,"84":3,"92":1,"101":5,"103":6,"113":3,"114":1,"126":3,"127":1,"135":2,"137":6,"146":2,"155":5}}],["segformer中的mit",{"2":{"130":1}}],["segfix和crm的细化结果对比",{"2":{"51":1}}],["segfix和mgmatting的性能",{"2":{"51":1}}],["segfix作为高分辨率分割细化方法",{"2":{"51":1}}],["seg",{"2":{"101":1}}],["seg相比",{"2":{"59":1}}],["setting",{"2":{"72":1}}],["settings",{"2":{"49":1,"70":1}}],["set",{"2":{"49":1,"53":1,"101":1,"103":1}}],["setup>",{"2":{"0":1}}],["several",{"2":{"40":1}}],["semi",{"0":{"29":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1},"2":{"30":1}}],["semantics",{"2":{"70":3,"72":1,"74":1,"103":1,"113":2}}],["semantically",{"2":{"18":1,"70":1,"155":1}}],["semantic",{"0":{"6":1,"17":1,"29":1,"39":1,"48":1,"52":1,"70":1,"73":1,"82":1,"91":1,"100":1,"102":1,"112":1,"125":1,"136":1,"145":2},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"49":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"101":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"146":2,"147":2,"148":2,"149":2,"150":2,"151":2,"152":2,"153":2},"2":{"8":4,"11":1,"12":1,"18":3,"22":1,"30":1,"40":1,"44":1,"49":1,"53":5,"57":2,"70":3,"72":1,"74":3,"75":1,"84":2,"92":3,"101":4,"103":4,"113":2,"114":1,"126":3,"127":1,"137":6,"146":4,"155":1}}],["self",{"0":{"73":1,"136":1},"1":{"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1},"2":{"18":2,"40":3,"44":2,"62":1,"74":2,"75":1,"137":5,"194":7}}],["a4paper",{"2":{"173":1}}],["author",{"2":{"173":1}}],["auto",{"2":{"178":1,"179":2,"182":1}}],["autoclean",{"2":{"170":1,"171":1,"182":1}}],["autobuild",{"2":{"170":1,"171":1,"182":1}}],["aux",{"2":{"170":1,"171":1,"182":1}}],["australia",{"2":{"125":1}}],["augment",{"2":{"126":1}}],["augmentation",{"0":{"125":1},"1":{"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"126":1}}],["augmented",{"2":{"74":1}}],["aug",{"2":{"35":1}}],["a6000",{"2":{"101":1}}],["ai",{"0":{"135":1},"2":{"70":1,"135":1}}],["aid",{"2":{"62":1}}],["aim",{"2":{"30":1}}],["aims",{"2":{"8":1,"40":1,"53":1}}],["america",{"2":{"154":1}}],["ambiguity",{"2":{"92":1}}],["amp",{"0":{"48":1},"1":{"49":1}}],["amount",{"2":{"155":1}}],["amounts",{"2":{"18":1}}],["among",{"2":{"18":1,"51":1,"62":1,"84":1,"126":1}}],["after",{"2":{"40":1}}],["args",{"2":{"170":4,"171":4,"178":2,"179":2,"182":6}}],["architecture",{"2":{"135":1,"137":1}}],["arbitrary",{"2":{"62":1}}],["are",{"2":{"40":2,"53":1,"62":2,"70":1,"72":2,"101":1,"113":1,"135":1,"137":1,"146":2}}],["article",{"2":{"173":1}}],["artificial",{"2":{"154":1}}],["arts",{"0":{"14":1},"2":{"72":2}}],["art",{"0":{"13":1},"1":{"14":1,"15":1},"2":{"40":1,"49":1,"53":1,"62":1,"70":1,"72":1,"74":1,"92":1,"103":3,"126":1,"155":1}}],["about",{"2":{"171":1}}],["above",{"2":{"40":1,"137":1}}],["abstract",{"2":{"173":2}}],["absrel",{"2":{"109":1}}],["absolute",{"2":{"53":1}}],["ability",{"2":{"49":1,"51":1}}],["ablation",{"0":{"13":1,"15":1,"27":1,"36":1,"46":1,"59":1,"68":1,"98":1,"108":1,"121":1,"132":1,"143":1,"152":1,"161":1},"1":{"14":1,"15":1,"109":1,"110":1,"122":1,"123":1},"2":{"72":2}}],["aeronautics",{"2":{"39":1}}],["attaining",{"2":{"126":1}}],["attribute",{"2":{"53":1}}],["attracted",{"2":{"53":1}}],["attentions",{"2":{"53":1}}],["attention",{"2":{"8":3,"53":2,"57":6,"62":2,"103":1}}],["at",{"2":{"30":1,"40":1,"49":1,"51":1,"62":1,"70":1,"101":4,"103":2,"113":1,"126":1,"135":1,"146":1,"155":1}}],["available",{"2":{"30":1,"40":1,"49":1,"51":1,"62":1,"70":1,"101":1,"103":3,"113":1,"126":1,"146":1}}],["agmm和tel的性能下降约10",{"2":{"120":1}}],["aggregate",{"2":{"70":1,"84":1}}],["aggregates",{"2":{"51":1}}],["aggregation",{"2":{"8":1,"12":1}}],["age",{"2":{"40":1}}],["agnostic",{"2":{"30":1,"40":2,"72":1,"155":1}}],["a2gnn",{"2":{"26":1}}],["assignment",{"2":{"90":1}}],["assistance",{"2":{"137":1}}],["assist",{"2":{"84":1}}],["assume",{"2":{"74":1}}],["assess",{"2":{"49":1}}],["aspp",{"2":{"70":1}}],["asd提高了0",{"2":{"68":2}}],["asd改善了0",{"2":{"67":1}}],["astronautics",{"2":{"39":1}}],["as",{"2":{"18":2,"30":1,"40":1,"51":1,"53":1,"62":1,"70":2,"92":1,"103":1,"113":1,"146":1,"155":1,"194":3}}],["acr",{"2":{"170":1,"171":1,"182":1}}],["across",{"2":{"49":1}}],["acn",{"2":{"170":1,"171":1,"182":1}}],["acquire",{"2":{"146":1,"164":1}}],["acp",{"2":{"88":1,"90":1}}],["academy",{"0":{"83":1},"2":{"91":2}}],["activating",{"2":{"72":1}}],["according",{"2":{"113":1}}],["acceleration",{"2":{"101":1}}],["accelerate",{"2":{"101":1}}],["accessing",{"2":{"49":1}}],["access",{"2":{"0":1}}],["accurarcy",{"2":{"191":1}}],["accuracy",{"2":{"51":1,"74":1,"92":1,"101":1,"191":2}}],["accurately",{"2":{"40":1,"155":1}}],["accurate",{"2":{"30":1}}],["achieve",{"2":{"103":1,"137":1}}],["achieves",{"2":{"18":1,"30":1,"40":1,"49":1,"72":1,"84":1,"101":1,"103":1,"126":1,"137":1}}],["achieved",{"2":{"18":1,"103":1,"146":1}}],["ap",{"2":{"51":1,"109":1,"110":1}}],["app",{"2":{"178":1,"179":1,"182":1}}],["applied",{"2":{"137":1}}],["applications",{"2":{"53":1,"103":1,"137":1}}],["appealing",{"2":{"137":1}}],["appearance",{"2":{"40":1}}],["approaches",{"2":{"30":1,"40":1,"53":1}}],["approach",{"2":{"8":1,"30":1,"49":1,"74":1,"103":1,"126":1}}],["apis",{"2":{"0":1,"5":1}}],["api",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1}}],["alg",{"2":{"170":1,"171":1,"182":1}}],["algorithm",{"2":{"146":1}}],["alternatives",{"2":{"113":1}}],["although",{"2":{"84":1}}],["al",{"2":{"103":2,"104":2}}],["ali",{"2":{"173":1}}],["alibaba",{"2":{"91":1}}],["align",{"2":{"148":1}}],["align从噪声图像",{"2":{"101":1}}],["aligns",{"2":{"51":1}}],["alignment",{"2":{"8":2,"84":1,"88":1}}],["alabtion",{"0":{"80":1}}],["all",{"2":{"113":1}}],["allows",{"2":{"92":1}}],["allowing",{"2":{"70":1}}],["allowed",{"2":{"62":1}}],["alleviate",{"2":{"8":1,"18":1,"74":1,"92":1,"137":1}}],["also",{"2":{"30":1,"62":1,"74":1,"103":1,"113":1,"155":1}}],["adjust",{"2":{"164":1}}],["advancde",{"2":{"164":1}}],["advantage",{"2":{"137":1}}],["advantages",{"2":{"113":1}}],["adapts",{"2":{"113":1}}],["adaptation",{"2":{"103":1}}],["adaptability",{"2":{"84":1}}],["adapt",{"2":{"101":1,"146":1}}],["adapting",{"2":{"84":1}}],["adaptively",{"2":{"72":1,"74":1}}],["adaptive",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1},"2":{"8":1,"12":1,"70":1,"74":1}}],["adapter",{"2":{"57":1}}],["ade20k数据集",{"2":{"58":1}}],["ade20k",{"2":{"53":1,"58":1,"101":3,"109":1,"137":1,"138":1,"141":1,"142":1,"144":1,"160":1}}],["adopting",{"2":{"113":1}}],["adopts",{"2":{"18":1}}],["adobe",{"0":{"51":1}}],["additionally",{"2":{"103":1,"146":1}}],["additional",{"2":{"53":1,"70":1,"220":1}}],["addition",{"2":{"40":1}}],["address",{"2":{"18":1,"51":1,"62":1,"103":1,"126":1,"146":1,"155":1}}],["addresses",{"2":{"8":1}}],["ad",{"2":{"40":1}}],["aware",{"0":{"71":1},"1":{"72":1},"2":{"8":3,"70":1,"72":1,"84":2,"88":1}}],["anything",{"0":{"134":1},"1":{"135":1},"2":{"135":8,"155":1}}],["any",{"2":{"49":1,"103":1}}],["an",{"2":{"18":1,"49":1,"53":1,"62":2,"70":1,"74":2,"84":1,"92":2,"101":1,"146":1,"221":2}}],["annotation",{"2":{"18":2,"113":1,"126":1}}],["annotations",{"2":{"18":2,"103":2,"113":2,"126":1,"137":1}}],["annotated",{"2":{"8":1,"30":1,"103":1,"113":1}}],["and",{"0":{"13":1,"17":1,"71":1,"108":1},"1":{"14":1,"15":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"72":1,"109":1,"110":1},"2":{"0":2,"8":5,"18":9,"22":1,"26":2,"30":1,"39":1,"40":5,"45":1,"49":3,"51":6,"53":4,"62":7,"70":9,"72":9,"74":8,"84":3,"90":1,"92":7,"101":3,"103":8,"113":5,"118":1,"135":8,"137":4,"146":3,"155":4}}],["a",{"0":{"6":1,"100":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"101":1},"2":{"8":3,"18":5,"30":2,"40":5,"49":4,"53":2,"62":3,"70":2,"72":2,"74":3,"84":1,"92":1,"101":8,"103":10,"113":3,"126":2,"135":2,"137":7,"143":1,"146":1,"155":4,"173":2,"174":2,"221":8}}],["txt",{"2":{"170":3,"171":10,"173":1,"178":1,"179":6,"182":1}}],["tl",{"2":{"164":1}}],["t的moby方法高出2",{"2":{"142":1}}],["typical",{"2":{"74":1}}],["typically",{"2":{"70":1,"84":1}}],["t",{"2":{"72":1,"88":1,"143":1}}],["true",{"2":{"170":2,"171":2,"182":2}}],["tree",{"2":{"51":2}}],["tranbilchan",{"2":{"127":1}}],["tranquilchan",{"2":{"126":1}}],["transferability",{"2":{"84":1}}],["transfers",{"2":{"72":1}}],["transfer",{"0":{"71":1},"1":{"72":1},"2":{"72":4,"135":1}}],["transform",{"2":{"40":1}}],["transformation",{"0":{"39":1},"1":{"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1},"2":{"40":6,"44":2,"62":1}}],["transformer的窗口注意力块来处理局部信息",{"2":{"141":1}}],["transformer的编码器",{"2":{"130":1}}],["transformer引入卷积风格窗口计算",{"2":{"140":1}}],["transformers",{"0":{"136":1},"1":{"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1}}],["transformer等推动性能提升",{"2":{"106":1}}],["transformer因强大特征表示能力受关注",{"2":{"65":1}}],["transformer因能捕捉长距离相关性",{"2":{"11":1}}],["transformer应用",{"2":{"11":1}}],["transformer",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1},"2":{"8":1,"12":1,"62":1,"101":2,"141":1}}],["transition",{"2":{"18":1}}],["traction",{"2":{"113":1}}],["trade",{"2":{"74":1}}],["traditional",{"2":{"53":1}}],["trainable",{"2":{"49":4}}],["trained",{"2":{"49":1,"84":2,"101":1,"135":1}}],["train",{"2":{"18":1,"40":1}}],["training",{"2":{"18":3,"30":1,"51":1,"53":3,"84":1,"101":2,"103":2,"126":1,"137":4}}],["tuple",{"2":{"193":5}}],["tug",{"2":{"164":1}}],["tures",{"2":{"51":1}}],["tuning",{"2":{"49":2,"84":2,"88":1,"103":2}}],["tunes",{"2":{"49":1}}],["tune",{"2":{"40":1,"72":2}}],["tsf策略微调少量参数",{"2":{"47":1}}],["tsf",{"2":{"40":1,"41":1,"44":1}}],["ti+∑jnji−nii",{"2":{"191":1}}],["ti=∑jnij表示标签中所有分类为i的像素点数量",{"2":{"191":1}}],["title",{"2":{"173":1}}],["tion",{"2":{"51":1,"84":1}}],["times",{"2":{"101":1}}],["time",{"0":{"70":1},"2":{"40":1,"44":1,"70":7}}],["tiple",{"2":{"62":1}}],["tip",{"2":{"34":1,"57":1,"145":1,"156":1,"221":4}}],["two",{"2":{"30":1,"70":1,"155":1}}],["tex的latex文件路径",{"2":{"182":1}}],["tex测试文件下载",{"0":{"173":1}}],["tex文件编译",{"0":{"172":1},"1":{"173":1,"174":1}}],["texstudio",{"2":{"171":1}}],["texworks",{"2":{"164":2}}],["texlive",{"2":{"164":1}}],["tex",{"0":{"164":1,"174":1},"2":{"164":6,"171":7,"173":2,"174":4,"178":1,"179":3,"180":1,"182":2}}],["text",{"2":{"101":1,"104":1,"155":3}}],["teaches",{"2":{"92":1}}],["teacher架构的方法",{"2":{"33":1}}],["teacher架构或自训练策略",{"2":{"32":1}}],["tend",{"2":{"72":1}}],["tends",{"2":{"62":1}}],["technology",{"2":{"70":1}}],["technically",{"2":{"62":1}}],["testfile",{"2":{"173":1}}],["test",{"2":{"40":1,"44":1,"173":1,"194":2}}],["testing",{"2":{"40":1,"51":1,"92":1}}],["tel",{"2":{"26":1}}],["tem通过多尺度局部上下文相关性增强前景特征",{"2":{"16":1}}],["tem通过探索多尺度局部上下文的相关性",{"2":{"9":1}}],["tem旨在减轻骨干网络的固有偏差并增强查询前景区域",{"2":{"15":1}}],["tem和dcam协同作用可提升2",{"2":{"15":1}}],["tem",{"2":{"8":2,"9":1,"12":1,"15":2,"16":1}}],["taborbrowser",{"2":{"179":1}}],["tab",{"2":{"179":1}}],["tableofcontents",{"2":{"173":1}}],["table",{"2":{"90":2}}],["tation",{"2":{"155":1}}],["taxonomies",{"2":{"103":1}}],["tackle",{"2":{"70":1}}],["taking",{"2":{"30":1}}],["tasks",{"2":{"49":1,"72":1,"84":2,"126":1,"135":2,"137":1,"146":2,"155":1}}],["task",{"2":{"8":1,"51":1,"62":1,"70":1,"72":2,"92":1,"101":1,"135":5,"137":1,"155":1}}],["target",{"2":{"8":1,"12":1,"51":1,"53":1,"62":1,"72":2}}],["torch",{"2":{"193":1,"194":4}}],["tool是name标签所对应的编译顺序",{"2":{"171":1}}],["tools",{"2":{"170":7,"171":7,"182":7}}],["toc",{"2":{"170":1,"171":1,"182":1}}],["towards",{"0":{"145":1},"1":{"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1}}],["top",{"2":{"101":1}}],["token汇总为一个类token",{"2":{"78":1}}],["tokens",{"2":{"49":1,"84":1}}],["together",{"2":{"18":1}}],["to",{"2":{"0":1,"8":7,"18":13,"30":5,"40":9,"49":3,"51":5,"53":4,"62":11,"70":5,"72":8,"74":6,"84":7,"92":5,"101":7,"103":6,"113":6,"126":3,"135":5,"137":3,"146":4,"155":2,"165":1,"173":1,"176":1}}],["things和stuff",{"2":{"199":1}}],["this",{"2":{"0":1,"8":1,"18":1,"30":1,"40":1,"49":2,"53":1,"62":1,"70":2,"72":1,"74":1,"101":1,"103":2,"113":2,"126":1,"135":1,"146":1,"155":3,"167":1,"171":1,"173":2,"219":1,"221":10}}],["thus",{"2":{"113":1}}],["thousands",{"2":{"40":1}}],["those",{"2":{"18":1}}],["thatworks",{"2":{"72":1}}],["that",{"2":{"30":1,"40":2,"49":2,"51":2,"53":2,"70":2,"72":2,"74":5,"101":1,"103":2,"126":3,"135":1,"155":2}}],["through",{"2":{"8":1,"126":1,"155":1}}],["three",{"2":{"8":1,"53":1,"72":1}}],["therefrom",{"2":{"103":1}}],["thereby",{"2":{"84":1}}],["their",{"2":{"70":1,"103":1}}],["them",{"2":{"155":1}}],["themselves",{"2":{"40":1}}],["theme",{"0":{"2":1},"2":{"0":4,"2":1}}],["they",{"2":{"40":1,"137":1,"146":1}}],["these",{"2":{"18":1,"30":1,"51":1,"92":1,"126":1,"137":1,"155":1}}],["then",{"0":{"70":1},"2":{"8":1,"30":1,"62":1,"70":3}}],["the",{"0":{"13":2,"14":2},"1":{"14":2,"15":2},"2":{"0":3,"5":2,"8":7,"18":15,"30":10,"40":10,"49":8,"51":7,"53":6,"62":13,"70":10,"72":16,"74":8,"84":9,"92":12,"101":7,"103":13,"113":9,"126":6,"135":5,"137":7,"146":4,"155":8,"173":1,"219":1,"222":2}}],["blg",{"2":{"170":1,"171":1,"182":1}}],["block",{"2":{"137":1,"221":2}}],["bbl",{"2":{"170":1,"171":1,"182":1}}],["bbbbchan",{"2":{"30":1,"31":1}}],["bcai",{"2":{"154":1}}],["bce",{"2":{"44":1}}],["b5时",{"2":{"132":1}}],["b1仍使miou提高了0",{"2":{"133":1}}],["b1骨干网络在全监督数据集上的性能稍弱",{"2":{"131":1}}],["b1作为骨干网络",{"2":{"131":1}}],["b1",{"2":{"130":1}}],["b时",{"2":{"101":1}}],["browser",{"2":{"179":1}}],["branch",{"2":{"92":2}}],["breakthroughs",{"2":{"53":1}}],["box",{"2":{"221":2}}],["bool",{"2":{"193":1}}],["bosch",{"2":{"154":2}}],["boundary",{"2":{"113":1}}],["boundaries",{"2":{"74":2}}],["both",{"2":{"0":1,"62":1,"113":2,"137":1}}],["b",{"2":{"72":1,"78":1,"101":2,"174":2}}],["bg",{"2":{"72":1}}],["bdd100k",{"2":{"70":2,"109":1}}],["bib的编译",{"2":{"174":1}}],["bib",{"2":{"171":1}}],["bibtex",{"2":{"170":8,"171":8,"174":1,"182":8}}],["billion",{"2":{"135":1}}],["bine",{"2":{"101":1}}],["biqiwhu",{"2":{"62":1,"63":1}}],["bias=true",{"2":{"193":1}}],["bias",{"2":{"8":6,"193":1}}],["building",{"2":{"137":1}}],["built",{"2":{"49":1,"135":1,"219":1}}],["but",{"2":{"18":1,"30":3,"72":1,"74":1,"103":1,"155":1}}],["bpg",{"2":{"26":1}}],["base",{"2":{"53":1}}],["based",{"0":{"6":1,"112":1,"125":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"8":1,"12":1,"18":1,"40":3,"44":1,"53":1,"70":2,"84":1,"101":3,"103":1,"113":1,"124":1,"126":1,"137":1,"155":1}}],["balance",{"2":{"51":1,"84":1}}],["backbone",{"2":{"30":1,"49":2,"101":3,"194":2}}],["backgrounds",{"2":{"18":1}}],["background",{"2":{"8":1,"40":2,"74":1,"92":2}}],["begin",{"2":{"173":2}}],["because",{"2":{"137":1}}],["beijing",{"2":{"112":1}}],["being",{"2":{"70":1}}],["better",{"2":{"92":1,"101":1}}],["between",{"2":{"30":1,"40":1,"51":2,"62":1,"70":1,"74":2,"84":1,"92":3}}],["benchmark",{"2":{"103":1}}],["benchmarks",{"2":{"30":1,"53":1,"62":1,"72":1,"92":1,"155":1}}],["benefit",{"2":{"70":1}}],["besides",{"2":{"51":1,"62":1}}],["been",{"2":{"18":1,"70":1,"137":1}}],["be",{"2":{"0":1,"62":2,"70":1,"72":1,"74":1,"135":1}}],["by",{"2":{"0":1,"8":2,"30":3,"49":1,"51":1,"53":3,"62":2,"70":2,"84":3,"92":1,"103":1,"113":3,"135":1,"137":1,"146":1,"219":1,"220":1}}],["pythonimport",{"2":{"194":1}}],["pytorch教程",{"0":{"192":1}}],["pytorch",{"2":{"184":1}}],["pytorch笔记",{"0":{"184":1}}],["pdf查看器用于在",{"2":{"182":1}}],["pdflatex",{"2":{"170":9,"171":13,"182":9}}],["pdf",{"0":{"181":1},"2":{"170":5,"171":7,"174":4,"175":5,"178":8,"179":16,"180":4,"182":15}}],["pfa",{"2":{"126":1,"127":1}}],["pfenet等",{"2":{"95":1}}],["publicly",{"2":{"103":2}}],["pp",{"2":{"101":1,"103":1}}],["p",{"2":{"101":1,"164":1,"182":1}}],["pc",{"2":{"101":2}}],["pce",{"2":{"26":1}}],["plain",{"2":{"101":1}}],["ptc",{"2":{"78":1}}],["pmg可以将目标对象清晰地划分为不同的互补部分区域",{"2":{"72":1}}],["pmg",{"2":{"72":5}}],["phi2",{"2":{"59":1}}],["pq",{"2":{"51":1}}],["powered",{"2":{"220":1}}],["power",{"2":{"135":1}}],["powerful",{"2":{"72":1,"84":1}}],["pose",{"2":{"113":1}}],["poses",{"2":{"74":1}}],["posed",{"2":{"51":1}}],["points",{"2":{"72":1}}],["popular",{"2":{"18":1,"30":1,"40":1,"53":1}}],["pipeline",{"2":{"155":1,"159":1}}],["ping",{"2":{"51":1}}],["pixelclip可应用于现有利用clip进行零样本掩码分类的框架",{"2":{"153":1}}],["pixelclip可以直接应用于现有的使用clip作为零样本掩码分类器的框架",{"2":{"151":1}}],["pixelclip通过利用无标签图像和掩码微调预训练视觉",{"2":{"153":1}}],["pixelclip在不使用语义标签的情况下表现出竞争力",{"2":{"151":1}}],["pixelclip在所有基准测试中均显著优于clip",{"2":{"151":1}}],["pixelclip表现出了显著的性能改进和竞争结果",{"2":{"147":1}}],["pixelclip",{"2":{"146":3,"147":2}}],["pixelnerf",{"2":{"51":1}}],["pixel",{"2":{"30":1,"74":1,"92":1,"101":2,"113":1,"137":1,"146":2,"191":1}}],["pixels",{"2":{"30":3,"101":1,"113":1,"126":3}}],["pst900",{"2":{"89":1}}],["pspnet",{"2":{"51":1}}],["ps",{"2":{"33":1}}],["psi",{"2":{"26":1}}],["pseudo",{"2":{"18":1,"30":1,"113":4}}],["peft",{"2":{"49":3}}],["per",{"2":{"51":1,"101":1,"113":1}}],["performed",{"2":{"101":1}}],["perform",{"2":{"30":1,"84":1,"137":1}}],["performant",{"2":{"30":1}}],["performance",{"2":{"18":3,"30":1,"40":1,"62":1,"72":1,"84":1,"92":1,"103":3,"126":2,"135":1,"137":1,"146":1,"155":1}}],["perception",{"2":{"8":1,"72":1}}],["peking",{"2":{"17":1,"112":1}}],["padding=",{"2":{"194":13}}],["padding=0",{"2":{"193":2,"194":5}}],["padding",{"2":{"193":6}}],["pacc=∑inii∑iti该指标表示所有像素中分类正确的比例",{"2":{"191":1}}],["package",{"2":{"170":1,"171":1,"182":1}}],["pack",{"2":{"166":1}}],["panoptic",{"2":{"109":1}}],["panzhiyi",{"2":{"18":1}}],["pasca",{"2":{"101":1}}],["pascal数据集",{"0":{"202":1}}],["pascal−5i",{"2":{"92":1}}],["pascal",{"2":{"8":1,"14":1,"26":2,"30":1,"31":1,"35":2,"45":1,"53":1,"58":4,"60":1,"74":1,"75":1,"79":2,"93":1,"97":2,"101":4,"103":1,"104":1,"109":2,"118":1,"126":1,"127":1,"131":1,"137":2,"138":2,"141":1,"144":1,"160":1,"162":1}}],["pas",{"2":{"101":1}}],["pat在三个流行的fss基准测试中创造了新的最优性能",{"2":{"72":1}}],["pat在四个任务中表现优异",{"2":{"72":1}}],["pat与其他fss方法的结合",{"2":{"72":1}}],["pat取得了最佳的分割性能",{"2":{"72":1}}],["pat",{"2":{"72":6}}],["patterns",{"2":{"62":2,"84":1}}],["pattern",{"2":{"62":1,"72":1}}],["patch",{"2":{"51":1,"141":1}}],["patnet等",{"2":{"43":1}}],["parser",{"2":{"70":1}}],["parse",{"0":{"70":1},"2":{"70":2}}],["paragraphs",{"2":{"103":1}}],["paradigm",{"2":{"53":1,"70":1,"72":1,"84":1}}],["parameter",{"2":{"49":2}}],["parameters",{"2":{"49":3,"70":1,"84":3}}],["part",{"2":{"72":2,"155":1}}],["partial",{"2":{"130":1}}],["partitioning",{"2":{"155":1}}],["partition",{"2":{"53":1,"74":1}}],["particularly",{"2":{"53":1}}],["particular",{"2":{"8":1}}],["parts",{"2":{"18":1}}],["pairwise",{"2":{"30":1}}],["pairs",{"2":{"30":1,"155":1}}],["paper",{"2":{"18":1,"30":1,"40":1,"49":1,"53":1,"72":1,"101":1,"126":1,"155":1}}],["page",{"0":{"3":1,"4":1},"2":{"0":7,"3":1,"146":1,"219":1}}],["print",{"2":{"194":1}}],["privacy",{"2":{"135":1}}],["primarily",{"2":{"113":1}}],["primary",{"2":{"92":1}}],["priors",{"2":{"53":2}}],["prior",{"2":{"53":1,"57":2,"70":2,"135":1}}],["practical",{"2":{"62":1}}],["predefined",{"2":{"103":1}}],["predict",{"2":{"53":1,"101":1}}],["prediction",{"2":{"40":2}}],["predictions",{"2":{"18":4,"70":1}}],["preserve",{"2":{"74":1}}],["present",{"2":{"51":1,"146":1}}],["presents",{"2":{"30":1,"126":1}}],["precision",{"2":{"74":1}}],["precise",{"2":{"51":1,"70":1,"155":1}}],["precisely",{"2":{"49":1,"72":1}}],["pretrained",{"2":{"72":1}}],["pre",{"2":{"49":1,"53":3,"84":2,"101":1,"137":3}}],["prevent",{"2":{"40":1}}],["previous",{"2":{"8":1,"30":2,"40":1,"53":2,"92":1,"113":1}}],["pre>",{"2":{"0":6}}],["project",{"2":{"135":1,"146":1}}],["progressive",{"0":{"73":1},"1":{"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1}}],["promotion",{"2":{"113":2,"114":1}}],["promoting",{"0":{"112":1},"1":{"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1}}],["promptable",{"2":{"135":2}}],["prompter",{"2":{"84":2,"88":2,"90":2}}],["prompts",{"2":{"72":3,"84":1}}],["prompting",{"0":{"82":1},"1":{"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1},"2":{"72":2}}],["prompt",{"0":{"71":1},"1":{"72":1},"2":{"72":5,"84":3,"88":1}}],["promising",{"2":{"18":1}}],["prostate",{"2":{"62":1}}],["probing",{"2":{"137":1}}],["problem",{"2":{"62":1}}],["probability",{"2":{"18":1}}],["pro",{"2":{"51":1,"113":1}}],["processing",{"2":{"70":1,"74":1}}],["process",{"2":{"49":1,"113":1}}],["procedure",{"2":{"18":1}}],["provides",{"2":{"220":1}}],["provide",{"2":{"53":1}}],["provided",{"2":{"0":1,"219":1}}],["proven",{"2":{"40":1}}],["properties",{"2":{"137":1}}],["propagates",{"2":{"126":1}}],["propagation",{"0":{"29":1},"1":{"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1},"2":{"30":3,"126":1,"137":1}}],["propaga",{"2":{"84":1}}],["proposing",{"2":{"70":1,"92":1}}],["proposes",{"2":{"72":1,"126":1}}],["proposed",{"2":{"18":1,"53":1,"62":2,"70":1,"74":1,"92":1}}],["propose",{"2":{"8":1,"18":1,"30":1,"40":3,"51":1,"53":1,"62":1,"74":2,"84":1,"92":2,"101":1,"103":1,"146":1}}],["producing",{"2":{"18":1}}],["produces",{"2":{"49":1}}],["produce",{"2":{"18":2}}],["prototypes",{"2":{"92":1,"126":1,"127":1}}],["prototype",{"0":{"6":1,"125":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1},"2":{"8":1,"12":1,"92":1,"126":1,"127":1}}]],"serializationVersion":2}';export{e as default};
