import{_ as n,c as o,I as s,j as t,a,a8 as i,D as r,o as g}from"./chunks/framework.CLo04awk.js";const f=JSON.parse(`{"title":"Segment Anything","description":"","frontmatter":{"head":[["script",{"charset":"UTF-8","id":"LA_COLLECT","src":"//sdk.51.la/js-sdk-pro.min.js"}],["script",{},"typeof LA !== 'undefined' && LA.init({\\"id\\":\\"3LPXyA1ZitpV3O1s\\",\\"ck\\":\\"3LPXyA1ZitpV3O1s\\",\\"autoTrack\\":true,\\"hashMode\\":true})"]]},"headers":[],"relativePath":"column/Paper/Segment Anything.md","filePath":"column/Paper/Segment Anything.md","lastUpdated":1742900169000}`),l={name:"column/Paper/Segment Anything.md"},p=t("h1",{id:"segment-anything",tabindex:"-1"},[a("Segment Anything "),t("a",{class:"header-anchor",href:"#segment-anything","aria-label":'Permalink to "Segment Anything"'},"​")],-1),c=i('<h2 id="meta-ai" tabindex="-1"><strong>Meta AI</strong> <a class="header-anchor" href="#meta-ai" aria-label="Permalink to &quot;**Meta AI**&quot;">​</a></h2><p><a href="https://segment-anything.com" target="_blank" rel="noreferrer">https://segment-anything.com</a></p><blockquote><p>摘要：We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and cor-responding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision</p></blockquote><blockquote><p>翻译：我们推出了Segment Anything (SA)项目：一个全新的图像分割任务、模型和数据集。通过使用我们高效的模型进行数据收集，我们成功构建了迄今为止最大的图像分割数据集，包含超过10亿个分割掩码和1100万张符合许可且尊重隐私的图像。这个模型被特别设计并训练为能够接受简单提示，因此它可以零样本迁移到不同的图像类型和任务。我们在多个任务中测试了该模型，发现它的零样本表现非常优秀——通常与之前的完全监督方法相当，甚至在某些情况下表现更好。我们将在segment-anything.com网站发布Segment Anything Model (SAM)和对应的数据集(SA-1B)，该数据集包括10亿个掩码和1100万张图像，旨在推动计算机视觉领域的基础模型研究。</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-28-44.png" alt="Snipaste_2025-02-25_20-28-44" loading="lazy"></p><p><strong>task, model, dataset, data engine, experiments, responsible AI, release</strong></p><ul><li><p>What <strong>task</strong> will enable zero-shot generalization?</p></li><li><p>What is the corresponding <strong>model</strong> architecture?</p></li><li><p>What <strong>data</strong> can power this task and model?</p></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-37-11.png" alt="Snipaste_2025-02-25_20-37-11" loading="lazy"></p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-39-37.png" alt="Snipaste_2025-02-25_20-39-37"><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-25_20-40-29.png" alt="Snipaste_2025-02-25_20-40-29" loading="lazy"></p><blockquote><p>**研究背景：**本文聚焦于构建图像分割基础模型，其研究背景主要源于自然语言处理（NLP）和计算机视觉领域的发展现状与需求。</p><p>在NLP中，基于大规模网络数据集预训练的大语言模型展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布，且性能随模型规模、数据集大小和训练计算量的增加而提升。</p><p>计算机视觉领域虽也对基础模型有所探索，如CLIP和ALIGN利用对比学习训练文本和图像编码器实现零样本泛化，但计算机视觉问题广泛，许多问题缺乏充足的训练数据。</p><p>在图像分割方面，尚无网络规模的数据源，且现有方法难以实现强大的泛化能力。因此，本文旨在开发一个可提示的模型，并在广泛的数据集上进行预训练，以解决新数据分布下的一系列下游分割问题。具体通过定义可提示的分割任务、设计相应的模型架构（SAM）以及构建数据引擎收集大规模数据集（SA - 1B）来实现这一目标，从而推动图像分割进入基础模型时代。</p></blockquote><p><strong>研究现状：</strong></p><ul><li><strong>基础模型发展</strong>：大语言模型在自然语言处理（NLP）领域展现出强大的零样本和少样本泛化能力，通过提示工程可适应多种任务和数据分布。计算机视觉领域也在探索基础模型，如CLIP和ALIGN利用对比学习训练文本和图像编码器，实现零样本泛化。</li><li><strong>图像分割任务</strong>：图像分割领域存在多种任务，如交互式分割、边缘检测、实例分割等，但缺乏大规模、多样化的分割数据集，且现有模型在泛化能力和处理模糊提示方面存在不足。</li></ul><p><strong>创新点：</strong></p><ol><li><strong>任务创新</strong>：提出可提示分割任务，能作为预训练目标，通过提示工程实现零样本迁移到下游分割任务。<strong>（Task）</strong></li><li><strong>模型创新</strong>：设计Segment Anything Model（SAM），由图像编码器、提示编码器和掩码解码器组成，支持灵活提示、实时计算且能处理歧义。<strong>（Model）</strong></li><li><strong>数据创新</strong>：构建数据引擎收集SA - 1B数据集，含超10亿掩码，数量和质量远超现有数据集，为模型训练提供强大支撑。<strong>（Data）</strong></li></ol><p><strong>最后提出本文的不足：</strong></p><ol><li><strong>细节处理欠佳</strong>：SAM可能会遗漏图像中的精细结构，有时会生成小的、不相连的虚假组件，且生成的边界不如一些计算密集型的“放大”方法清晰。</li><li><strong>特定场景表现弱</strong>：在提供多个提示点时，专门的交互式分割方法通常会优于SAM，因为SAM更侧重于通用性和广泛适用性，而非高IoU的交互式分割。</li><li><strong>文本到掩码任务待完善</strong>：文本到掩码任务的探索还不够成熟，不够稳健，需要更多的努力来改进。</li><li><strong>特定提示设计困难</strong>：目前尚不清楚如何设计简单的提示来实现语义和全景分割。</li><li><strong>特定领域表现不佳</strong>：在特定领域，一些专门的工具可能会比SAM表现更好。</li></ol><blockquote><p>写作启发：<strong>Promptable Segment（提示词分割</strong>），<strong>Foundation models（基础模型）</strong>，<strong>数据集的创新</strong></p></blockquote>',18);function d(m,h,u,_,A,S){const e=r("ArticleMetadata");return g(),o("div",null,[p,s(e),c])}const k=n(l,[["render",d]]);export{f as __pageData,k as default};
