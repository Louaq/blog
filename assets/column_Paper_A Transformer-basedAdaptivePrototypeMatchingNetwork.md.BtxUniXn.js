import{_ as r,c as a,I as i,j as t,a as n,a8 as e,D as l,o}from"./chunks/framework.CLo04awk.js";const D=JSON.parse(`{"title":"A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation","description":"","frontmatter":{"head":[["script",{"charset":"UTF-8","id":"LA_COLLECT","src":"//sdk.51.la/js-sdk-pro.min.js"}],["script",{},"typeof LA !== 'undefined' && LA.init({\\"id\\":\\"3LPXyA1ZitpV3O1s\\",\\"ck\\":\\"3LPXyA1ZitpV3O1s\\",\\"autoTrack\\":true,\\"hashMode\\":true})"]]},"headers":[],"relativePath":"column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.md","filePath":"column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.md","lastUpdated":1742987500000}`),d={name:"column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.md"},h=t("h1",{id:"a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation",tabindex:"-1"},[n("A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation "),t("a",{class:"header-anchor",href:"#a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation","aria-label":'Permalink to "A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation"'},"​")],-1),c=e('<h2 id="南京信息工程大学、青海师范大学、澳门大学、中国科学院" tabindex="-1">南京信息工程大学、青海师范大学、澳门大学、中国科学院 💯 <a class="header-anchor" href="#南京信息工程大学、青海师范大学、澳门大学、中国科学院" aria-label="Permalink to &quot;南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:&quot;">​</a></h2><h2 id="摘要" tabindex="-1"><strong>摘要：</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要：**&quot;">​</a></h2><blockquote><p><strong>Few-shot semantic segmentation (FSS)</strong> aims to generate a model for segmenting novel classes using a limited number of annotated samples. Previous FSS methods have shown sensitivity to background noise due to inherent bias, attention bias, and spatial-aware bias. In this study, we propose a <strong>Transformer-Based Adaptive Prototype Matching Network</strong> to establish robust matching relationships by improving the semantic and spatial perception of query features. The model includes three modules: <strong>target enhancement module (TEM)</strong>, <strong>dual constraint aggregation module (DCAM)</strong>, and <strong>dual classification module (DCM)</strong>. In particular, TEM mitigates inherent bias by exploring the relevance of multi-scale local context to enhance foreground features. Then, DCAM addresses attention bias through the dual semantic-aware attention mechanism to strengthen constraints. Finally, the DCM module decouples the segmentation task into semantic alignment and spatial alignment to alleviate spatial-aware bias. Extensive experiments on <strong>PASCAL-5i</strong> and <strong>COCO-20i</strong> confirm the effectiveness of our approach.</p></blockquote><h2 id="翻译" tabindex="-1"><strong>翻译：</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译：**&quot;">​</a></h2><blockquote><p>Few-shot语义分割（FSS）旨在通过少量的标注样本为新的类别生成一个分割模型。以往的FSS方法由于固有偏差、注意力偏差和空间感知偏差，往往对背景噪声过于敏感。在本研究中，我们提出了一种基于Transformer的自适应原型匹配网络，通过增强查询特征的语义和空间感知能力，建立更为稳定的匹配关系。该模型包含三个模块：目标增强模块（TEM）、双重约束聚合模块（DCAM）和双重分类模块（DCM）。其中，TEM通过探索多尺度局部上下文的相关性，增强前景特征，从而减轻固有的偏差。接着，DCAM通过双重语义感知注意力机制解决了注意力偏差问题，强化了约束效果。最后，DCM模块将分割任务拆解为语义对齐和空间对齐，帮助缓解空间感知偏差。我们在PASCAL-5i和COCO-20i数据集上进行了大量实验，验证了该方法的有效性。</p></blockquote><h2 id="研究背景" tabindex="-1"><strong>研究背景：</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景：**&quot;">​</a></h2><p>近年来，由于深度学习在计算机视觉领域的快速发展，所以传统的语义分割取得了飞速进步。在这种情况下，少样本分割(few-shot segmentation, FSS)被提出用于模拟有限数据和多类别的真实世界场景。</p><p>FSS遵循元学习框架，执行过程分特征提取、匹配和分类三个阶段。现有FSS模型虽有成果，但受背景干扰，存在三方面问题：一是特征提取阶段，预训练骨干网络有固有偏差，易优先提取无关特征；二是特征匹配阶段，注意力机制在目标类别内差异大时，会导致注意力偏差；三是分类阶段，现有方法多依赖语义相关性，忽略空间信息，产生空间感知偏差。</p><p>基于上述问题，作者提出一种基于Transformer的自适应原型匹配网络，通过在模型执行的三个阶段进行策略性和高效交互，减轻FSS中的背景干扰，利用查询特征的语义和空间感知，增强模型的鲁棒性，以解决现有FSS模型存在的问题。</p><h2 id="研究现状" tabindex="-1"><strong>研究现状：</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状：**&quot;">​</a></h2><ul><li><strong>Few - Shot Semantic Segmentation（FSS）</strong>：FSS旨在用<strong>少量标注样本</strong>为新类别生成分割模型，基于度量学习的FSS主要分为基于原型和基于像素匹配两类方法。<strong>基于原型的方法</strong>用原型代表目标类信息进行匹配预测；<strong>基于像素匹配</strong>的方法建立支持像素和查询像素的密集关联。</li><li><strong>Transformer应用</strong>：Transformer因能捕捉长距离相关性，在FSS中得到应用，如动态调整分类器权重、过滤无关像素、聚合多级别支持掩码等。</li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型：</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型：**&quot;">​</a></h2><p>本文提出了一种基于Transformer的自适应原型匹配网络（Transformer - Based Adaptive Prototype Matching Network），用于<strong>少样本语义分割（Few - Shot Semantic Segmentation，FSS）<strong>任务，以解决现有FSS模型存在的</strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的对背景噪声敏感的问题。该模型主要包含以下三个模块：</p><ol><li><blockquote><p><strong>目标增强模块（Target Enhancement Module，TEM）</strong> <strong>设计目的</strong>：缓解骨干网络的<strong>固有偏差</strong>，增强前景特征。在特征提取阶段，以往工作依赖预训练骨干网络直接提取的特征，存在固有偏差，倾向于提取与当前任务无关的特征。 <strong>具体方法</strong>：引入基于卷积Transformer架构的多尺度局部感知调制Transformer进行多尺度特征提取，采用多尺度自适应局部注意力增强前景信息、减轻背景干扰；用可逆神经网络（INN）替代标准多层感知器（MLP），在前馈过程中保留更细粒度的特征。</p></blockquote></li><li><blockquote><p><strong>双约束聚合模块（Dual Constraint Aggregation Module，DCAM）</strong> <strong>设计目的</strong>：解决特征匹配阶段的<strong>注意力偏差</strong>问题。现有方法利用单层注意力机制建立支持集和查询集的关系，在目标类别存在显著类内差异时，这种关系不足以准确匹配，导致注意力偏差。 <strong>具体方法</strong>：由类内差异表示和双语义感知注意力机制两个关键部分组成。类内差异表示利用一组可学习向量建模支持集和查询集之间的差异；双语义感知注意力机制通过两层约束，先以支持原型为参考在查询特征中选择匹配置信度高的点，再以此为指导在整个查询特征图中寻找特征相似度高的点，生成鲁棒的支持类别原型。</p></blockquote></li><li><blockquote><p><strong>双分类模块（Dual Classification Module，DCM）</strong> <strong>设计目的</strong>：解决特征分类阶段的<strong>空间感知偏差</strong>问题。现有方法主要基于语义一致性进行预测，忽略了目标对象的空间一致性，导致难以准确定位目标类别。 <strong>具体方法</strong>：将分割任务解耦为语义对齐和空间对齐两个子任务。通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别；利用查询特征的内在引导，挖掘目标对象自身的空间一致性，得到基于空间分布概率的掩码用于精确的定位，最后将两个掩码相加得到最终的查询前景分割图。 实验结果表明，该模型在PASCAL - 5i和COCO - 20i两个基准数据集上取得了优于现有方法的性能，且参数数量较少。</p></blockquote></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" alt="Snipaste_2025-03-08_10-36-07" loading="lazy"></p><h2 id="实验-compared-with-the-state-of-the-art-models-and-ablation-experiments" tabindex="-1"><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong> <a class="header-anchor" href="#实验-compared-with-the-state-of-the-art-models-and-ablation-experiments" aria-label="Permalink to &quot;**实验（compared with the state-of-the-art models and ablation experiments）**&quot;">​</a></h2><ul><li><h3 id="comparison-with-the-state-of-the-arts" tabindex="-1"><strong>Comparison with the State-of-the-Arts</strong> <a class="header-anchor" href="#comparison-with-the-state-of-the-arts" aria-label="Permalink to &quot;**Comparison with the State-of-the-Arts**&quot;">​</a></h3></li></ul>',17),m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},g={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"0.74ex",height:"1.879ex",role:"img",focusable:"false",viewBox:"0 -830.4 327 830.4","aria-hidden":"true"},p=e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mi" transform="translate(33,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),u=[p],T=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mrow",{"data-mjx-texclass":"ORD"},[t("msup",null,[t("mi"),t("mi",null,"i")])])])],-1),_={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},f={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"0.74ex",height:"1.879ex",role:"img",focusable:"false",viewBox:"0 -830.4 327 830.4","aria-hidden":"true"},S=e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mi" transform="translate(33,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),x=[S],b=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mrow",{"data-mjx-texclass":"ORD"},[t("msup",null,[t("mi"),t("mi",null,"i")])])])],-1),w=e('<p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" alt="Snipaste_2025-03-08_10-39-17" loading="lazy"></p><ul><li><h3 id="ablation-experiments" tabindex="-1"><strong>ablation experiments</strong> <a class="header-anchor" href="#ablation-experiments" aria-label="Permalink to &quot;**ablation experiments**&quot;">​</a></h3></li></ul><ol><li><strong>组件分析</strong>：该方法包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）三个主要模块。与基线相比，单独使用TEM增强查询前景特征可使性能提升0.9%，单独使用DCAM增强类别原型的判别能力可提升2.2%，TEM和DCAM协同作用可提升2.6%，使用DCM实现语义对齐和空间对齐可额外提升1.3%。模型整体比基线提升了3.9%，表明引入的模块有效解决了固有偏差、注意力偏差和空间感知偏差问题，减少了背景干扰，实现了精确分割。</li><li><strong>目标增强模块（TEM）</strong>：TEM旨在减轻骨干网络的固有偏差并增强查询前景区域。通过与其他方法在计算量和准确性方面进行对比实验，包括采用自对齐模块（SA）、卷积变压器架构（SAM）、多尺度自适应局部注意力（MSLA + MLP）以及用可逆神经网络（INN）代替多层感知器（MLP）作为前馈网络（MSLA + INN）。结果表明，该方法在降低计算复杂度的同时保持了较高的准确性，且前馈网络在略微增加计算成本的情况下保留了更多特征细节。</li><li><strong>双约束聚合模块（DCAM）</strong>：对DCAM中的关键组件进行了全面分析，通过修改模型采用不同的注意力机制，如原始的普通注意力（VA）、掩码注意力（MA）、双语义感知注意力（DSAA）和类内差异表示（IDR）。结果显示，使用掩码注意力减轻背景噪声干扰对性能提升影响不大，因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战。而双语义感知注意力机制通过可学习的方式减轻背景干扰，能应对类内差异的敏感性，类内差异表示在三种不同的注意力机制中都有益。</li><li><strong>双分类模块（DCM）</strong>：通过消融实验评估不同的DCM组件。仅使用基于语义相似度的掩码可使模型性能提升1.5%，证明了优化类别原型和查询特征的必要性；仅使用基于空间分布概率的分割图时，性能下降2.3%，这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域，导致对未知类别的分割失败。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" alt="Snipaste_2025-03-08_10-41-19" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" alt="Snipaste_2025-03-08_10-41-25" loading="lazy"></p><h2 id="结论" tabindex="-1"><strong>结论：</strong> <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;**结论：**&quot;">​</a></h2><blockquote><p>作者提出了一种<strong>基于Transformer的自适应原型匹配网络</strong>，以应对少样本语义分割（FSS）中<strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的背景干扰问题。该网络包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）。TEM通过多尺度局部上下文相关性增强前景特征，解决固有偏差；DCAM利用双语义感知注意力机制加强约束，处理注意力偏差；DCM将分割任务解耦为语义对齐和空间对齐，缓解空间感知偏差。实验表明，该方法在PASCAL - 5i和COCO - 20i数据集上以最少的参数达到了最先进的性能，有效减少了背景干扰，实现了精确分割。</p></blockquote>',7);function A(M,C,k,y,P,v){const s=l("ArticleMetadata");return o(),a("div",null,[h,i(s),c,t("p",null,[n("数据集：PASCAL-5"),t("mjx-container",m,[(o(),a("svg",g,u)),T]),n("，COCO-20"),t("mjx-container",_,[(o(),a("svg",f,x)),b])]),w])}const q=r(d,[["render",A]]);export{D as __pageData,q as default};
