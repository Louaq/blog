 # BiFormer

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

**BiFormer**æ˜¯ä¸€ç§ç»“åˆäº†Bi-level Routing Attentionçš„è§†è§‰**Transformeræ¨¡å‹**ï¼ŒBiFormeræ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯å¼•å…¥äº†åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨BiFormerä¸­ï¼Œæ¯ä¸ªå›¾åƒå—éƒ½ä¸ä¸€ä¸ªä½ç½®è·¯ç”±å™¨ç›¸å…³è”ã€‚è¿™äº›ä½ç½®è·¯ç”±å™¨æ ¹æ®ç‰¹å®šçš„è§„åˆ™å°†å›¾åƒå—åˆ†é…ç»™ä¸Šå±‚å’Œä¸‹å±‚è·¯ç”±å™¨ã€‚ä¸Šå±‚è·¯ç”±å™¨è´Ÿè´£æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œä¸‹å±‚è·¯ç”±å™¨åˆ™è´Ÿè´£æ•æ‰å±€éƒ¨åŒºåŸŸçš„ç»†èŠ‚ã€‚

å…·ä½“æ¥è¯´ï¼Œä¸Šå±‚è·¯ç”±å™¨é€šè¿‡å…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¹æ‰€æœ‰å›¾åƒå—è¿›è¡Œäº¤äº’ï¼Œå¹¶ç”Ÿæˆå…¨å±€å›¾åƒè¡¨ç¤ºã€‚ä¸‹å±‚è·¯ç”±å™¨åˆ™ä½¿ç”¨å±€éƒ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯ä¸ªå›¾åƒå—ä¸å…¶é‚»è¿‘çš„å›¾åƒå—è¿›è¡Œäº¤äº’ï¼Œå¹¶ç”Ÿæˆå±€éƒ¨å›¾åƒè¡¨ç¤ºã€‚é€šè¿‡è¿™ç§åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼ŒBiFormerèƒ½å¤ŸåŒæ—¶æ•æ‰å…¨å±€å’Œå±€éƒ¨çš„ç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

äºŒã€Biformerçš„ä½œç”¨æœºåˆ¶
---------------

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/9889f7327642442eb5be90a357ca5d80.png)

**è®ºæ–‡åœ°å€**:Â [Biformerè®ºæ–‡åœ°å€CSDN](https://download.csdn.net/download/java1314777/88496483 "Â Biformerè®ºæ–‡åœ°å€CSDN")

**ä»£ç åœ°å€:**[https://github.com/rayleizhu/BiFormer](https://github.com/rayleizhu/BiFormer/ "https://github.com/rayleizhu/BiFormer")

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6baf13284e9f455e8722327d73fcbde2.png)

åœ¨å¼€å§‹ä»‹ç»ä½œç”¨æœºåˆ¶ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹**ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆæœ**ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/aff96e1ca34f43e68c02beef24fccd40.png)

ä»a-fåˆ†åˆ«æ˜¯ï¼š

> **(a) åŸå§‹æ³¨æ„åŠ›**ï¼šå…¨å±€æ“ä½œï¼Œä¼šäº§ç”Ÿé«˜è®¡ç®—å¤æ‚åº¦å’Œå¤§å†…å­˜å ç”¨ã€‚
> 
> **(b)-(d) ç¨€ç–æ³¨æ„åŠ›**ï¼šä¸ºäº†å‡å°‘æ³¨æ„åŠ›çš„å¤æ‚åº¦ï¼Œä¸€äº›æ–¹æ³•å¼•å…¥äº†ç¨€ç–æ¨¡å¼ï¼Œå¦‚å±€éƒ¨çª—å£ã€è½´å‘æ¡çº¹å’Œæ‰©å¼ çª—å£ã€‚è¿™äº›æ¨¡å¼å°†æ³¨æ„åŠ›é™åˆ¶åœ¨ç‰¹å®šåŒºåŸŸï¼Œå‡å°‘äº†è€ƒè™‘çš„é”®-å€¼å¯¹æ•°é‡ã€‚
> 
> **(e) å¯å˜å½¢æ³¨æ„åŠ›**ï¼šå¯å˜å½¢æ³¨æ„åŠ›é€šè¿‡æ”¹å˜è§„åˆ™ç½‘æ ¼æ¥å®ç°å›¾åƒè‡ªé€‚åº”çš„ç¨€ç–æ€§ã€‚è¿™ä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥é›†ä¸­å…³æ³¨è¾“å…¥å›¾åƒçš„ä¸åŒåŒºåŸŸã€‚
> 
> **(f) åŒå±‚è·¯ç”±æ³¨æ„åŠ›**ï¼šæ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡åŒå±‚è·¯ç”±å®ç°äº†åŠ¨æ€çš„ã€æŸ¥è¯¢æ„ŸçŸ¥çš„ç¨€ç–æ€§ã€‚é¦–å…ˆç¡®å®šäº†å‰kä¸ªï¼ˆæœ¬ä¾‹ä¸­k=3ï¼‰ç›¸å…³åŒºåŸŸï¼Œç„¶åå…³æ³¨å®ƒä»¬çš„å¹¶é›†ã€‚è¿™ä½¿å¾—æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªæŸ¥è¯¢è‡ªé€‚åº”åœ°å…³æ³¨æœ€æœ‰è¯­ä¹‰ç›¸å…³çš„é”®-å€¼å¯¹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è®¡ç®—ã€‚

ä¸‹é¢æ¥ä»‹ç»ä½œç”¨æœºåˆ¶Biformeræ˜¯ä¸€ç§**ç»“åˆäº†Bi-level Routing Attentionçš„è§†è§‰Transformeræ¨¡å‹**ï¼Œæ‰€ä»¥å®ƒå…·æœ‰Transformeræ¨¡å‹çš„ç‰¹æ€§ï¼Œå…¶ä¸æœ¬è´¨ä¸Šæ˜¯å±€éƒ¨æ“ä½œçš„å·ç§¯(Conv)ä¸åŒï¼Œæ³¨æ„åŠ›çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯å…¨å±€æ„Ÿå—é‡ï¼Œä½¿å¾—è§†è§‰Transformerèƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™ç§ç‰¹æ€§æ˜¯æœ‰ä»£ä»·çš„ï¼šç”±äºæ³¨æ„åŠ›åœ¨æ‰€æœ‰ç©ºé—´ä½ç½®ä¸Šè®¡ç®—ä»¤ç‰Œä¹‹é—´çš„å…³è”æ€§ï¼Œå®ƒå…·æœ‰è¾ƒé«˜çš„è®¡ç®—å¤æ‚åº¦å¹¶ä¸”éœ€è¦å¤§é‡çš„å†…å­˜ï¼Œæ‰€ä»¥æ•ˆç‡å¹¶ä¸é«˜ã€‚

ä¸ºäº†ä»¥é«˜æ•ˆçš„æ–¹å¼å…¨å±€å®šä½æœ‰ä»·å€¼çš„é”®-å€¼å¯¹è¿›è¡Œå…³æ³¨ï¼Œæå‡ºäº†ä¸€ç§åŒºåŸŸåˆ°åŒºåŸŸçš„è·¯ç”±æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ç²—ç²’åº¦çš„åŒºåŸŸçº§åˆ«ä¸Šè¿‡æ»¤æ‰æœ€ä¸ç›¸å…³çš„é”®-å€¼å¯¹ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨ç»†ç²’åº¦çš„æ ‡è®°çº§åˆ«ä¸Šè¿›è¡Œè¿‡æ»¤ã€‚é¦–å…ˆï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªåŒºåŸŸçº§åˆ«çš„äº²å’Œåº¦å›¾ï¼Œç„¶åå¯¹å…¶è¿›è¡Œä¿®å‰ªï¼Œä¿ç•™æ¯ä¸ªèŠ‚ç‚¹çš„å‰kä¸ªè¿æ¥ï¼Œä»è€Œå®ç°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œæ¯ä¸ªåŒºåŸŸåªéœ€è¦å…³æ³¨å‰kä¸ªè·¯ç”±çš„åŒºåŸŸã€‚ç¡®å®šäº†å…³æ³¨çš„åŒºåŸŸåï¼Œä¸‹ä¸€æ­¥æ˜¯åº”ç”¨æ ‡è®°åˆ°æ ‡è®°çš„æ³¨æ„åŠ›ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ­¥éª¤ï¼Œå› ä¸ºç°åœ¨å‡è®¾é”®-å€¼å¯¹åœ¨ç©ºé—´ä¸Šæ˜¯åˆ†æ•£çš„ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œè™½ç„¶ç¨€ç–çŸ©é˜µä¹˜æ³•æ˜¯é€‚ç”¨çš„ï¼Œä½†åœ¨ç°ä»£GPUä¸Šæ•ˆç‡è¾ƒä½ï¼Œå› ä¸ºç°ä»£GPUä¾èµ–äºè¿ç»­å†…å­˜æ“ä½œï¼Œå³ä¸€æ¬¡è®¿é—®å‡ åä¸ªè¿ç»­å­—èŠ‚çš„å—ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ”¶é›†é”®/å€¼æ ‡è®°æ¥å¤„ç†ï¼Œå…¶ä¸­åªæ¶‰åŠåˆ°å¯¹äºç¡¬ä»¶å‹å¥½çš„ç¨ å¯†çŸ©é˜µä¹˜æ³•ã€‚æˆ‘ä»¬å°†è¿™ç§æ–¹æ³•ç§°ä¸º**åŒå±‚è·¯ç”±æ³¨æ„åŠ›ï¼ˆBi-level Routing Attentionï¼Œç®€ç§°BRAï¼‰**ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸€ä¸ªåŒºåŸŸçº§åˆ«çš„è·¯ç”±æ­¥éª¤å’Œä¸€ä¸ªæ ‡è®°çº§åˆ«çš„æ³¨æ„åŠ›æ­¥éª¤ã€‚

> **æ€»ç»“**ï¼šå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŒå±‚è·¯ç”±æœºåˆ¶æ¥æ”¹è¿›ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é€‚åº”æŸ¥è¯¢å¹¶å®ç°å†…å®¹æ„ŸçŸ¥çš„ç¨€ç–æ¨¡å¼ã€‚åˆ©ç”¨åŒå±‚è·¯ç”±æ³¨æ„åŠ›ä½œä¸ºåŸºæœ¬æ„å»ºæ¨¡å—ï¼Œæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è§†è§‰Transformeræ¨¡å‹ï¼Œåä¸ºBiFormerã€‚åœ¨åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²åœ¨å†…çš„å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BiFormeråœ¨ç›¸ä¼¼çš„æ¨¡å‹å¤§å°ä¸‹æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚Â 

ä¸‰ã€Biformerçš„ä¼˜åŠ£åŠ¿
--------------

BiFormeræ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿å¦‚ä¸‹ï¼š

**ä¼˜åŠ¿ï¼š**  
**1\. é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½**ï¼šBiFormeråˆ©ç”¨åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æŸ¥è¯¢æ„ŸçŸ¥çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä»¥å†…å®¹æ„ŸçŸ¥çš„æ–¹å¼å…³æ³¨æœ€ç›¸å…³çš„é”®-å€¼å¯¹ï¼Œä»è€Œå®ç°ç¨€ç–æ€§ã€‚è¿™ç§ç¨€ç–æ€§å‡å°‘äº†è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œä½¿å¾—BiFormeråœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹èƒ½å¤Ÿå®ç°æ›´é«˜çš„è®¡ç®—æ€§èƒ½ï¼Œä¸‹é¢æˆ‘é€šè¿‡å›¾ç‰‡æ¥è¾…åŠ©å¤§å®¶ç†è§£è¿™ä¸€ä¼˜åŠ¿ï¼

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f0e94d90b204409a8c72f4563925e968.png)

> ä¸Šå›¾å±•ç¤ºäº†é€šè¿‡æ”¶é›†å‰kä¸ªç›¸å…³çª—å£ä¸­çš„é”®-å€¼å¯¹ï¼Œåˆ©ç”¨ç¨€ç–æ€§è·³è¿‡æœ€ä¸ç›¸å…³åŒºåŸŸçš„è®¡ç®—è¿‡ç¨‹ï¼Œåªè¿›è¡Œé€‚ç”¨äºGPUçš„å¯†é›†çŸ©é˜µä¹˜æ³•è¿ç®—ã€‚
> 
> åœ¨ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼šå¯¹æ‰€æœ‰çš„é”®-å€¼å¯¹è¿›è¡Œå…¨å±€çš„è®¡ç®—ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚è€Œåœ¨BiFormerä¸­ï¼Œé€šè¿‡åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œåªå…³æ³¨ä¸æŸ¥è¯¢ç›¸å…³çš„å‰kä¸ªçª—å£ï¼Œå¹¶ä¸”ä»…è¿›è¡Œé€‚ç”¨äºGPUçš„å¯†é›†çŸ©é˜µä¹˜æ³•è¿ç®—ã€‚
> 
> è¿™ç§åšæ³•åˆ©ç”¨äº†ç¨€ç–æ€§ï¼Œé¿å…äº†åœ¨æœ€ä¸ç›¸å…³çš„åŒºåŸŸè¿›è¡Œå†—ä½™è®¡ç®—ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚åªæœ‰ä¸æŸ¥è¯¢ç›¸å…³çš„é”®-å€¼å¯¹å‚ä¸åˆ°å¯†é›†çŸ©é˜µä¹˜æ³•è¿ç®—ä¸­ï¼Œå‡å°‘äº†è®¡ç®—é‡å’Œå†…å­˜å ç”¨ã€‚

**2\. æŸ¥è¯¢æ„ŸçŸ¥çš„è‡ªé€‚åº”æ€§**ï¼šBiFormerçš„åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹æ ¹æ®æ¯ä¸ªæŸ¥è¯¢è‡ªé€‚åº”åœ°å…³æ³¨æœ€ç›¸å…³çš„é”®-å€¼å¯¹ã€‚è¿™ç§è‡ªé€‚åº”æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¾“å…¥æ•°æ®çš„è¯­ä¹‰å…³è”ï¼Œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ€§èƒ½ã€‚

**åŠ£åŠ¿**ï¼š  
1\. å¯èƒ½å­˜åœ¨ä¿¡æ¯æŸå¤±ï¼šç”±äºBiFormeré‡‡ç”¨äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œåªå…³æ³¨æœ€ç›¸å…³çš„é”®-å€¼å¯¹ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æ¬¡è¦çš„æˆ–è¾ƒè¿œçš„å…³è”ä¿¡æ¯è¢«å¿½ç•¥ã€‚è¿™å¯èƒ½ä¼šåœ¨æŸäº›æƒ…å†µä¸‹å¯¼è‡´æ¨¡å‹æ€§èƒ½çš„ä¸‹é™ã€‚

2\. å‚æ•°è°ƒæ•´çš„æŒ‘æˆ˜ï¼šBiFormerçš„åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥äº†é¢å¤–çš„å‚æ•°å’Œè¶…å‚æ•°ï¼Œéœ€è¦è¿›è¡Œé€‚å½“çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚è¿™å¯èƒ½éœ€è¦æ›´å¤šçš„å®éªŒå’Œè°ƒè¯•å·¥ä½œï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„å‚æ•°é…ç½®ã€‚

> æ€»ä½“è€Œè¨€ï¼ŒBiFormerçš„æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½å’ŒæŸ¥è¯¢æ„ŸçŸ¥çš„è‡ªé€‚åº”æ€§ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„è§†è§‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œéœ€è¦åœ¨å…·ä½“ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¿›è¡Œé€‚å½“çš„å®éªŒå’Œè°ƒæ•´ï¼Œä»¥å‘æŒ¥å…¶æœ€ä½³æ€§èƒ½ã€‚

å››ã€Biformerçš„ç»“æ„
-------------

æˆ‘ä»¬é€šè¿‡ä¸‹å›¾æ¥çœ‹ä¸€ä¸‹Biformerçš„ç½‘ç»œç»“æ„Â 

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f230c9d2f3bf42e88ef747c31df9914a.png)

ä¸Šå›¾å±•ç¤ºäº†BiFormerçš„æ•´ä½“æ¶æ„å’Œä¸€ä¸ªBiFormerå—çš„è¯¦ç»†ä¿¡æ¯ã€‚

å·¦ä¾§ï¼šBiFormerçš„æ•´ä½“æ¶æ„ã€‚è¯¥æ¶æ„åŒ…æ‹¬å¤šä¸ªBiFormerå—çš„å †å ï¼Œå¹¶ä¸”æ ¹æ®å…·ä½“ä»»åŠ¡å’Œéœ€æ±‚å¯ä»¥è¿›è¡Œä¸åŒçš„é…ç½®ã€‚BiFormeré€šè¿‡å¼•å…¥åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æ¯ä¸ªå—ä¸­å®ç°å†…å®¹æ„ŸçŸ¥çš„ç¨€ç–æ€§ï¼Œä»è€Œæé«˜äº†è®¡ç®—æ€§èƒ½å’Œä»»åŠ¡è¡¨ç°ã€‚

å³ä¾§ï¼šBiFormerå—çš„è¯¦ç»†ä¿¡æ¯ã€‚BiFormerå—æ˜¯BiFormerçš„åŸºæœ¬æ„å»ºå•å…ƒï¼Œç”±å¤šä¸ªå­å±‚ç»„æˆã€‚å…¶ä¸­åŒ…æ‹¬è‡ªæ³¨æ„åŠ›å­å±‚ï¼ˆself-attentionï¼‰å’Œå‰é¦ˆç¥ç»ç½‘ç»œå­å±‚ï¼ˆfeed-forward neural networkï¼‰ã€‚è‡ªæ³¨æ„åŠ›å­å±‚ä½¿ç”¨åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®æŸ¥è¯¢è‡ªé€‚åº”åœ°å…³æ³¨æœ€ç›¸å…³çš„é”®-å€¼å¯¹ã€‚å‰é¦ˆç¥ç»ç½‘ç»œå­å±‚é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢å’Œç‰¹å¾æå–ã€‚è¿™æ ·çš„ç»„åˆä½¿å¾—BiFormerå…·å¤‡äº†é€‚åº”æ€§å’Œè¡¨è¾¾èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å‘æŒ¥ä¼˜å¼‚çš„æ€§èƒ½ã€‚

é€šè¿‡æ•´ä½“æ¶æ„å’ŒBiFormerå—çš„è®¾è®¡ï¼ŒBiFormerèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨åŒå±‚è·¯ç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å†…å®¹æ„ŸçŸ¥çš„ç¨€ç–æ€§ï¼Œå¹¶æä¾›çµæ´»æ€§å’Œå¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚

äº”ã€æ·»åŠ Biformeræ³¨æ„åŠ›æœºåˆ¶
-----------------

æˆ‘ä»¬éœ€è¦æ”¹åŠ¨çš„åœ°æ–¹æœ‰ä¸‰å¤„ï¼Œå³å¯åœ¨YOLOv8æ¨¡å‹ä¸­æ·»åŠ Biformeræ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è¿™ä¸ªæ·»åŠ çš„è¿‡ç¨‹ä¸­ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿä¸¾ä¸€åä¸‰ï¼Œå…¶å®ƒçš„æ³¨æ„åŠ›æœºåˆ¶æ·»åŠ çš„æ–¹å¼éƒ½å¤§è‡´ç›¸åŒã€‚

è¿™é‡Œå…ˆä»‹ç»æˆ‘ç”¨çš„YOLOv8æ¨¡å‹ç‰ˆæœ¬,å› ä¸ºä¸åŒçš„YOLOv8æ¨¡å‹ç‰ˆæœ¬å¯èƒ½ç›®å½•ç»“æ„ä¸åŒã€‚

### æ­¥éª¤ä¸€Â 

è¿™é‡Œæ­£å¼å¼€å§‹æ·»åŠ Biformeræ³¨æ„åŠ›æœºåˆ¶ï¼Œé¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°è¯¥ç›®å½•'ultralytics/nn/modules'è¯¥ç›®å½•çš„æ„é€ å¦‚ä¸‹->

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a03d6ec7b75a477fa76c753bdfcce0b0.png)

æˆ‘ä»¬åœ¨å…¶ä¸­åˆ›å»ºä¸€ä¸ªåå­—ä¸ºBiformerçš„pyæ–‡ä»¶å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­å¤åˆ¶å¦‚ä¸‹ä»£ç å³å¯

```python
"""
Bi-Level Routing Attention.
"""
from typing import Tuple, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from torch import Tensor, LongTensor
 
__all__ = ['BiLevelRoutingAttention']
 
class TopkRouting(nn.Module):
    """
    differentiable topk routing with scaling
    Args:
        qk_dim: int, feature dimension of query and key
        topk: int, the 'topk'
        qk_scale: int or None, temperature (multiply) of softmax activation
        with_param: bool, wether inorporate learnable params in routing unit
        diff_routing: bool, wether make routing differentiable
        soft_routing: bool, wether make output value multiplied by routing weights
    """
 
    def __init__(self, qk_dim, topk=4, qk_scale=None, param_routing=False, diff_routing=False):
        super().__init__()
        self.topk = topk
        self.qk_dim = qk_dim
        self.scale = qk_scale or qk_dim ** -0.5
        self.diff_routing = diff_routing
        # TODO: norm layer before/after linear?
        self.emb = nn.Linear(qk_dim, qk_dim) if param_routing else nn.Identity()
        # routing activation
        self.routing_act = nn.Softmax(dim=-1)
 
    def forward(self, query: Tensor, key: Tensor) -> Tuple[Tensor]:
        """
        Args:
            q, k: (n, p^2, c) tensor
        Return:
            r_weight, topk_index: (n, p^2, topk) tensor
        """
        if not self.diff_routing:
            query, key = query.detach(), key.detach()
        query_hat, key_hat = self.emb(query), self.emb(key)  # per-window pooling -> (n, p^2, c)
        attn_logit = (query_hat * self.scale) @ key_hat.transpose(-2, -1)  # (n, p^2, p^2)
        topk_attn_logit, topk_index = torch.topk(attn_logit, k=self.topk, dim=-1)  # (n, p^2, k), (n, p^2, k)
        r_weight = self.routing_act(topk_attn_logit)  # (n, p^2, k)
 
        return r_weight, topk_index
 
 
class KVGather(nn.Module):
    def __init__(self, mul_weight='none'):
        super().__init__()
        assert mul_weight in ['none', 'soft', 'hard']
        self.mul_weight = mul_weight
 
    def forward(self, r_idx: Tensor, r_weight: Tensor, kv: Tensor):
        """
        r_idx: (n, p^2, topk) tensor
        r_weight: (n, p^2, topk) tensor
        kv: (n, p^2, w^2, c_kq+c_v)
        Return:
            (n, p^2, topk, w^2, c_kq+c_v) tensor
        """
        # select kv according to routing index
        n, p2, w2, c_kv = kv.size()
        topk = r_idx.size(-1)
        # print(r_idx.size(), r_weight.size())
        # FIXME: gather consumes much memory (topk times redundancy), write cuda kernel?
        topk_kv = torch.gather(kv.view(n, 1, p2, w2, c_kv).expand(-1, p2, -1, -1, -1),
                               # (n, p^2, p^2, w^2, c_kv) without mem cpy
                               dim=2,
                               index=r_idx.view(n, p2, topk, 1, 1).expand(-1, -1, -1, w2, c_kv)
                               # (n, p^2, k, w^2, c_kv)
                               )
 
        if self.mul_weight == 'soft':
            topk_kv = r_weight.view(n, p2, topk, 1, 1) * topk_kv  # (n, p^2, k, w^2, c_kv)
        elif self.mul_weight == 'hard':
            raise NotImplementedError('differentiable hard routing TBA')
        # else: #'none'
        #     topk_kv = topk_kv # do nothing
 
        return topk_kv
 
 
class QKVLinear(nn.Module):
    def __init__(self, dim, qk_dim, bias=True):
        super().__init__()
        self.dim = dim
        self.qk_dim = qk_dim
        self.qkv = nn.Linear(dim, qk_dim + qk_dim + dim, bias=bias)
 
    def forward(self, x):
        q, kv = self.qkv(x).split([self.qk_dim, self.qk_dim + self.dim], dim=-1)
        return q, kv
        # q, k, v = self.qkv(x).split([self.qk_dim, self.qk_dim, self.dim], dim=-1)
        # return q, k, v
 
 
class BiLevelRoutingAttention(nn.Module):
    """
    n_win: number of windows in one side (so the actual number of windows is n_win*n_win)
    kv_per_win: for kv_downsample_mode='ada_xxxpool' only, number of key/values per window. Similar to n_win, the actual number is kv_per_win*kv_per_win.
    topk: topk for window filtering
    param_attention: 'qkvo'-linear for q,k,v and o, 'none': param free attention
    param_routing: extra linear for routing
    diff_routing: wether to set routing differentiable
    soft_routing: wether to multiply soft routing weights
    """
 
    def __init__(self, dim, n_win=7, num_heads=8, qk_dim=None, qk_scale=None,
                 kv_per_win=4, kv_downsample_ratio=4, kv_downsample_kernel=None, kv_downsample_mode='identity',
                 topk=4, param_attention="qkvo", param_routing=False, diff_routing=False, soft_routing=False,
                 side_dwconv=3,
                 auto_pad=True):
        super().__init__()
        # local attention setting
        self.dim = dim
        self.n_win = n_win  # Wh, Ww
        self.num_heads = num_heads
        self.qk_dim = qk_dim or dim
        assert self.qk_dim % num_heads == 0 and self.dim % num_heads == 0, 'qk_dim and dim must be divisible by num_heads!'
        self.scale = qk_scale or self.qk_dim ** -0.5
 
        ################side_dwconv (i.e. LCE in ShuntedTransformer)###########
        self.lepe = nn.Conv2d(dim, dim, kernel_size=side_dwconv, stride=1, padding=side_dwconv // 2,
                              groups=dim) if side_dwconv > 0 else \
            lambda x: torch.zeros_like(x)
 
        ################ global routing setting #################
        self.topk = topk
        self.param_routing = param_routing
        self.diff_routing = diff_routing
        self.soft_routing = soft_routing
        # router
        assert not (self.param_routing and not self.diff_routing)  # cannot be with_param=True and diff_routing=False
        self.router = TopkRouting(qk_dim=self.qk_dim,
                                  qk_scale=self.scale,
                                  topk=self.topk,
                                  diff_routing=self.diff_routing,
                                  param_routing=self.param_routing)
        if self.soft_routing:  # soft routing, always diffrentiable (if no detach)
            mul_weight = 'soft'
        elif self.diff_routing:  # hard differentiable routing
            mul_weight = 'hard'
        else:  # hard non-differentiable routing
            mul_weight = 'none'
        self.kv_gather = KVGather(mul_weight=mul_weight)
 
        # qkv mapping (shared by both global routing and local attention)
        self.param_attention = param_attention
        if self.param_attention == 'qkvo':
            self.qkv = QKVLinear(self.dim, self.qk_dim)
            self.wo = nn.Linear(dim, dim)
        elif self.param_attention == 'qkv':
            self.qkv = QKVLinear(self.dim, self.qk_dim)
            self.wo = nn.Identity()
        else:
            raise ValueError(f'param_attention mode {self.param_attention} is not surpported!')
 
        self.kv_downsample_mode = kv_downsample_mode
        self.kv_per_win = kv_per_win
        self.kv_downsample_ratio = kv_downsample_ratio
        self.kv_downsample_kenel = kv_downsample_kernel
        if self.kv_downsample_mode == 'ada_avgpool':
            assert self.kv_per_win is not None
            self.kv_down = nn.AdaptiveAvgPool2d(self.kv_per_win)
        elif self.kv_downsample_mode == 'ada_maxpool':
            assert self.kv_per_win is not None
            self.kv_down = nn.AdaptiveMaxPool2d(self.kv_per_win)
        elif self.kv_downsample_mode == 'maxpool':
            assert self.kv_downsample_ratio is not None
            self.kv_down = nn.MaxPool2d(self.kv_downsample_ratio) if self.kv_downsample_ratio > 1 else nn.Identity()
        elif self.kv_downsample_mode == 'avgpool':
            assert self.kv_downsample_ratio is not None
            self.kv_down = nn.AvgPool2d(self.kv_downsample_ratio) if self.kv_downsample_ratio > 1 else nn.Identity()
        elif self.kv_downsample_mode == 'identity':  # no kv downsampling
            self.kv_down = nn.Identity()
        elif self.kv_downsample_mode == 'fracpool':
            # assert self.kv_downsample_ratio is not None
            # assert self.kv_downsample_kenel is not None
            # TODO: fracpool
            # 1. kernel size should be input size dependent
            # 2. there is a random factor, need to avoid independent sampling for k and v
            raise NotImplementedError('fracpool policy is not implemented yet!')
        elif kv_downsample_mode == 'conv':
            # TODO: need to consider the case where k != v so that need two downsample modules
            raise NotImplementedError('conv policy is not implemented yet!')
        else:
            raise ValueError(f'kv_down_sample_mode {self.kv_downsaple_mode} is not surpported!')
 
        # softmax for local attention
        self.attn_act = nn.Softmax(dim=-1)
 
        self.auto_pad = auto_pad
 
    def forward(self, x, ret_attn_mask=False):
        """
        x: NHWC tensor
        Return:
            NHWC tensor
        """
        x = rearrange(x, "n c h w -> n h w c")
        # NOTE: use padding for semantic segmentation
        ###################################################
        if self.auto_pad:
            N, H_in, W_in, C = x.size()
 
            pad_l = pad_t = 0
            pad_r = (self.n_win - W_in % self.n_win) % self.n_win
            pad_b = (self.n_win - H_in % self.n_win) % self.n_win
            x = F.pad(x, (0, 0,  # dim=-1
                          pad_l, pad_r,  # dim=-2
                          pad_t, pad_b))  # dim=-3
            _, H, W, _ = x.size()  # padded size
        else:
            N, H, W, C = x.size()
            assert H % self.n_win == 0 and W % self.n_win == 0  #
        ###################################################
 
        # patchify, (n, p^2, w, w, c), keep 2d window as we need 2d pooling to reduce kv size
        x = rearrange(x, "n (j h) (i w) c -> n (j i) h w c", j=self.n_win, i=self.n_win)
 
        #################qkv projection###################
        # q: (n, p^2, w, w, c_qk)
        # kv: (n, p^2, w, w, c_qk+c_v)
        # NOTE: separte kv if there were memory leak issue caused by gather
        q, kv = self.qkv(x)
 
        # pixel-wise qkv
        # q_pix: (n, p^2, w^2, c_qk)
        # kv_pix: (n, p^2, h_kv*w_kv, c_qk+c_v)
        q_pix = rearrange(q, 'n p2 h w c -> n p2 (h w) c')
        kv_pix = self.kv_down(rearrange(kv, 'n p2 h w c -> (n p2) c h w'))
        kv_pix = rearrange(kv_pix, '(n j i) c h w -> n (j i) (h w) c', j=self.n_win, i=self.n_win)
 
        q_win, k_win = q.mean([2, 3]), kv[..., 0:self.qk_dim].mean(
            [2, 3])  # window-wise qk, (n, p^2, c_qk), (n, p^2, c_qk)
 
        ##################side_dwconv(lepe)##################
        # NOTE: call contiguous to avoid gradient warning when using ddp
        lepe = self.lepe(rearrange(kv[..., self.qk_dim:], 'n (j i) h w c -> n c (j h) (i w)', j=self.n_win,
                                   i=self.n_win).contiguous())
        lepe = rearrange(lepe, 'n c (j h) (i w) -> n (j h) (i w) c', j=self.n_win, i=self.n_win)
 
        ############ gather q dependent k/v #################
 
        r_weight, r_idx = self.router(q_win, k_win)  # both are (n, p^2, topk) tensors
 
        kv_pix_sel = self.kv_gather(r_idx=r_idx, r_weight=r_weight, kv=kv_pix)  # (n, p^2, topk, h_kv*w_kv, c_qk+c_v)
        k_pix_sel, v_pix_sel = kv_pix_sel.split([self.qk_dim, self.dim], dim=-1)
        # kv_pix_sel: (n, p^2, topk, h_kv*w_kv, c_qk)
        # v_pix_sel: (n, p^2, topk, h_kv*w_kv, c_v)
 
        ######### do attention as normal ####################
        k_pix_sel = rearrange(k_pix_sel, 'n p2 k w2 (m c) -> (n p2) m c (k w2)',
                              m=self.num_heads)  # flatten to BMLC, (n*p^2, m, topk*h_kv*w_kv, c_kq//m) transpose here?
        v_pix_sel = rearrange(v_pix_sel, 'n p2 k w2 (m c) -> (n p2) m (k w2) c',
                              m=self.num_heads)  # flatten to BMLC, (n*p^2, m, topk*h_kv*w_kv, c_v//m)
        q_pix = rearrange(q_pix, 'n p2 w2 (m c) -> (n p2) m w2 c',
                          m=self.num_heads)  # to BMLC tensor (n*p^2, m, w^2, c_qk//m)
 
        # param-free multihead attention
        attn_weight = (
                              q_pix * self.scale) @ k_pix_sel  # (n*p^2, m, w^2, c) @ (n*p^2, m, c, topk*h_kv*w_kv) -> (n*p^2, m, w^2, topk*h_kv*w_kv)
        attn_weight = self.attn_act(attn_weight)
        out = attn_weight @ v_pix_sel  # (n*p^2, m, w^2, topk*h_kv*w_kv) @ (n*p^2, m, topk*h_kv*w_kv, c) -> (n*p^2, m, w^2, c)
        out = rearrange(out, '(n j i) m (h w) c -> n (j h) (i w) (m c)', j=self.n_win, i=self.n_win,
                        h=H // self.n_win, w=W // self.n_win)
 
        out = out + lepe
        # output linear
        out = self.wo(out)
 
        # NOTE: use padding for semantic segmentation
        # crop padded region
        if self.auto_pad and (pad_r > 0 or pad_b > 0):
            out = out[:, :H_in, :W_in, :].contiguous()
 
        if ret_attn_mask:
            return out, r_weight, r_idx, attn_weight
        else:
            return rearrange(out, "n h w c -> n c h w")
```

### æ­¥éª¤äºŒÂ 

**æˆ‘ä»¬æ‰¾åˆ°è¯¥æ–‡ä»¶'ultralytics/nn/tasks.py'çš„å¼€å¤´æ¨¡å—å¯¼å…¥éƒ¨åˆ†åœ¨å…¶ä¸­æ·»åŠ å¦‚ä¸‹ä¸€è¡Œä»£ç **

```python
from ultralytics.nn.modules.Biformer import BiLevelRoutingAttention 
```

* * *

### æ­¥éª¤ä¸‰

ç°åœ¨æˆ‘ä»¬ä»¥åŠå°†Biformeræ–‡ä»¶å¯¼å…¥äº†æ¨¡å‹ä¸­äº†ï¼Œä¸‹ä¸€æ­¥æˆ‘ä»¬å°±éœ€è¦æ·»åŠ è¯¥æœºåˆ¶åˆ°æ¨¡å‹ä¸­è®©æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åœ¨æ­¥éª¤äºŒçš„æ–‡ä»¶ä¸­''ultralytics/nn/tasks.py''æŒ‰å¿«æ·é”®Ctrl+Få¯ä»¥è¿›è¡Œæ–‡ä»¶æœç´¢ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/9830d7746d6644a2a0371b21b62a3d73.png)

å½“ç„¶å¦‚æœä½ ä¸æƒ³ç”¨å¿«æ·é”®ä¹Ÿå¯ä»¥è‡ªå·±å¯»æ‰¾å¤§æ¦‚åœ¨Â 650è¡Œå·¦å³ï¼Œæœ‰ä¸€ä¸ªæ–¹æ³•çš„åå­—å«"parse\_model"

æˆ‘ä»¬æ‰¾åˆ°è¯¥æ–¹æ³•å¯¹å…¶è¿›è¡Œä¿®æ”¹ï¼Œæ·»åŠ å¦‚ä¸‹å›¾æ‰€ç¤ºå†…å®¹ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0a48e53ab7df41bf8fcd5bdd7c8dc4f8.png)

è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå­—å…¸ï¼Œæˆ‘ä»¬ä»¥ååœ¨æƒ³å¯¼å…¥å…¶å®ƒçš„æ³¨æ„åŠ›æœºåˆ¶å°±å¯ä»¥é‡å¤æ­¥éª¤ä¸€å’Œæ­¥éª¤äºŒï¼Œç„¶ååœ¨æ­¥éª¤ä¸‰è¿™é‡Œå®šä¹‰çš„å­—å…¸ä¸­æ·»åŠ ä½ å¯¼å…¥çš„æ³¨æ„åŠ›æœºåˆ¶åå­—å³å¯ã€‚Â 

å…­ã€é…ç½®Biformeræ³¨æ„åŠ›æœºåˆ¶
-----------------

æ­å–œä½ ï¼Œåˆ°è¿™é‡Œæˆ‘ä»¬å°±å·²ç»æˆåŠŸçš„å¯¼å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¦»ä¿®æ”¹æ¨¡å‹åªå·®æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å¦‚ä¸‹æ–‡ä»¶è¿›è¡Œä¿®æ”¹"ultralytics/cfg/models/v8/yolov8.yaml",æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶ä¹‹ååˆå§‹å¦‚ä¸‹æ‰€ç¤ºï¼Œ

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f8eb256281234a99b6384b35c0fb452b.png)

æˆ‘ä»¬å¯ä»¥åœ¨æŸä¸€å±‚ä¸­æ·»åŠ Biformeræ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·ä½“æ·»åŠ åˆ°å“ªé‡Œç”±ä½ è‡ªå·±å†³å®šï¼Œæˆ‘è¿™é‡Œå»ºè®®æ·»åŠ åˆ°Â  Neckå±‚ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ç‰¹å¾èåˆå±‚ï¼Œæ·»åŠ ä¹‹åçš„æ•ˆæœå¦‚ä¸‹ï¼Œè¿™é‡Œæˆ‘åœ¨ä¸‰ä¸ªåœ°æ–¹æ·»åŠ äº†Biformeræ³¨æ„åŠ›æœºåˆ¶ã€‚

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 1, BiLevelRoutingAttention, []]  # 15 (P3/8-small)
  - [-1, 3, C2f, [256]]  # 16 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 1, BiLevelRoutingAttention, []]  # 15 (P3/8-small)
  - [-1, 3, C2f, [512]]  # 20 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 1, BiLevelRoutingAttention, []]  # 15 (P3/8-small)
  - [-1, 3, C2f, [1024]]  # 24 (P5/32-large)
 
  - [[16, 20, 24], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

å¦‚æœå¤§å®¶çœ‹yamlæ–‡ä»¶çœ‹çš„ä¸æ¸…æ¥šè‡ªå·±æŠŠç»“æ„æ·»åŠ åˆ°äº†å“ªé‡Œå¯ä»¥çœ‹ä¸‹å›¾ï¼Œå¤§å®¶åº”è¯¥å°±æ¸…æ¥šäº†ï¼Œæˆ‘æŠŠä¸‰ä¸ªBiformeræ·»åŠ åˆ°äº†ç½‘ç»œç»“æ„ä¸­çš„é‚£ä¸ªéƒ¨ä½ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/c9c4f5bacccd4c239e8f10364a9327bd.png)

ä¸ƒã€è®­ç»ƒæ¨¡å‹
------

åˆ°æ­¤æˆ‘ä»¬çš„æ‰€æœ‰å‡†å¤‡å·¥ä½œéƒ½å·²å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è¿›è¡Œè®­ç»ƒäº†ã€‚

æˆ‘ä»¬åœ¨æ§åˆ¶å°è¾“å…¥

```python
 yolo cfg='ultralytics/cfg/default.yaml'
```

å¯ä»¥çœ‹åˆ°å¼€å§‹è®­ç»ƒï¼Œæ§åˆ¶å°è¿›è¡Œäº†æ¨¡å‹ç»“æ„çš„è¾“å‡ºå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ç»“æ„ä»¥åŠæ‰“å°åœ¨æ§åˆ¶å°äº†ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/73d4860e1d644164b2b00d162648621e.png)

* * *

å…«ã€ç»“æœåˆ†æ
------

æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ—¶é—´æ¯”è¾ƒé•¿ï¼Œæˆ‘ä»¬ç»è¿‡ç­‰å¾…ä¹‹åï¼Œå¯ä»¥çœ‹åˆ°ç»“æœæ–‡ä»¶ä¸­ä¿å­˜äº†æˆ‘ä»¬çš„è¿è¡Œç»“æœã€‚Â 

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/03f9d634b54a4207aca902d6fb323bf4.png)

ç»“æœå›¾å¦‚ä¸‹ï¼Œå¤§å®¶å¯ä»¥è‡ªè¡Œè®­ç»ƒï¼Œæˆ‘æ·»åŠ çš„ä½ç½®åªæ˜¯éšä¾¿æ·»åŠ çš„ï¼Œä¸ä¸€å®šä½ç½®å¯¹å¹¶æ²¡æœ‰è¿›è¡Œå¤šæ¬¡å®éªŒè°ƒå‚ä»€ä¹ˆçš„ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/95260e59f29d442d8796d9cf95b35a78.png)
