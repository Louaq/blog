# å¤šå°ºåº¦ç©ºæ´æ³¨æ„åŠ› ï¼ˆ:100:ï¼‰

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

æœ¬æ–‡ç»™å¤§å®¶å¸¦æ¥çš„æ”¹è¿›æœºåˆ¶æ˜¯**MSDAï¼ˆå¤šå°ºåº¦ç©ºæ´æ³¨æ„åŠ›ï¼‰å‘è¡¨äºä»Šå¹´çš„ä¸­ç§‘é™¢ä¸€åŒº(ç®—æ˜¯å›½å†…è®¡ç®—æœºé¢†åŸŸçš„æœ€é«˜æœŸåˆŠäº†)**ï¼Œå…¶å…¨ç§°æ˜¯"DilateFormer: Multi-Scale Dilated Transformer for Visual Recognition"ã€‚MSDAçš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡çº¿æ€§æŠ•å½±å¾—åˆ°ç‰¹å¾å›¾Xçš„ç›¸åº”æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚ç„¶åï¼Œå°†ç‰¹å¾å›¾çš„é€šé“åˆ†æˆnä¸ªä¸åŒçš„å¤´éƒ¨ï¼Œå¹¶åœ¨ä¸åŒçš„å¤´éƒ¨ä¸­ä»¥ä¸åŒçš„æ‰©å¼ ç‡æ‰§è¡Œå¤šå°ºåº¦SWDAæ¥æé«˜æ¨¡å‹çš„å¤„ç†æ•ˆç‡å’Œæ£€æµ‹ç²¾åº¦ã€‚**äº²æµ‹åœ¨å°ç›®æ ‡æ£€æµ‹å’Œå¤§å°ºåº¦ç›®æ ‡æ£€æµ‹çš„æ•°æ®é›†ä¸Šéƒ½æœ‰å¤§å¹…åº¦çš„æ¶¨ç‚¹æ•ˆæœ**(**mAPç›´æ¥æ¶¨äº†å¤§æ¦‚æœ‰0.06å·¦å³**)ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a8f88a4e9eed49c8af44de3e73db75db.png)

äºŒã€MSDAæ¡†æ¶åŸç†
----------

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0aa19669c1c844b2841c869557edb42d.png)â€‹

**è®ºæ–‡åœ°å€ï¼š[å®˜æ–¹è®ºæ–‡åœ°å€ç‚¹å‡»å³å¯è·³è½¬](https://arxiv.org/pdf/2302.01791.pdf "å®˜æ–¹è®ºæ–‡åœ°å€ç‚¹å‡»å³å¯è·³è½¬")**

**ä»£ç åœ°å€ï¼š[å®˜æ–¹ä»£ç åœ°å€ç‚¹å‡»å³å¯è·³è½¬](https://github.com/jiaojiayuasd/dilateformer "å®˜æ–¹ä»£ç åœ°å€ç‚¹å‡»å³å¯è·³è½¬")**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/16b293b5075a4b9bab37056ef34a3523.png)â€‹

* * *

åœ¨DilateFormerè®ºæ–‡ä¸­ï¼Œ**å¤šå°ºåº¦æ‰©å¼ æ³¨æ„åŠ›**ï¼ˆ**MSDA**ï¼‰æ¨¡å—æ˜¯ä¸ºäº†åˆ©ç”¨è‡ªæ³¨æ„æœºåˆ¶åœ¨ä¸åŒå°ºåº¦ä¸Šçš„ç¨€ç–æ€§ã€‚MSDAé€šè¿‡çº¿æ€§æŠ•å½±å¾—åˆ°ç‰¹å¾å›¾Xçš„ç›¸åº”æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚ç„¶åï¼Œå°†ç‰¹å¾å›¾çš„é€šé“åˆ†æˆnä¸ªä¸åŒçš„å¤´éƒ¨ï¼Œå¹¶åœ¨ä¸åŒçš„å¤´éƒ¨ä¸­ä»¥ä¸åŒçš„æ‰©å¼ ç‡æ‰§è¡Œå¤šå°ºåº¦SWDAã€‚å…·ä½“æ¥è¯´ï¼ŒMSDAè¢«å…¬å¼åŒ–å¦‚ä¸‹ï¼šå¯¹äºæ¯ä¸ªå¤´éƒ¨iï¼Œè¿›è¡ŒSWDAæ“ä½œï¼Œå¹¶ä¸”å¯¹æ‰€æœ‰çš„è¾“å‡º${h_i}$è¿›è¡Œè¿æ¥åé€å…¥ä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œç‰¹å¾èšåˆã€‚é€šè¿‡ä¸ºä¸åŒçš„å¤´éƒ¨è®¾ç½®ä¸åŒçš„æ‰©å¼ ç‡ï¼ŒMSDAèƒ½å¤Ÿåœ¨è¢«å…³æ³¨çš„æ¥å—åŸŸå†…æœ‰æ•ˆåœ°èšåˆä¸åŒå°ºåº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸éœ€è¦å¤æ‚æ“ä½œå’Œé¢å¤–è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°å‡å°‘è‡ªæ³¨æ„æœºåˆ¶çš„å†—ä½™

**MSDAï¼ˆå¤šå°ºåº¦æ‰©å¼ æ³¨æ„åŠ›ï¼‰æ¨¡å—çš„ä¸»è¦æ”¹è¿›æœºåˆ¶åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š**

**1\. å¤šå°ºåº¦ç‰¹å¾æå–**ï¼šé€šè¿‡ä¸åŒå¤´éƒ¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒMSDAèƒ½å¤Ÿæ•æ‰åˆ°å¤šå°ºåº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™å¯¹äºç†è§£å›¾åƒçš„ä¸åŒæŠ½è±¡å±‚æ¬¡æ˜¯éå¸¸é‡è¦çš„ã€‚

**2\. ç¨€ç–æ€§åˆ©ç”¨**ï¼šMSDAåˆ©ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸åŒå°ºåº¦çš„ç¨€ç–æ€§ï¼Œé™ä½äº†è®¡ç®—çš„å†—ä½™ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚

**3\. å¤´éƒ¨é€šé“åˆ†ç¦»**ï¼šMSDAå°†ç‰¹å¾å›¾çš„é€šé“åˆ†ç¦»æˆå¤šä¸ªå¤´éƒ¨ï¼Œæ¯ä¸ªå¤´éƒ¨å¤„ç†ä¸åŒçš„ç‰¹å¾å­é›†ï¼Œè¿™æ ·å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›å’Œæ•ˆç‡ã€‚

**4\. ä¸åŒçš„æ‰©å¼ ç‡**ï¼šé€šè¿‡åœ¨ä¸åŒå¤´éƒ¨è®¾ç½®ä¸åŒçš„æ‰©å¼ ç‡ï¼ŒMSDAèƒ½å¤Ÿåœ¨å„ä¸ªå¤´éƒ¨å…³æ³¨ä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œä»è€Œèƒ½æ›´åŠ å…¨é¢åœ°æ•æ‰å›¾åƒä¸­çš„ä¿¡æ¯ã€‚

**5\. ç‰¹å¾èšåˆ**ï¼šMSDAçš„è¾“å‡ºé€šè¿‡è¿æ¥æ“ä½œåˆå¹¶ï¼Œå¹¶é€šè¿‡çº¿æ€§å±‚è¿›è¡Œç‰¹å¾èšåˆï¼Œè¿™æ ·å¯ä»¥æ•´åˆå„ä¸ªå¤´éƒ¨å­¦ä¹ åˆ°çš„ä¿¡æ¯ï¼Œå¾—åˆ°æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚

è¿™äº›æ”¹è¿›ä½¿å¾—MSDAåœ¨ä¸å¢åŠ é¢å¤–è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡å’Œæ•ˆæœã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/5b3b0730ac394a79a7b804cb83b702af.png)

è¿™å¹…å›¾å±•ç¤ºäº†ViT-Smallçš„ç¬¬ä¸‰ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attention, MHSAï¼‰å—çš„æ³¨æ„åŠ›å›¾çš„å¯è§†åŒ–ã€‚åœ¨æ¯å¼ å›¾ä¸­ï¼Œä¸€ä¸ªç‰¹å®šçš„æŸ¥è¯¢å—ï¼ˆçº¢è‰²æ¡†å†…çš„åŒºåŸŸï¼‰è¢«ç”¨æ¥å±•ç¤ºå…¶å®ƒå„ä¸ªå—å¯¹å®ƒçš„æ³¨æ„åŠ›ç¨‹åº¦ã€‚æ³¨æ„åŠ›å›¾æ˜¾ç¤ºäº†å…·æœ‰é«˜æ³¨æ„åŠ›å¾—åˆ†çš„å—åœ¨æŸ¥è¯¢å—å‘¨å›´ç¨€ç–åˆ†å¸ƒï¼Œè€Œå…¶å®ƒå—çš„æ³¨æ„åŠ›å¾—åˆ†è¾ƒä½ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0d4f8d81e9f64054b22b5d45cf09c6a3.png)

è¿™å¼ å›¾å±•ç¤ºäº†å¤šå°ºåº¦æ‰©å¼ æ³¨æ„åŠ›ï¼ˆMSDAï¼‰çš„å·¥ä½œåŸç†ã€‚åœ¨MSDAä¸­ï¼Œç‰¹å¾å›¾çš„é€šé“é¦–å…ˆè¢«åˆ†å‰²æˆä¸åŒçš„å¤´éƒ¨ï¼Œç„¶åæ¯ä¸ªå¤´éƒ¨å†…éƒ¨ä½¿ç”¨ä¸åŒçš„æ‰©å¼ ç‡ï¼ˆdilation ratesï¼‰ræ¥æ‰§è¡Œè‡ªæ³¨æ„åŠ›æ“ä½œã€‚è¿™äº›æ“ä½œåœ¨å›´ç»•çº¢è‰²æŸ¥è¯¢å—çš„çª—å£å†…çš„å½©è‰²å—ä¹‹é—´è¿›è¡Œã€‚

å›¾ä¸­çš„ä¾‹å­å±•ç¤ºäº†ä¸‰ç§ä¸åŒçš„æ‰©å¼ ç‡ï¼ˆr=1, 2, 3ï¼‰(**è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯å’±ä»¬æˆ‘çš„ç½‘ç»œä¸­éœ€è¦æ”¹æˆå››ç§çš„æ‰©å¼ ç‡**)ï¼Œå®ƒä»¬åˆ†åˆ«å¯¹åº”ä¸åŒçš„æ„Ÿå—é‡å¤§å°ï¼ˆ3x3, 5x5, 7x7ï¼‰ã€‚æ¯ä¸ªå¤´éƒ¨çš„è‡ªæ³¨æ„åŠ›æ“ä½œé’ˆå¯¹çš„æ˜¯å…¶å¯¹åº”çš„æ‰©å¼ ç‡å’Œæ„Ÿå—é‡ã€‚è¿™æ ·ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„å°ºåº¦ä¸Šæ•æ‰å›¾åƒç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾éšåè¢«è¿æ¥åœ¨ä¸€èµ·ï¼Œå¹¶é€å…¥ä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œç‰¹å¾èšåˆã€‚

è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„å°ºåº¦ä¸Šç†è§£å›¾åƒï¼Œä»è€Œæé«˜å¯¹å›¾åƒå†…å®¹çš„æ•´ä½“ç†è§£ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒMSDAä¸ä»…å¯ä»¥æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼Œä¹Ÿèƒ½å¤Ÿæ„ŸçŸ¥åˆ°æ›´å¹¿æ³›åŒºåŸŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚

ä¸‰ã€MSDAæ ¸å¿ƒä»£ç 
----------

ä¸‹é¢çš„ä»£ç æ˜¯MSDAçš„æ ¸å¿ƒä»£ç ï¼Œæˆ‘ä»¬å°†å…¶å¤åˆ¶å¯¼'ultralytics/nn/modules'ç›®å½•ä¸‹ï¼Œåœ¨å…¶ä¸­åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œæˆ‘è¿™é‡Œèµ·åä¸ºDilationç„¶åç²˜è´´è¿›å»ï¼Œå…¶ä½™ä½¿ç”¨æ–¹å¼çœ‹ç« èŠ‚å››ã€‚

```python
import torch
import torch.nn as nn
from functools import partial
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
from timm.models.registry import register_model
from timm.models.vision_transformer import _cfg
 
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)
 
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
 
 
class DilateAttention(nn.Module):
    "Implementation of Dilate-attention"
    def __init__(self, head_dim, qk_scale=None, attn_drop=0, kernel_size=3, dilation=1):
        super().__init__()
        self.head_dim = head_dim
        self.scale = qk_scale or head_dim ** -0.5
        self.kernel_size=kernel_size
        self.unfold = nn.Unfold(kernel_size, dilation, dilation*(kernel_size-1)//2, 1)
        self.attn_drop = nn.Dropout(attn_drop)
 
    def forward(self,q,k,v):
        #B, C//3, H, W
        B,d,H,W = q.shape
        q = q.reshape([B, d//self.head_dim, self.head_dim, 1 ,H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,1,d
        k = self.unfold(k).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 2, 3)  #B,h,N,d,k*k
        attn = (q @ k) * self.scale  # B,h,N,1,k*k
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        v = self.unfold(v).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,k*k,d
        x = (attn @ v).transpose(1, 2).reshape(B, H, W, d)
        return x
 
 
class MultiDilatelocalAttention(nn.Module):
    "Implementation of Dilate-attention"
 
    def __init__(self, dim, num_heads=8, qkv_bias=True, qk_scale=None,
                 attn_drop=0.,proj_drop=0., kernel_size=3, dilation=[1, 2, 3, 4]):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.dilation = dilation
        self.kernel_size = kernel_size
        self.scale = qk_scale or head_dim ** -0.5
        self.num_dilation = len(dilation)
        assert num_heads % self.num_dilation == 0, f"num_heads{num_heads} must be the times of num_dilation{self.num_dilation}!!"
        self.qkv = nn.Conv2d(dim, dim * 3, 1, bias=qkv_bias)
        self.dilate_attention = nn.ModuleList(
            [DilateAttention(head_dim, qk_scale, attn_drop, kernel_size, dilation[i])
             for i in range(self.num_dilation)])
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
 
    def forward(self, x):
        B, C, H, W = x.shape
        # x = x.permute(0, 3, 1, 2)# B, C, H, W
        y = x.clone()
        qkv = self.qkv(x).reshape(B, 3, self.num_dilation, C//self.num_dilation, H, W).permute(2, 1, 0, 3, 4, 5)
        #num_dilation,3,B,C//num_dilation,H,W
        y1 = y.reshape(B, self.num_dilation, C//self.num_dilation, H, W).permute(1, 0, 3, 4, 2 )
        # num_dilation, B, H, W, C//num_dilation
        for i in range(self.num_dilation):
            y1[i] = self.dilate_attention[i](qkv[i][0], qkv[i][1], qkv[i][2])# B, H, W,C//num_dilation
        y2 = y1.permute(1, 2, 3, 0, 4).reshape(B, H, W, C)
        y3 = self.proj(y2)
        y4 = self.proj_drop(y3).permute(0, 3, 1, 2)
        return y4
 
 
class DilateBlock(nn.Module):
    "Implementation of Dilate-attention block"
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0.,act_layer=nn.GELU, norm_layer=nn.LayerNorm, kernel_size=3, dilation=[1, 2, 3],
                 cpe_per_block=False):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.mlp_ratio = mlp_ratio
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.cpe_per_block = cpe_per_block
        if self.cpe_per_block:
            self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)
        self.norm1 = norm_layer(dim)
        self.attn = MultiDilatelocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                                attn_drop=attn_drop, kernel_size=kernel_size, dilation=dilation)
 
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
 
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)
 
    def forward(self, x):
        if self.cpe_per_block:
            x = x + self.pos_embed(x)
        x = x.permute(0, 2, 3, 1)
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        x = x.permute(0, 3, 1, 2)
        #B, C, H, W
        return x
 
 
class GlobalAttention(nn.Module):
    "Implementation of self-attention"
 
    def __init__(self, dim,  num_heads=8, qkv_bias=False,
                 qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim**-0.5
 
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
 
    def forward(self, x):
        B, H, W, C = x.shape
        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads,
                                  C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
 
        x = (attn @ v).transpose(1, 2).reshape(B, H, W, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
 
 
class GlobalBlock(nn.Module):
    """
    Implementation of Transformer
    """
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,qk_scale=None, drop=0.,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 cpe_per_block=False):
        super().__init__()
        self.cpe_per_block = cpe_per_block
        if self.cpe_per_block:
            self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)
        self.norm1 = norm_layer(dim)
        self.attn = GlobalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias,
                              qk_scale=qk_scale, attn_drop=attn_drop)
 
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
 
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)
 
    def forward(self, x):
        if self.cpe_per_block:
            x = x + self.pos_embed(x)
        x = x.permute(0, 2, 3, 1)
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        x = x.permute(0, 3, 1, 2)
        return x
 
 
class PatchEmbed(nn.Module):
    """Image to Patch Embedding.
    """
    def __init__(self, img_size=224, in_chans=3, hidden_dim=16,
                 patch_size=4, embed_dim=96, patch_way=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.num_patches = patches_resolution[0] * patches_resolution[1]
        self.img_size = img_size
        assert patch_way in ['overlaping', 'nonoverlaping', 'pointconv'],\
            "the patch embedding way isn't exist!"
        if patch_way == "nonoverlaping":
            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        elif patch_way == "overlaping":
            self.proj = nn.Sequential(
                nn.Conv2d(in_chans, hidden_dim, kernel_size=3, stride=1,
                          padding=1, bias=False),  # 224x224
                nn.BatchNorm2d(hidden_dim),
                nn.GELU( ),
                nn.Conv2d(hidden_dim, int(hidden_dim*2), kernel_size=3, stride=2,
                          padding=1, bias=False),  # 112x112
                nn.BatchNorm2d(int(hidden_dim*2)),
                nn.GELU( ),
                nn.Conv2d(int(hidden_dim*2), int(hidden_dim*4), kernel_size=3, stride=1,
                          padding=1, bias=False),  # 112x112
                nn.BatchNorm2d(int(hidden_dim*4)),
                nn.GELU( ),
                nn.Conv2d(int(hidden_dim*4), embed_dim, kernel_size=3, stride=2,
                          padding=1, bias=False),  # 56x56
            )
        else:
            self.proj = nn.Sequential(
                nn.Conv2d(in_chans, hidden_dim, kernel_size=3, stride=2,
                          padding=1, bias=False),  # 112x112
                nn.BatchNorm2d(hidden_dim),
                nn.GELU( ),
                nn.Conv2d(hidden_dim, int(hidden_dim*2), kernel_size=1, stride=1,
                          padding=0, bias=False),  # 112x112
                nn.BatchNorm2d(int(hidden_dim*2)),
                nn.GELU( ),
                nn.Conv2d(int(hidden_dim*2), int(hidden_dim*4), kernel_size=3, stride=2,
                          padding=1, bias=False),  # 56x56
                nn.BatchNorm2d(int(hidden_dim*4)),
                nn.GELU( ),
                nn.Conv2d(int(hidden_dim*4), embed_dim, kernel_size=1, stride=1,
                          padding=0, bias=False),   # 56x56
            )
 
    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)  # B, C, H, W
        return x
 
 
class PatchMerging(nn.Module):
    """ Patch Merging Layer.
    """
    def __init__(self, in_channels, out_channels, merging_way, cpe_per_satge, norm_layer=nn.BatchNorm2d):
        super().__init__()
        assert merging_way in ['conv3_2', 'conv2_2', 'avgpool3_2', 'avgpool2_2'], \
            "the merging way is not exist!"
        self.cpe_per_satge = cpe_per_satge
        if merging_way == 'conv3_2':
            self.proj = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),
                norm_layer(out_channels),
            )
        elif merging_way == 'conv2_2':
            self.proj = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0),
                norm_layer(out_channels),
            )
        elif merging_way == 'avgpool3_2':
            self.proj = nn.Sequential(
                nn.AvgPool2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),
                norm_layer(out_channels),
            )
        else:
            self.proj = nn.Sequential(
                nn.AvgPool2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0),
                norm_layer(out_channels),
            )
        if self.cpe_per_satge:
            self.pos_embed = nn.Conv2d(out_channels, out_channels, 3, padding=1, groups=out_channels)
 
    def forward(self, x):
        #x: B, C, H ,W
        x = self.proj(x)
        if self.cpe_per_satge:
            x = x + self.pos_embed(x)
        return x
 
 
class Dilatestage(nn.Module):
    """ A basic Dilate Transformer layer for one stage.
    """
    def __init__(self, dim, depth, num_heads, kernel_size, dilation,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0.,
                 attn_drop=0., drop_path=0., act_layer=nn.GELU,
                 norm_layer=nn.LayerNorm, cpe_per_satge=False, cpe_per_block=False,
                 downsample=True, merging_way=None):
 
        super().__init__()
        # build blocks
        self.blocks = nn.ModuleList([
            DilateBlock(dim=dim, num_heads=num_heads,
                        kernel_size=kernel_size, dilation=dilation,
                        mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                        qk_scale=qk_scale, drop=drop, attn_drop=attn_drop,
                        drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                        norm_layer=norm_layer, act_layer=act_layer, cpe_per_block=cpe_per_block)
            for i in range(depth)])
 
        # patch merging layer
        self.downsample = PatchMerging(dim, int(dim * 2), merging_way, cpe_per_satge) if downsample else nn.Identity()
 
    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        x = self.downsample(x)
        return x
 
 
class Globalstage(nn.Module):
    """ A basic Transformer layer for one stage."""
    def __init__(self, dim, depth, num_heads, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 cpe_per_satge=False, cpe_per_block=False,
                 downsample=True, merging_way=None):
 
        super().__init__()
        # build blocks
        self.blocks = nn.ModuleList([
            GlobalBlock(dim=dim, num_heads=num_heads,
                        mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,
                        qk_scale=qk_scale, drop=drop, attn_drop=attn_drop,
                        drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                        norm_layer=norm_layer, act_layer=act_layer, cpe_per_block=cpe_per_block)
            for i in range(depth)])
 
        # patch merging layer
        self.downsample = PatchMerging(dim, int(dim*2), merging_way, cpe_per_satge) if downsample else nn.Identity()
 
    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        x = self.downsample(x)
        return x
 
 
class Dilateformer(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, embed_dim=96,
                 depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], kernel_size=3, dilation=[1, 2, 3],
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,
                 norm_layer=partial(nn.LayerNorm, eps=1e-6),
                 merging_way='conv3_2',
                 patch_way='overlaping',
                 dilate_attention=[True, True, False, False],
                 downsamples=[True, True, True, False],
                 cpe_per_satge=False, cpe_per_block=True):
        super().__init__()
        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
 
        #patch embedding
        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,
                                      in_chans=in_chans, embed_dim=embed_dim, patch_way=patch_way)
        dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]
        self.stages = nn.ModuleList()
        for i_layer in range(self.num_layers):
            if dilate_attention[i_layer]:
                stage = Dilatestage(dim=int(embed_dim * 2 ** i_layer),
                                    depth=depths[i_layer],
                                    num_heads=num_heads[i_layer],
                                    kernel_size=kernel_size,
                                    dilation=dilation,
                                    mlp_ratio=self.mlp_ratio,
                                    qkv_bias=qkv_bias, qk_scale=qk_scale,
                                    drop=drop, attn_drop=attn_drop,
                                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                    norm_layer=norm_layer,
                                    downsample=downsamples[i_layer],
                                    cpe_per_block=cpe_per_block,
                                    cpe_per_satge=cpe_per_satge,
                                    merging_way=merging_way
                                    )
            else:
                stage = Globalstage(dim=int(embed_dim * 2 ** i_layer),
                                    depth=depths[i_layer],
                                    num_heads=num_heads[i_layer],
                                    mlp_ratio=self.mlp_ratio,
                                    qkv_bias=qkv_bias, qk_scale=qk_scale,
                                    drop=drop, attn_drop=attn_drop,
                                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                                    norm_layer=norm_layer,
                                    downsample=downsamples[i_layer],
                                    cpe_per_block=cpe_per_block,
                                    cpe_per_satge=cpe_per_satge,
                                    merging_way=merging_way
                                    )
            self.stages.append(stage)
        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
 
        self.apply(self._init_weights)
 
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
 
    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}
 
    def forward_features(self, x):
        x = self.patch_embed(x)
        for stage in self.stages:
            x = stage(x)
 
        x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x
 
    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
 
 
@register_model
def dilateformer_tiny(pretrained=True, **kwargs):
    model = Dilateformer(depths=[2, 2, 6, 2], embed_dim=72, num_heads=[ 3, 6, 12, 24 ], **kwargs)
    model.default_cfg = _cfg()
    return model
 
 
@register_model
def dilateformer_small(pretrained=True, **kwargs):
    model = Dilateformer(depths=[3, 5, 8, 3], embed_dim=72, num_heads=[ 3, 6, 12, 24 ],  **kwargs)
    model.default_cfg = _cfg()
    return model
 
 
@register_model
def dilateformer_base(pretrained=True, **kwargs):
    model = Dilateformer(depths=[4, 8, 10, 3], embed_dim=96, num_heads=[ 3, 6, 12, 24 ],  **kwargs)
    model.default_cfg = _cfg()
    return model
 
 
 
 
 
if __name__ == "__main__":
    x = torch.rand([1, 3, 224,224])
    m = dilateformer_tiny(pretrained=False)
    y = m(x)
    print(y.shape)
```

* * *

å››ã€æ‰‹æŠŠæ‰‹æ•™ä½ æ·»åŠ MSDAæ¨¡å—
---------------

### 4.1Â MSDAæ·»åŠ æ­¥éª¤

#### 4.1.1 æ­¥éª¤ä¸€

é¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„ç›®å½•'ultralytics/nn/modules'ï¼Œç„¶ååœ¨è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶ï¼Œåå­—ä¸ºDilationå³å¯(ä½ ä¹Ÿå¯ä»¥æ ¹æ®ä½ è‡ªå·±çš„ä¹ æƒ¯èµ·)ï¼Œç„¶åå°†MSDAçš„æ ¸å¿ƒä»£ç å¤åˆ¶è¿›å»ã€‚

#### 4.1.2 æ­¥éª¤äºŒ

ä¹‹åæˆ‘ä»¬æ‰¾åˆ°'ultralytics/nn/tasks.py'æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æ³¨å†Œæˆ‘ä»¬çš„MSDAæ¨¡å—ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦åœ¨æ–‡ä»¶çš„å¼€å¤´å¯¼å…¥æˆ‘ä»¬çš„MSDAæ¨¡å—ï¼Œ**å¦‚ä¸‹å›¾æ‰€ç¤º->**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a2bf127c2b9946918670d5f47917e869.png)â€‹

#### 4.1.3 æ­¥éª¤ä¸‰

æˆ‘ä»¬æ‰¾åˆ°parse\_modelè¿™ä¸ªæ–¹æ³•ï¼Œå¯ä»¥ç”¨æœç´¢ä¹Ÿå¯ä»¥è‡ªå·±æ‰‹åŠ¨æ‰¾ï¼Œå¤§æ¦‚åœ¨å…­ç™¾å¤šè¡Œå§ã€‚Â æˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„åœ°æ–¹ï¼Œç„¶åå°†MSDAæ·»åŠ è¿›å»å³å¯ï¼Œæ¨¡ä»¿æˆ‘æ·»åŠ å³å¯ï¼Œå…¶ä¸­çš„ä¸¤å¤–ä¸¤ä¸ªæ¨¡å—ï¼Œä½ æ²¡æœ‰åˆ é™¤å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/aa8351605bf14199a987db4122071f53.png)â€‹

**åˆ°æ­¤æˆ‘ä»¬å°±æ³¨å†ŒæˆåŠŸäº†ï¼Œå¯ä»¥ä¿®æ”¹yamlæ–‡ä»¶ä¸­è¾“å…¥MSDAä½¿ç”¨è¿™ä¸ªæ¨¡å—äº†ã€‚**

* * *

### 4.2 MSDAçš„yamlæ–‡ä»¶å’Œè®­ç»ƒæˆªå›¾

**ä¸‹é¢æ¨èå‡ ä¸ªç‰ˆæœ¬çš„yamlæ–‡ä»¶ç»™å¤§å®¶ï¼Œå¤§å®¶å¯ä»¥å¤åˆ¶è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯ç»„åˆç”¨å¾ˆå¤šå…·ä½“é‚£ç§æœ€æœ‰æ•ˆæœéƒ½ä¸ä¸€å®šï¼Œé’ˆå¯¹ä¸åŒçš„æ•°æ®é›†æ•ˆæœä¹Ÿä¸ä¸€æ ·ï¼Œæˆ‘ä¸å¯æ¯ä¸€ç§éƒ½åšå®éªŒï¼Œæ‰€ä»¥æˆ‘ä¸‹é¢æ¨èäº†å‡ ç§æˆ‘è‡ªå·±è®¤ä¸ºå¯èƒ½æœ‰æ•ˆæœçš„é…åˆæ–¹å¼ï¼Œä½ ä¹Ÿå¯ä»¥è‡ªå·±è¿›è¡Œç»„åˆã€‚**

* * *

#### 4.2.1 MSDAçš„yamlç‰ˆæœ¬ä¸€(æ¨è)

ä¸‹é¢çš„æ·»åŠ MSDAæ˜¯æˆ‘å®éªŒç»“æœçš„ç‰ˆæœ¬ï¼Œæˆ‘ä»…åœ¨å¤§ç›®æ ‡æ£€æµ‹å±‚çš„è¾“å‡ºæ·»åŠ äº†ä¸€ä¸ªMSDAæ¨¡å—ï¼Œå°±æ¶¨ç‚¹äº†0.05å·¦å³ï¼Œæ‰€ä»¥å¤§å®¶å¯ä»¥åœ¨ä¸­ç­‰å’Œå°ç›®æ ‡æ£€æµ‹å±‚éƒ½æ·»åŠ MSDAæ¨¡å—è¿›è¡Œå°è¯•ï¼Œä¸‹é¢çš„yamlæ–‡ä»¶æˆ‘ä¼šç»™å¤§å®¶æ¨èã€‚

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
 
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)
 
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)
  - [-1, 1, MultiDilatelocalAttention, []]  # 22
 
  - [[15, 18, 22], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

#### 4.2.2 MSDAçš„yamlç‰ˆæœ¬äºŒ

**æ·»åŠ çš„ç‰ˆæœ¬äºŒå…·ä½“é‚£ç§é€‚åˆä½ éœ€è¦å¤§å®¶è‡ªå·±å¤šåšå®éªŒæ¥å°è¯•ã€‚**

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
  - [-1, 1, MultiDilatelocalAttention, []]  # 16
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 19 (P4/16-medium)
  - [-1, 1, MultiDilatelocalAttention, []]  # 20
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 23 (P5/32-large)
  - [-1, 1, MultiDilatelocalAttention, []]  # 24
 
  - [[16, 20, 24], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

### 4.3 æ¨èMSDAå¯æ·»åŠ çš„ä½ç½®Â 

**MSDAæ˜¯ä¸€ç§å³æ’å³ç”¨çš„å¯æ›¿æ¢å·ç§¯çš„æ¨¡å—**ï¼Œå…¶å¯ä»¥æ·»åŠ çš„ä½ç½®æœ‰å¾ˆå¤šï¼Œæ·»åŠ çš„ä½ç½®ä¸åŒæ•ˆæœä¹Ÿä¸åŒï¼Œæ‰€ä»¥æˆ‘ä¸‹é¢æ¨èå‡ ä¸ªæ·»åŠ çš„ä½ï¼Œç½®å¤§å®¶å¯ä»¥è¿›è¡Œå‚è€ƒï¼Œå½“ç„¶ä¸ä¸€å®šè¦æŒ‰ç…§æˆ‘æ¨èçš„åœ°æ–¹æ·»åŠ ã€‚

> 1.  **æ®‹å·®è¿æ¥ä¸­**ï¼šåœ¨æ®‹å·®ç½‘ç»œçš„æ®‹å·®è¿æ¥ä¸­åŠ å…¥MHSA**ã€‚**
>     
> 2.  **Neckéƒ¨åˆ†**ï¼šYOLOv8çš„Neckéƒ¨åˆ†è´Ÿè´£ç‰¹å¾èåˆï¼Œè¿™é‡Œæ·»åŠ MSDAå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°èåˆä¸åŒå±‚æ¬¡çš„ç‰¹å¾(**yamlæ–‡ä»¶ä¸€å’ŒäºŒ**)ã€‚
>     
> 3.  **Backbone**ï¼šå¯ä»¥æ›¿æ¢ä¸­å¹²ç½‘ç»œä¸­çš„å·ç§¯éƒ¨åˆ†
>     
> 4.  èƒ½æ·»åŠ çš„ä½ç½®å¾ˆå¤šï¼Œä¸€ç¯‡æ–‡ç« å¾ˆéš¾å…¨éƒ¨ä»‹ç»åˆ°ï¼ŒåæœŸæˆ‘ä¼šå‘æ–‡ä»¶é‡Œé¢é›†æˆä¸Šç™¾ç§çš„æ”¹è¿›æœºåˆ¶ï¼Œç„¶åè¿˜æœ‰è®¸å¤šèåˆæ¨¡å—ï¼Œç»™å¤§å®¶ã€‚
>     

* * *

### 4.4 MSDAçš„è®­ç»ƒè¿‡ç¨‹æˆªå›¾Â  

**ä¸‹é¢æ˜¯æ·»åŠ äº†**MSDA**çš„è®­ç»ƒæˆªå›¾ã€‚**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/abc37d81e6d04e5b8c93957779f9df53.png)â€‹

â€‹â€‹
