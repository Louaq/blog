# Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings

## 摘要

The **state-of-the-art semantic segmentation methods** have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort.We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in
Int J Comput Visi 111(1):98–136, 2015) at 60% and 65% mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at https://github.com/YvanYin/SSIW.

## 翻译

当前最先进的语义分割方法在预设的封闭数据集上表现出色，但其在零样本（zero-shot）领域和未见过类别上的泛化能力仍然有限。由于大规模数据标注既困难又昂贵，开发跨领域通用的鲁棒语义分割模型成为研究热点。然而，现有公开数据集的不同分类标准阻碍了它们的直接融合。为此，我们提出了一种高效扩展多领域语义分割数据集的方法：用文本嵌入（text embedding）替代传统类别标签，这种向量化的语义表示可以融合不同领域、不同标签体系的数据。通过整合包含 200 万张图像的精细标注数据与公开的带噪声弱标注数据，我们训练的模型在 7 个主流测试集上达到了监督学习的顶尖水平，尽管完全没有使用这些测试集的训练图像。与人工统一标签体系不同，我们通过语言模型生成短文本描述来表征类别语义。在标准数据集微调后，模型在 NYUD-V2 (Silberman et al., 2012) 和 PASCAL-context (Everingham et al., 2015) 上分别取得 60% 和 65% 的平均交并比（mIoU），显著超越现有监督方法。该方法通过计算语义相似度实现未见过标签的分割，展现出优异的跨领域泛化能力，同时在深度估计、实例分割等下游任务中带来显著性能提升。代码已开源：https://github.com/YvanYin/SSIW



## 研究背景





















## 研究现状



















## 提出的模型























## 实验（Compared with SOTA）





























## 实验（Ablation Experiments）:1st_place_medal:























## 结论







































