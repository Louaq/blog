# ODConv 

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

è¿™ç¯‡æ–‡ç« ç»™å¤§å®¶å¸¦æ¥çš„æ˜¯å‘è¡¨äº2022å¹´çš„**ODConv**(Omni-Dimensional Dynamic Convolution)**ä¸­æ–‡åå­—å…¨ç»´åº¦åŠ¨æ€å·ç§¯**ï¼Œè¯¥å·ç§¯å¯ä»¥**å³æ’å³ç”¨**ï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢ç½‘ç»œç»“æ„ä¸­çš„ä»»ä½•ä¸€ä¸ªå·ç§¯æ¨¡å—ï¼Œ**åœ¨æœ¬æ–‡çš„æœ«å°¾æä¾›å¯ä»¥ç›´æ¥æ›¿æ¢å·ç§¯æ¨¡å—çš„ODConvï¼Œæ·»åŠ ODConvæ¨¡å—çš„C2få’ŒBottleneck(é…åˆæ•™ç¨‹å°†ä»£ç å¤åˆ¶ç²˜è´´åˆ°ä½ è‡ªå·±çš„ä»£ç ä¸­å³å¯è¿è¡Œ)ç»™å¤§å®¶**ï¼Œè¯¥å·ç§¯æ¨¡å—ä¸»è¦å…·æœ‰æ›´å°çš„è®¡ç®—é‡å’Œæ›´é«˜çš„ç²¾åº¦ï¼Œå…¶ä¸­æ·»åŠ ODConvæ¨¡å—çš„ç½‘ç»œ(åªæ›¿æ¢äº†ä¸€å¤„C2fä¸­çš„å·ç§¯)å‚æ•°é‡ç”±8.9GFLOPSå‡å°åˆ°8.8GFLOPSï¼Œ**ç²¾åº¦ä¹Ÿæœ‰æé«˜**ä¸‹é¢çš„å›¾ç‰‡æ˜¯ç²¾åº¦çš„å¯¹æ¯”(å› ä¸ºè®­ç»ƒæˆæœ¬æˆ‘åªæ˜¯ç”¨äº†ç›¸åŒçš„æ•°æ®é›†100å¼ å›¾ç‰‡é™¤äº†ä¿®æ”¹äº†ODConvä»¥åå…¶ä»–é…ç½®éƒ½ç›¸åŒä¸‹é¢æ˜¯æ•ˆæœå¯¹æ¯”å›¾å·¦é¢ä¸ºä¿®æ”¹ç‰ˆæœ¬ï¼Œå³é¢ä¸ºåŸºç¡€ç‰ˆæœ¬)

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/96441984e1124a65874cbbe26f4c65f3.png)Â 

* * *

äºŒã€åŸºæœ¬åŸç†ä»‹ç»
--------

![](https://img-blog.csdnimg.cn/7746cbcb513641839f0862bc2cbf1295.png)

**è®ºæ–‡åœ°å€ï¼š[è®ºæ–‡åœ°å€ç‚¹å‡»å³å¯è·³è½¬é˜…è¯»](https://openreview.net/pdf?id=DmpCfq6Mg39 "è®ºæ–‡åœ°å€ç‚¹å‡»å³å¯è·³è½¬é˜…è¯»")**

**ä»£ç åœ°å€ï¼šæ–‡æœ«æä¾›å¤åˆ¶ç²˜è´´çš„ä»£ç å—**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/71538e92b9824062ab2bd1f7ca98f464.png)

* * *

å¤§å®¶ä¼°è®¡åªæ˜¯å†²ç€ä»£ç æ¥çœ‹ï¼Œä¼°è®¡å¾ˆå°‘æƒ³è¦çœ‹å…¶åŸç†çš„ï¼Œ**æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œåªåšç•¥å¾®çš„ä»‹ç»å’Œç®€å•çš„è®²è§£**ï¼Œæœ€èµ·ç çŸ¥é“å…¶åŸºæœ¬çš„åŸç†ã€‚

* * *

### 2.1ODConvåŸºæœ¬åŸç†ä»‹ç»Â 

ODConvçš„**åˆ›æ–°ä¹‹å¤„**åœ¨äºå®ƒ**é‡‡ç”¨äº†ä¸€ç§å¤šç»´æ³¨æ„åŠ›æœºåˆ¶**ã€‚è¿™ç§æœºåˆ¶é€šè¿‡å¹¶è¡Œç­–ç•¥æ¥å­¦ä¹ **å·ç§¯æ ¸**åœ¨æ ¸ç©ºé—´æ‰€æœ‰å››ä¸ªç»´åº¦ï¼ˆå³ç©ºé—´å¤§å°ã€è¾“å…¥é€šé“æ•°å’Œæ¯ä¸ªå·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ï¼‰ä¸Šçš„äº’è¡¥æ³¨æ„åŠ›ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºä»»ä½•å·ç§¯å±‚ï¼Œå¢å¼ºäº†ç½‘ç»œçš„çµæ´»æ€§å’Œé€‚åº”æ€§(è¿™ä¸ªå››ä¸ªç»´åº¦çš„å·ç§¯å¯ä»¥åœ¨ä»£ç ä¸­æ¸…æ™°çš„ä½“ç°å‡ºæ¥)

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a07e54215c004ad6bd3b28face1788dc.png)

ODConvå·ç§¯ä¸»è¦çš„æ”¹è¿›æœºåˆ¶å°±æ˜¯åœ¨ä¸Šé¢çš„åœ°æ–¹ä½“ç°å‡ºæ¥çš„ï¼Œè¿™ä¸ªå˜é‡åaggregate\_weightå°±æ˜¯æ–‡ä¸­æåˆ°çš„å››ä¸ªç»´åº¦ï¼Œå…¶ä¸­çš„é€šé“æ•°\[16ï¼Œ16ï¼Œ3ï¼Œ3\]çš„å«ä¹‰ä¸ºå…¶å…·æœ‰16ä¸ªç»´åº¦ï¼Œæ¯ä¸ªå·ç§¯æ ¸æœ‰16ä¸ªé€šé“ï¼Œå·ç§¯æ ¸çš„å¤§å°æ˜¯3x3ï¼Œæ‰€ä»¥è¿™å¤„å°±æ˜¯ä½“ç°å…¶å…·ä½“æ”¹åŠ¨çš„åœ°æ–¹ï¼Œè¿™å¤„çš„ä»£ç å¯ä»¥åœ¨æˆ‘æ–‡æœ«æä¾›çš„ä»£ç ä¸­å¯ä»¥æ‰¾åˆ°å¦‚æœæƒ³è¦äº†è§£å¯ä»¥è‡ªå·±debugçœ‹ä¸€ä¸‹ã€‚

é‚£ä¹ˆå¤§å®¶å¯èƒ½æƒ³é—®äº†ï¼Œå®ƒæ˜¯å¦‚ä½•å‡å°‘è®¡ç®—é‡çš„å‘¢ï¼Ÿå› ä¸ºä»–å…·æœ‰16ä¸ªé€šé“æ•°æ‰€ä»¥ä»–è®¡ç®—æ˜¯é€šè¿‡å¹¶è¡Œçš„æ–¹å¼ï¼Œå› ä¸ºè¿™ä¸€æœºåˆ¶å¯¼è‡´å®ƒçš„è®¡ç®—é‡ä¹Ÿå˜å°äº†(**æ˜¯ä¸æ˜¯æ„Ÿè§‰ä¸€ä¸¾ä¸¤å¾—**)ã€‚

> **æ€»ç»“ï¼šå…¶å®ODConvå°±æ˜¯æå‡ºäº†è¿™ä¹ˆä¸€ç§å…·æœ‰å¤šç»´åº¦çš„å·ç§¯æ ¸æ‰€ä»¥å…¶å‡å°‘äº†è®¡ç®—é‡åŒæ—¶æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚**

* * *

### **2.2è®ºæ–‡æ€»ç»“**

Omni-Dimensional Dynamic Convolutionï¼ˆODConvï¼‰çš„åŸºæœ¬åŸç†æ˜¯å¯¹ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­å·ç§¯å±‚çš„è®¾è®¡è¿›è¡Œåˆ›æ–°ã€‚åœ¨ä¼ ç»Ÿçš„CNNä¸­ï¼Œæ¯ä¸ªå·ç§¯å±‚é€šå¸¸ä½¿ç”¨å›ºå®šçš„ã€é™æ€çš„å·ç§¯æ ¸æ¥æå–ç‰¹å¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒODConvå¼•å…¥äº†ä¸€ç§åŠ¨æ€çš„ã€å¤šç»´çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹å·ç§¯æ ¸çš„è®¾è®¡è¿›è¡Œäº†å…¨é¢çš„æ”¹è¿›ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»å…¶åŸç†ï¼š

> 1.  **å¤šç»´åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶**ï¼šODConvçš„æ ¸å¿ƒåˆ›æ–°æ˜¯å…¶å¤šç»´åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¼ ç»Ÿçš„åŠ¨æ€å·ç§¯é€šå¸¸åªåœ¨å·ç§¯æ ¸æ•°é‡è¿™ä¸€ä¸ªç»´åº¦ä¸Šå®ç°åŠ¨æ€æ€§ï¼Œå³é€šè¿‡å¯¹å¤šä¸ªå·ç§¯æ ¸è¿›è¡ŒåŠ æƒç»„åˆä»¥é€‚åº”ä¸åŒçš„è¾“å…¥ç‰¹å¾ã€‚ODConvåˆ™è¿›ä¸€æ­¥æ‰©å±•äº†è¿™ä¸€æ¦‚å¿µï¼Œå®ƒä¸ä»…åœ¨å·ç§¯æ ¸æ•°é‡ä¸ŠåŠ¨æ€è°ƒæ•´ï¼Œè¿˜æ¶‰åŠåˆ°å·ç§¯æ ¸çš„å…¶ä»–ä¸‰ä¸ªç»´åº¦ï¼šç©ºé—´å¤§å°ã€è¾“å…¥é€šé“æ•°ã€è¾“å‡ºé€šé“æ•°ã€‚è¿™æ„å‘³ç€ODConvèƒ½å¤Ÿæ›´ç²¾ç»†åœ°é€‚åº”è¾“å…¥æ•°æ®çš„ç‰¹å¾ï¼Œä»è€Œæé«˜ç‰¹å¾æå–çš„æ•ˆæœã€‚
>     
> 2.  **å¹¶è¡Œç­–ç•¥**ï¼šODConvé‡‡ç”¨å¹¶è¡Œç­–ç•¥æ¥åŒæ—¶å­¦ä¹ ä¸åŒç»´åº¦ä¸Šçš„æ³¨æ„åŠ›ã€‚è¿™ç§ç­–ç•¥å…è®¸ç½‘ç»œåœ¨å¤„ç†æ¯ä¸ªç»´åº¦çš„ç‰¹å¾æ—¶æ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶ç¡®ä¿å„ç»´åº¦ä¹‹é—´çš„äº’è¡¥æ€§å’ŒååŒä½œç”¨ã€‚
>     

**ä¸‹é¢çš„å›¾ç‰‡å·¦è¾¹çš„æ˜¯ä¼ ç»Ÿçš„åŠ¨æ€å·ç§¯å³è¾¹æ˜¯æ–‡ç« ä¸­æå‡ºçš„ODConvã€‚**Â 

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3f12f29373bc43c9a2150965cde58d77.png)

**ODConvçš„ç‹¬ç‰¹ä¹‹å¤„**ï¼šä¸DyConvå’ŒCondConvä¸åŒï¼ŒODConvé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¤šç»´æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§æœºåˆ¶ä¸æ˜¯ä»…é’ˆå¯¹æ¯ä¸ªå·ç§¯æ ¸è®¡ç®—ä¸€ä¸ªå•ä¸€çš„æ³¨æ„åŠ›æ ‡é‡ï¼Œè€Œæ˜¯æ²¿ç€å·ç§¯æ ¸ç©ºé—´çš„æ‰€æœ‰å››ä¸ªç»´åº¦è®¡ç®—å››ç§ç±»å‹çš„æ³¨æ„åŠ›ï¼šÎ±siâ€‹, Î±ciâ€‹, Î±fiâ€‹, å’ŒÎ±wiâ€‹ã€‚è¿™æ ·çš„è®¾è®¡å…è®¸ODConvåœ¨ç©ºé—´å¤§å°ã€è¾“å…¥é€šé“æ•°ã€è¿‡æ»¤å™¨æ•°é‡ï¼ˆè¾“å‡ºé€šé“æ•°ï¼‰å’Œå·ç§¯æ ¸æ•°é‡è¿™å››ä¸ªç»´åº¦ä¸Šè¿›è¡Œç»†ç²’åº¦çš„åŠ¨æ€è°ƒæ•´ã€‚**è¿™ä¸€å¤„åœ¨ä»£ç ä¸­ä¹Ÿæœ‰æ¸…æ™°çš„ä½“ç°å¦‚ä¸‹å›¾->**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/8dcb36cac2ef447987419a1c6bdcd25f.png)

**å¹¶è¡Œè®¡ç®—**ï¼šODConvèƒ½å¤Ÿä»¥å¹¶è¡Œçš„æ–¹å¼è®¡ç®—è¿™å››ç§ç±»å‹çš„æ³¨æ„åŠ›ï¼Œè¿™æé«˜äº†å…¶æ•ˆç‡ï¼Œå¹¶å…è®¸å®ƒæ›´å…¨é¢åœ°æ•æ‰å’Œåˆ©ç”¨è¾“å…¥æ•°æ®çš„å¤šç»´ç‰¹å¾ã€‚

ä¸Šé¢æåˆ°äº†æ¯ä¸ªé€šé“åˆ†åˆ«è®¡ç®—å…¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œé‚£ä¹ˆå…¶æ˜¯å¦‚ä½•ç”Ÿæ•ˆçš„å‘¢ï¼Ÿå¹¶ä¸”æé«˜æ¨¡å‹ç²¾åº¦çš„å‘¢ï¼ŸÂ 

ä¸‹é¢å›¾ç‰‡ä¸­åˆ†åˆ«å…·æœ‰aï¼Œbï¼Œcï¼Œdä»£è¡¨å››ä¸ªä¸åŒé€šé“çš„æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæ•ˆæ–¹å¼ã€‚Â 

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1cabbb6a3801429794565c81f47bd401.png)

å¦‚ä½•åœ¨å…¶å››ç§ä¸åŒç±»å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­é€æ­¥åº”ç”¨åˆ°å·ç§¯æ ¸ä¸Šçš„è¿‡ç¨‹çš„å‘¢ï¼Œä¸‹é¢æ˜¯å¯¹è¿™ä¸€è¿‡ç¨‹çš„è§£é‡Šï¼š

1.  **ä½ç½®ç»´åº¦çš„é€ç‚¹ä¹˜æ³•ï¼ˆLocation-wise Multiplicationï¼‰**ï¼šå¦‚ä¸Šå›¾(a)æ‰€ç¤ºï¼Œè¿™ä¸€æ­¥æ¶‰åŠåˆ°æ²¿ç€å·ç§¯æ ¸çš„ç©ºé—´ç»´åº¦ï¼ˆå³å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦ï¼‰è¿›è¡Œçš„ä¹˜æ³•æ“ä½œã€‚åœ¨è¿™é‡Œï¼ŒODConvè®¡ç®—çš„ä½ç½®ç»´åº¦æ³¨æ„åŠ›ï¼ˆÎ±siâ€‹ï¼‰è¢«åº”ç”¨äºå·ç§¯æ ¸çš„æ¯ä¸ªç©ºé—´ä½ç½®ä¸Šï¼Œè¿™å…è®¸ç½‘ç»œåŠ¨æ€è°ƒæ•´å·ç§¯æ ¸åœ¨å¤„ç†ä¸åŒç©ºé—´ä½ç½®çš„ä¿¡æ¯æ—¶çš„é‡è¦æ€§ã€‚
    
2.  **é€šé“ç»´åº¦çš„é€é€šé“ä¹˜æ³•ï¼ˆChannel-wise Multiplicationï¼‰**ï¼šå¦‚ä¸Šå›¾(b)æ‰€ç¤ºï¼Œè¿™ä¸€æ­¥æ˜¯æ²¿ç€è¾“å…¥é€šé“ç»´åº¦è¿›è¡Œçš„ä¹˜æ³•æ“ä½œã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œé€šé“ç»´åº¦çš„æ³¨æ„åŠ›ï¼ˆÎ±ciâ€‹ï¼‰è¢«åº”ç”¨äºå·ç§¯æ ¸çš„æ¯ä¸ªè¾“å…¥é€šé“ä¸Šï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒçš„è¾“å…¥ç‰¹å¾é€šé“åŠ¨æ€è°ƒæ•´å…¶å¤„ç†æ–¹å¼ã€‚
    
3.  **è¾“å‡ºé€šé“ç»´åº¦çš„é€æ»¤æ³¢å™¨ä¹˜æ³•ï¼ˆFilter-wise Multiplicationï¼‰**ï¼šå¦‚ä¸Šå›¾(c)æ‰€ç¤ºï¼Œè¿™ä¸€æ­¥æ˜¯æ²¿ç€è¾“å‡ºé€šé“ç»´åº¦è¿›è¡Œçš„ä¹˜æ³•æ“ä½œã€‚åœ¨è¿™é‡Œï¼Œè¾“å‡ºé€šé“ç»´åº¦çš„æ³¨æ„åŠ›ï¼ˆÎ±fiâ€‹ï¼‰å½±å“å·ç§¯æ ¸çš„æ¯ä¸ªè¾“å‡ºæ»¤æ³¢å™¨ï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿæ ¹æ®ä¸åŒè¾“å‡ºç‰¹å¾çš„é‡è¦æ€§è¿›è¡Œè°ƒæ•´ã€‚
    
4.  **å·ç§¯æ ¸ç»´åº¦çš„é€æ ¸ä¹˜æ³•ï¼ˆKernel-wise Multiplicationï¼‰**ï¼šå¦‚ä¸Šå›¾(d)æ‰€ç¤ºï¼Œè¿™ä¸€æ­¥æ˜¯æ²¿ç€å·ç§¯æ ¸ç»´åº¦è¿›è¡Œçš„ä¹˜æ³•æ“ä½œã€‚å·ç§¯æ ¸ç»´åº¦çš„æ³¨æ„åŠ›ï¼ˆÎ±wiâ€‹ï¼‰åœ¨è¿™ä¸€é˜¶æ®µè¢«åº”ç”¨ï¼Œå®ƒå…è®¸ç½‘ç»œåŠ¨æ€è°ƒæ•´ä¸åŒå·ç§¯æ ¸çš„é‡è¦æ€§ã€‚
    

é€šè¿‡è¿™ç§åˆ†æ­¥éª¤çš„ä¹˜æ³•æ“ä½œï¼ŒODConvèƒ½å¤Ÿåœ¨å·ç§¯è¿‡ç¨‹ä¸­ç»¼åˆè€ƒè™‘ç©ºé—´ä½ç½®ã€è¾“å…¥é€šé“ã€è¾“å‡ºé€šé“å’Œå·ç§¯æ ¸æ•°é‡ç­‰å¤šä¸ªç»´åº¦çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°å¯¹å·ç§¯æ ¸çš„å…¨æ–¹ä½åŠ¨æ€è°ƒæ•´ã€‚è¿™ç§ç»†è‡´çš„è°ƒæ•´æ–¹å¼ä½¿å¾—ODConvèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æå–ç‰¹å¾ï¼Œå¢å¼ºç½‘ç»œçš„å­¦ä¹ å’Œè¡¨è¾¾èƒ½åŠ›ã€‚è®ºæ–‡çš„æ–¹æ³•éƒ¨åˆ†å¯¹è¿™äº›æ“ä½œçš„å…·ä½“æ•°å­¦è¡¨è¾¾å’Œå®ç°ç»†èŠ‚ã€‚

ä¸‰ã€ODConvä»£ç Â 
-----------

ä¸‹é¢çš„ä»£ç å°±æ˜¯ODConvçš„ä»£ç ï¼Œå¤§å®¶å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒæ ·çš„æ–‡ä»¶å¦‚ä¸‹"ultralytics/nn/modules/ODConv.py" ï¼Œæˆ‘ä»¬åˆ›å»ºè¯¥æ–‡ä»¶ä¹‹åå°†ä¸‹é¢çš„ä»£ç å¤åˆ¶ç²˜è´´åˆ°å…¶ä¸­ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd
 
 
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
 
 
class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""
    default_act = nn.SiLU()  # default activation
 
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
 
    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))
 
    def forward_fuse(self, x):
        """Perform transposed convolution of 2D data."""
        return self.act(self.conv(x))
 
class Attention(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size=3, groups=1, reduction=0.0625, kernel_num=4, min_channel=16):
        super(Attention, self).__init__()
        attention_channel = max(int(in_planes * reduction), min_channel)
        self.kernel_size = kernel_size
        self.kernel_num = kernel_num
        self.temperature = 1.0
 
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)
        self.bn = nn.BatchNorm2d(attention_channel)
        self.relu = nn.ReLU(inplace=True)
 
        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)
        self.func_channel = self.get_channel_attention
 
        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution
            self.func_filter = self.skip
        else:
            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)
            self.func_filter = self.get_filter_attention
 
        if kernel_size == 1:  # point-wise convolution
            self.func_spatial = self.skip
        else:
            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)
            self.func_spatial = self.get_spatial_attention
 
        if kernel_num == 1:
            self.func_kernel = self.skip
        else:
            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)
            self.func_kernel = self.get_kernel_attention
 
        self._initialize_weights()
 
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            if isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
 
    def update_temperature(self, temperature):
        self.temperature = temperature
 
    @staticmethod
    def skip(_):
        return 1.0
 
    def get_channel_attention(self, x):
        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)
        return channel_attention
 
    def get_filter_attention(self, x):
        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)
        return filter_attention
 
    def get_spatial_attention(self, x):
        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)
        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)
        return spatial_attention
 
    def get_kernel_attention(self, x):
        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)
        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)
        return kernel_attention
 
    def forward(self, x):
        x = self.avgpool(x)
        x = self.fc(x)
        # x = self.bn(x) # åœ¨å¤–é¢æˆ‘æä¾›äº†ä¸€ä¸ªbnè¿™é‡Œä¼šæŠ¥é”™
        x = self.relu(x)
        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)
 
 
class ODConv2d(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=1, dilation=1, groups=1,
                 reduction=0.0625, kernel_num=4):
        super(ODConv2d, self).__init__()
        in_planes = in_planes
        self.in_planes = in_planes
        self.out_planes = out_planes
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.kernel_num = kernel_num
        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,
                                   reduction=reduction, kernel_num=kernel_num)
        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),
                                   requires_grad=True)
        self._initialize_weights()
 
        if self.kernel_size == 1 and self.kernel_num == 1:
            self._forward_impl = self._forward_impl_pw1x
        else:
            self._forward_impl = self._forward_impl_common
 
    def _initialize_weights(self):
        for i in range(self.kernel_num):
            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')
 
    def update_temperature(self, temperature):
        self.attention.update_temperature(temperature)
 
    def _forward_impl_common(self, x):
        # Multiplying channel attention (or filter attention) to weights and feature maps are equivalent,
        # while we observe that when using the latter method the models will run faster with less gpu memory cost.
        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)
        batch_size, in_planes, height, width = x.size()
        x = x * channel_attention
        x = x.reshape(1, -1, height, width)
        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)
        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(
            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])
        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,
                          dilation=self.dilation, groups=self.groups * batch_size)
        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))
        output = output * filter_attention
        return output
 
    def _forward_impl_pw1x(self, x):
        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)
        x = x * channel_attention
        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,
                          dilation=self.dilation, groups=self.groups)
        output = output * filter_attention
        return output
 
    def forward(self, x):
        return self._forward_impl(x)
 
 
class Bottleneck_ODConv(nn.Module):
    """Standard bottleneck."""
 
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
        expansion.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = ODConv2d(c_, c2, k[1][0], 1, groups=g)
        self.add = shortcut and c1 == c2
 
    def forward(self, x):
        """'forward()' applies the YOLO FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
 
 
class C2f_ODConv(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""
 
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck_ODConv(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))
 
    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
 
    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
```

* * *

å››ã€åœ¨æ¨¡å‹ä¸­è°ƒç”¨ODConvæ¨¡å—
----------------

### 4.1Â ODConvæ·»åŠ æ­¥éª¤

#### 4.1.1 æ­¥éª¤ä¸€

é¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„ç›®å½•'ultralytics/nn/modules'ï¼Œç„¶ååœ¨è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶ï¼Œåå­—ä¸ºä½ ä¹Ÿå¯ä»¥æ ¹æ®ä½ è‡ªå·±çš„ä¹ æƒ¯èµ·å³å¯ï¼Œç„¶åå°†æ ¸å¿ƒä»£ç å¤åˆ¶è¿›å»ã€‚

#### 4.1.2 æ­¥éª¤äºŒ

ä¹‹åæˆ‘ä»¬æ‰¾åˆ°'ultralytics/nn/tasks.py'æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æ³¨å†Œæˆ‘ä»¬çš„æ¨¡å—ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦åœ¨æ–‡ä»¶çš„å¼€å¤´å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—ï¼Œ**å¦‚ä¸‹å›¾æ‰€ç¤º->**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/582ad683608f477497bc7c7f1a89c140.png)



#### 4.1.3 æ­¥éª¤ä¸‰

æˆ‘ä»¬æ‰¾åˆ°parse\_modelè¿™ä¸ªæ–¹æ³•ï¼Œå¯ä»¥ç”¨æœç´¢ä¹Ÿå¯ä»¥è‡ªå·±æ‰‹åŠ¨æ‰¾ï¼Œå¤§æ¦‚åœ¨å…­ç™¾å¤šè¡Œå§ã€‚Â æˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„åœ°æ–¹ï¼Œç„¶åå°†æ¨¡å—æŒ‰ç…§æˆ‘çš„æ–¹æ³•æ·»åŠ è¿›å»å³å¯ï¼Œæ¨¡ä»¿æˆ‘æ·»åŠ å³å¯ï¼Œå…¶ä¸­å¦å¤–çš„æ¨¡å—ï¼Œä½ æ²¡æœ‰åˆ é™¤å³å¯ï¼Œæ·»åŠ çº¢æ¡†çš„å†…å®¹å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f167e65ab7ab429c9f7727774f23a976.png)â€‹

**åˆ°æ­¤æˆ‘ä»¬å°±æ³¨å†ŒæˆåŠŸäº†ï¼Œå¯ä»¥ä¿®æ”¹yamlæ–‡ä»¶ä½¿ç”¨æˆ‘ä»¬æ·»åŠ çš„æ¨¡å—äº†ã€‚**

* * *

### 4.2 ODConvçš„yamlæ–‡ä»¶ä¸€

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOP
 
# YOLOv8.0 backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f_ODConv, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f_ODConv, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f_ODConv, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
 
```

* * *

### 4.3 ODConvçš„yamlæ–‡ä»¶äºŒ

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOP
 
# YOLOv8.0 backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, ODConv2d, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, ODConv2d, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, ODConv2d, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, ODConv2d, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, ODConv2d, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, ODConv2d, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
 
```

