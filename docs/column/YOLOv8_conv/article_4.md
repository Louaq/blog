 # Context Guided Block

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

æœ¬æ–‡ç»™å¤§å®¶å¸¦æ¥çš„æ˜¯æ”¹è¿›æœºåˆ¶æ˜¯ä¸€ç§æ›¿æ¢Convçš„æ¨¡å—**Context Guided Block (CG block)**Â ï¼Œå…¶æ˜¯åœ¨CGNetè®ºæ–‡ä¸­æå‡ºçš„ä¸€ç§æ¨¡å—ï¼Œå…¶åŸºæœ¬åŸç†æ˜¯**æ¨¡æ‹Ÿäººç±»è§†è§‰ç³»ç»Ÿä¾èµ–ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç†è§£åœºæ™¯**ã€‚CG block ç”¨äºæ•è·å±€éƒ¨ç‰¹å¾ã€å‘¨å›´ä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†è¿™äº›**ä¿¡æ¯èåˆ**èµ·æ¥ä»¥æé«˜å‡†ç¡®æ€§ã€‚**(ç»è¿‡æˆ‘æ£€éªŒåˆ†åˆ«åœ¨ä¸‰ç§æ•°æ®é›†ä¸Šï¼Œå¤§ä¸­å°å‡è¿›è¡Œäº†150è½®æ¬¡çš„å®éªŒï¼Œå‡æœ‰ä¸€å®šç¨‹åº¦ä¸Šçš„æ¶¨ç‚¹ï¼Œä¸‹é¢æˆ‘é€‰å–äº†ä¸€ç§ä¸­ç­‰å¤§å°çš„æ•°æ®é›†çš„ç»“æœè¿›è¡Œäº†å¯¹æ¯”)**ï¼ŒåŒæ—¶æœ¬æ–‡çš„ä¿®æ”¹æ–¹æ³•å’Œä¹‹å‰çš„æ™®é€šå·ç§¯æ¨¡å—ä¹Ÿæœ‰æ‰€ä¸åŒï¼Œå¤§å®¶éœ€è¦æ³¨æ„çœ‹ç« èŠ‚å››è¿›è¡Œä¿®æ”¹ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/f15084a6bccc4358a6f70b8c55d6559c.png)

* * *

**äºŒã€ContextGuidedBlock\_Downæ¨¡å—åŸç†**
----------------------------------

![](https://img-blog.csdnimg.cn/direct/3a84787d86844123b85ad23eafd4f36d.png)â€‹

**è®ºæ–‡åœ°å€ï¼š[å®˜æ–¹è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1811.08201.pdf "å®˜æ–¹è®ºæ–‡åœ°å€")**

**ä»£ç åœ°å€ï¼š[å®˜æ–¹ä»£ç åœ°å€](https://github.com/wutianyiRosun/CGNet "å®˜æ–¹ä»£ç åœ°å€")**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e7b46a2e1fe840c79fc6fdeed35e4cd1.png)â€‹

* * *

### **2.1Â  ContextGuidedBlock\_Downçš„åŸºæœ¬åŸç†**

**Context Guided Block (CG block)** åœ¨CGNetä¸­çš„åŸºæœ¬åŸç†æ˜¯**æ¨¡æ‹Ÿäººç±»è§†è§‰ç³»ç»Ÿä¾èµ–ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç†è§£åœºæ™¯**ã€‚CG block ç”¨äºæ•è·å±€éƒ¨ç‰¹å¾ã€å‘¨å›´ä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯èåˆèµ·æ¥ä»¥æé«˜è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æ¨¡å—åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

**1\. å±€éƒ¨ç‰¹å¾æå–å™¨ï¼ˆflocï¼‰ï¼š**Â ä½¿ç”¨æ ‡å‡†å·ç§¯å±‚å­¦ä¹ å±€éƒ¨ç‰¹å¾ã€‚  
**2\. å‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨ï¼ˆfsurï¼‰ï¼š**Â ä½¿ç”¨ç©ºæ´/è†¨èƒ€å·ç§¯å±‚æ¥å­¦ä¹ æ›´å¤§æ¥æ”¶é‡çš„å‘¨å›´ä¸Šä¸‹æ–‡ã€‚  
**3\. è”åˆç‰¹å¾æå–å™¨ï¼ˆfjoiï¼‰ï¼š**Â é€šè¿‡è¿æ¥å±‚å’Œæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰ä»¥åŠå‚æ•°åŒ–ReLUï¼ˆPReLUï¼‰æ“ä½œæ¥èåˆå±€éƒ¨ç‰¹å¾å’Œå‘¨å›´ä¸Šä¸‹æ–‡çš„è¾“å‡ºï¼Œè·å–è”åˆç‰¹å¾ã€‚  
**4\. å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨ï¼ˆfgloï¼‰**ï¼šä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–å±‚èšåˆå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨æ¥è¿›ä¸€æ­¥æå–å…¨å±€ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œä½¿ç”¨ç¼©æ”¾å±‚ä»¥æå–çš„å…¨å±€ä¸Šä¸‹æ–‡å¯¹è”åˆç‰¹å¾è¿›è¡ŒåŠ æƒï¼Œä»¥å¼ºè°ƒæœ‰ç”¨çš„ç»„ä»¶å¹¶æŠ‘åˆ¶æ— ç”¨çš„ç»„ä»¶ã€‚

è¿™ä¸ªè¿‡ç¨‹æ˜¯è‡ªé€‚åº”çš„ï¼Œå› ä¸ºæå–çš„å…¨å±€ä¸Šä¸‹æ–‡æ˜¯åŸºäºè¾“å…¥å›¾åƒç”Ÿæˆçš„ã€‚CG block çš„è®¾è®¡å…è®¸CGNetèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»åº•å±‚åˆ°é¡¶å±‚èšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åœ¨**è¯­ä¹‰å±‚é¢**ï¼ˆæ¥è‡ªæ·±å±‚ï¼‰å’Œ**ç©ºé—´å±‚é¢**ï¼ˆæ¥è‡ªæµ…å±‚ï¼‰æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™å¯¹äºè¯­ä¹‰åˆ†å‰²è‡³å…³é‡è¦ã€‚

ä¸‹é¢å°±ä¸ºå¤§å®¶å±•ç¤ºäº†**ä¸‰ç§ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ä¸åŒæ¶æ„**ï¼š

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/efda067b88c84fe9ba032745c8e11886.png)



**(a) FCN-shapeï¼ˆå…¨å·ç§¯ç½‘ç»œå½¢çŠ¶ï¼‰ï¼š**Â æ­¤æ¨¡å‹éµå¾ªå›¾åƒåˆ†ç±»çš„è®¾è®¡åŸåˆ™ï¼Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®ƒå¯èƒ½ä½¿ç”¨ä¸€ç³»åˆ—å·ç§¯å’Œæ± åŒ–å±‚æ¥å¤„ç†è¾“å…¥å›¾åƒï¼Œå¹¶ç”Ÿæˆè¾“å‡ºï¼Œä½†æ²¡æœ‰æ˜¾å¼åœ°å¯¹å„ä¸ªå±‚æ¬¡çš„ç‰¹å¾å‘¨å›´ä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡ã€‚

**(b) FCN-CMï¼ˆå…¨å·ç§¯ç½‘ç»œ-ä¸Šä¸‹æ–‡æ¨¡å—ï¼‰ï¼š**Â æ­¤æ¨¡å‹åªåœ¨ç¼–ç é˜¶æ®µåæ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡æ‰§è¡Œä¸€ä¸ªä¸Šä¸‹æ–‡æ¨¡å—æ¥ä»è¯­ä¹‰å±‚æ¬¡æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**(c) æˆ‘ä»¬æå‡ºçš„CGNetï¼ˆä¸Šä¸‹æ–‡å¼•å¯¼ç½‘ç»œï¼‰ï¼š**Â æ•è·æ‰€æœ‰é˜¶æ®µçš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œä»è¯­ä¹‰å±‚æ¬¡å’Œç©ºé—´å±‚æ¬¡ä¸¤æ–¹é¢è¿›è¡Œã€‚

> Â **æ€»ç»“**ï¼šCGB\_Downçš„è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨å±€éƒ¨ç‰¹å¾ã€å‘¨å›´ä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡è¿™ç§ç»“æ„è®¾è®¡ï¼ŒCGNetèƒ½å¤Ÿåœ¨å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¹‹é—´å»ºç«‹è”ç³»ï¼Œè¿™å¯¹äºå‡†ç¡®åˆ†ç±»å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼ŒCGB\_Downè¿˜é‡‡ç”¨äº†æ®‹å·®å­¦ä¹ æ¥å¸®åŠ©å­¦ä¹ å¤æ‚ç‰¹å¾å¹¶åœ¨è®­ç»ƒæœŸé—´æ”¹å–„æ¢¯åº¦çš„åå‘ä¼ æ’­ã€‚

* * *

### 2.2Â Â **å±€éƒ¨ç‰¹å¾æå–å™¨**

**å±€éƒ¨ç‰¹å¾æå–å™¨**ï¼ˆè®°ä¸º${floc(*)}$ï¼‰æ˜¯ä¸Šä¸‹æ–‡å¼•å¯¼å—ï¼ˆCG blockï¼‰çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼Œä¸“é—¨ç”¨äº**å­¦ä¹ è¾“å…¥æ•°æ®ä¸­çš„å±€éƒ¨ç‰¹å¾**ã€‚åœ¨CGNetçš„è®¾è®¡ä¸­ï¼Œè¿™ä¸ªå±€éƒ¨ç‰¹å¾æå–å™¨é€šè¿‡**æ ‡å‡†çš„3Ã—3å·ç§¯å±‚**å®ç°ï¼Œå…¶ç›®çš„æ˜¯ä»å›¾åƒä¸­çš„å±€éƒ¨åŒºåŸŸæå–ç‰¹å¾ã€‚è¿™äº›å±€éƒ¨ç‰¹å¾éšåä¸å‘¨å›´ä¸Šä¸‹æ–‡ç‰¹å¾ç»“åˆï¼Œå½¢æˆäº†ç½‘ç»œå¯¹å„ä¸ªåŒºåŸŸçš„å…¨é¢ç†è§£ï¼Œè¿™å¯¹äºè¯­ä¹‰åˆ†å‰²å°¤ä¸ºé‡è¦ã€‚

CGNetä½¿ç”¨çš„å±€éƒ¨ç‰¹å¾æå–å™¨ä¸**å‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨**ï¼ˆ${fsur(*)}$ï¼‰ä¸€èµ·å·¥ä½œï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£æ¯ä¸ªåƒç´ æˆ–å±€éƒ¨åŒºåŸŸçš„ä¿¡æ¯ï¼Œè€Œä¸”è¿˜èƒ½ç†è§£è¿™äº›åŒºåŸŸåœ¨æ•´ä½“ä¸Šä¸‹æ–‡ä¸­çš„å…³ç³»ã€‚è¿™ç§æå–å™¨èƒ½å¤Ÿæ•æ‰åˆ°çš„ç»†èŠ‚å’Œå±€éƒ¨å˜åŒ–ä¿¡æ¯å¯¹äºç²¾ç¡®åœ°åˆ†ç±»å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦é¢„æµ‹çš„ä»»åŠ¡ä¸­ï¼Œå¦‚åœ¨å¤æ‚åœºæ™¯ä¸­åŒºåˆ†ä¸åŒçš„ç‰©ä½“å’Œè¡¨é¢ã€‚

CGNetçš„ç»“æ„è®¾è®¡è¿˜åŒ…æ‹¬**å‡å°‘å‚æ•°æ•°é‡**ï¼Œå…¶ä¸­å±€éƒ¨ç‰¹å¾æå–å™¨å’Œå‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨é‡‡ç”¨äº†**é€šé“å·ç§¯ï¼ˆchannel-wise convolutionsï¼‰**ï¼Œä»¥å‡å°‘è·¨é€šé“çš„è®¡ç®—æˆæœ¬å¹¶å¤§å¹…èŠ‚çº¦å†…å­˜ã€‚è¿™ç§è®¾è®¡å…è®¸CGNetå³ä½¿åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼ˆå¦‚ç§»åŠ¨è®¾å¤‡ï¼‰ä¹Ÿèƒ½æœ‰æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®ç‡å’Œå®æ—¶æ€§ã€‚

* * *

### 2.3Â Â **å‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨**

**å‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨**ï¼ˆ${f_{sur}(*)}$ï¼‰åœ¨CGNetæ¶æ„ä¸­çš„**ä½œç”¨å’ŒåŸç†**åŒ…æ‹¬ï¼š

**1\. æå–æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šå‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨ä½¿ç”¨æ‰©å±•å·ç§¯ï¼ˆä¾‹å¦‚ç©ºæ´å·ç§¯ï¼‰æ¥å¢åŠ æ„Ÿå—é‡çš„å¤§å°ï¼Œä»è€Œæ•è·æ›´å®½å¹¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™å…è®¸æ¨¡å‹è§‚å¯Ÿåˆ°æ›´å¤§åŒºåŸŸçš„ç‰¹å¾ï¼Œè€Œä¸ä»…ä»…æ˜¯å±€éƒ¨çš„ç»†èŠ‚ã€‚

**2\. è¾…åŠ©å±€éƒ¨ç‰¹å¾ç†è§£**ï¼šé€šè¿‡ç»“åˆå±€éƒ¨ç‰¹å¾å’Œå‘¨å›´ä¸Šä¸‹æ–‡ï¼ˆ${f_{sur}(*)}$ï¼‰èƒ½å¤Ÿæä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å¤æ‚çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è¾¨è¯†ä¸€ä¸ªç‰©ä½“æ—¶ï¼Œé™¤äº†ç‰©ä½“æœ¬èº«çš„ç‰¹å¾å¤–ï¼Œå®ƒçš„å‘¨å›´ç¯å¢ƒä¹Ÿæä¾›äº†é‡è¦çš„çº¿ç´¢ã€‚

**3\. æ”¹è¿›è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§**ï¼šç ”ç©¶è¡¨æ˜ï¼Œå‘¨å›´ä¸Šä¸‹æ–‡çš„ä¿¡æ¯å¯¹äºæé«˜è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§éå¸¸æœ‰ç›Šã€‚åœ¨ä¸åŒçš„æ¶æ„å®éªŒä¸­ï¼Œå¼•å…¥${f_{sur}(*)}$éƒ½èƒ½æ˜¾è‘—æå‡åˆ†å‰²çš„å‡†ç¡®ç‡ã€‚

**4\. åœ¨ç½‘ç»œçš„æ‰€æœ‰å—ä¸­ä½¿ç”¨**ï¼šä¸ºäº†å……åˆ†åˆ©ç”¨å‘¨å›´ä¸Šä¸‹æ–‡çš„ä¼˜åŠ¿ï¼Œ${f_{sur}(*)}$åœ¨CGNetçš„æ‰€æœ‰å—ä¸­éƒ½æœ‰åº”ç”¨ï¼Œä»¥ä¿è¯æ•´ä¸ªç½‘ç»œéƒ½èƒ½å—ç›Šäºå‘¨å›´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æå–ã€‚

**5\. ç©ºé—´é‡‘å­—å¡”æ± åŒ–**ï¼šåœ¨ä¸€äº›å˜ä½“ä¸­ï¼Œ${f_{sur}(*)}$å¯èƒ½ä¼šé‡‡ç”¨ç©ºé—´é‡‘å­—å¡”æ± åŒ–æ¥èšåˆä¸åŒå°ºåº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹æ•æ‰ä»æœ€å°çš„ç»†èŠ‚åˆ°æ•´ä½“å¸ƒå±€çš„ä¸åŒå±‚é¢çš„ä¿¡æ¯ã€‚

> **æ€»ç»“**ï¼šé€šè¿‡è¿™äº›è®¾è®¡ï¼Œå‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨åŠ å¼ºäº†CGNetå¤„ç†å„ç§å°ºåº¦ä¿¡æ¯çš„èƒ½åŠ›ï¼Œè¿™åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚åœºæ™¯çš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å°¤å…¶é‡è¦ã€‚Â 

* * *

### 2.4Â Â **è”åˆç‰¹å¾æå–å™¨**

**è”åˆç‰¹å¾æå–å™¨**ï¼ˆ$f_{joi}(*)$ï¼‰åœ¨CGNetä¸­çš„ä½œç”¨æ˜¯æ•´åˆç”±å±€éƒ¨ç‰¹å¾æå–å™¨å’Œå‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨æå–çš„ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾åˆ†åˆ«æ•æ‰åˆ°äº†è¾“å…¥æ•°æ®çš„ç»†èŠ‚ï¼ˆå±€éƒ¨ç‰¹å¾ï¼‰å’Œæ›´å¹¿é˜”åŒºåŸŸå†…çš„ä¿¡æ¯ï¼ˆå‘¨å›´ä¸Šä¸‹æ–‡ï¼‰ã€‚è”åˆç‰¹å¾æå–å™¨çš„è®¾è®¡ç›®çš„æ˜¯ä¸ºäº†ä½¿å¾—ç½‘ç»œèƒ½å¤ŸåŒæ—¶è€ƒè™‘å±€éƒ¨å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæé«˜è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚**ä¸‹é¢æ˜¯å®ƒçš„ä¸€äº›å…³é”®ç‚¹ï¼š**

**1\. ç‰¹å¾èåˆ**ï¼šè”åˆç‰¹å¾æå–å™¨é€šè¿‡è¿æ¥ï¼ˆconcatenationï¼‰æ“ä½œå°†å±€éƒ¨ç‰¹å¾å’Œå‘¨å›´ä¸Šä¸‹æ–‡ç‰¹å¾ç»“åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªç»¼åˆçš„ç‰¹å¾è¡¨ç¤ºã€‚

**2\. å¢å¼ºç‰¹å¾è¡¨ç¤º**ï¼šè”åˆåçš„ç‰¹å¾é€šè¿‡æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalization, BNï¼‰å’Œå‚æ•°åŒ–çš„çº¿æ€§å•å…ƒï¼ˆParametric ReLU, PReLUï¼‰ç­‰æ“ä½œè¿›è¡Œè¿›ä¸€æ­¥çš„åŠ å·¥ï¼Œä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›ã€‚

**3\. å…¨å±€ä¸Šä¸‹æ–‡çš„æ•´åˆ**ï¼šåœ¨æŸäº›è®¾è®¡ä¸­ï¼Œè”åˆç‰¹å¾è¿˜ä¼šä¸å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆ$f_{glo}(*)$ï¼‰ç»“åˆï¼Œä»¥åˆ©ç”¨æ•´ä¸ªè¾“å…¥å›¾åƒçš„ä¿¡æ¯æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾ã€‚

è”åˆç‰¹å¾æå–å™¨æ˜¯ä¸Šä¸‹æ–‡å¼•å¯¼ç½‘ç»œ**å®ç°å…¶é«˜æ•ˆè¯­ä¹‰åˆ†å‰²èƒ½åŠ›çš„å…³é”®è¿æ¥ç‚¹**ï¼Œå®ƒå…è®¸ç½‘ç»œåœ¨å±€éƒ¨ç²¾ç»†åº¦å’Œå…¨å±€ä¸Šä¸‹æ–‡é—´è¾¾åˆ°å¹³è¡¡.

ä¸‹å›¾ä¸ºå¤§å®¶å±•ç¤ºäº†**ä¸Šä¸‹æ–‡å¼•å¯¼ç½‘ç»œï¼ˆContext Guided Network, CGNetï¼‰çš„æ¶æ„**ã€‚è¿™ä¸ªç½‘ç»œé€šè¿‡ä»¥ä¸‹é˜¶æ®µå¤„ç†è¾“å…¥å›¾åƒæ¥ç”Ÿæˆé¢„æµ‹ï¼š

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a6bee5d5d0f74121bbf20053d83c2653.png)â€‹

**1\. Stage 1**ï¼šåŒ…å«è¿ç»­çš„3x3å·ç§¯å±‚ï¼Œè¿™äº›å±‚è´Ÿè´£æå–è¾“å…¥å›¾åƒçš„åˆæ­¥ç‰¹å¾ã€‚

**2\. Stage 2**ï¼šç”±å¤šä¸ªCGå—ç»„æˆï¼Œæ•°é‡ç”¨"M"è¡¨ç¤ºã€‚æ¯ä¸ªCGå—éƒ½ç»“åˆäº†å±€éƒ¨ç‰¹å¾æå–å™¨å’Œå‘¨å›´ä¸Šä¸‹æ–‡æå–å™¨ï¼Œå®ƒä»¬ä¸€èµ·å·¥ä½œä»¥æ•è·æ›´å¤æ‚çš„å±€éƒ¨å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**3\. Stage 3**ï¼šåŒ…å«æ›´å¤šçš„CGå—ï¼Œæ•°é‡ç”¨"N"è¡¨ç¤ºï¼Œè¿™ä¸€é˜¶æ®µè¿›ä¸€æ­¥æç‚¼ç‰¹å¾ï¼Œä»¥æ•æ‰æ›´é«˜å±‚æ¬¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**4\. 1x1 Conv**ï¼šä¸€ä¸ª1x1çš„å·ç§¯å±‚ç”¨äºå°†ç‰¹å¾æ˜ å°„åˆ°ç›®æ ‡ç±»åˆ«çš„æ•°é‡ï¼Œä¸ºæœ€ç»ˆçš„ä¸Šé‡‡æ ·å’Œåˆ†ç±»åšå‡†å¤‡ã€‚

**5\. ä¸Šé‡‡æ ·ï¼ˆUpsampleï¼‰**ï¼šä½¿ç”¨ä¸Šé‡‡æ ·æˆ–é€†å·ç§¯æ“ä½œå°†ç‰¹å¾å›¾å°ºå¯¸æ‰©å¤§å›è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚

**6\. é¢„æµ‹ï¼ˆPredictionï¼‰**ï¼šæœ€ç»ˆçš„é¢„æµ‹å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¢«åˆ†é…äº†ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ï¼Œå±•ç¤ºäº†å¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„ç»“æœã€‚

> **æ€»ç»“**ï¼šCGNetçš„è®¾è®¡æ—¨åœ¨å®ç°é«˜æ•ˆçš„è¯­ä¹‰åˆ†å‰²ï¼Œé€šè¿‡åœ¨ç½‘ç»œçš„ä¸åŒé˜¶æ®µåˆ©ç”¨å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è½»é‡çº§ç‰¹æ€§ã€‚è¿™ä½¿CGNetç‰¹åˆ«é€‚åˆäºèµ„æºå—é™çš„è®¾å¤‡ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼ç³»ç»Ÿã€‚åœ¨å›¾ä¸­çš„é¢„æµ‹ç¤ºä¾‹ä¸­ï¼Œå¯ä»¥çœ‹åˆ°ç½‘ç»œå·²ç»å°†ä¸åŒçš„äº¤é€šå‚ä¸è€…å’ŒèƒŒæ™¯è¦ç´ æˆåŠŸåœ°åˆ†å‰²å‡ºæ¥ï¼Œç”¨ä¸åŒçš„é¢œè‰²æ ‡è®°ä¸åŒçš„ç±»åˆ«ã€‚

* * *

### 2.5Â  **å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨**

**å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨**ï¼ˆ${f_{glo}(*)}$ï¼‰åœ¨CGNetä¸­çš„ä½œç”¨æ˜¯**æ•è·å¹¶åˆ©ç”¨æ•´ä¸ªè¾“å…¥å›¾åƒçš„å…¨å±€ä¿¡æ¯**ï¼Œä»¥å¢å¼ºè”åˆç‰¹å¾æå–å™¨å­¦ä¹ åˆ°çš„ç‰¹å¾ã€‚**ä»¥ä¸‹æ˜¯å®ƒçš„åŸºæœ¬åŸç†**ï¼š

**1\. å…¨å±€ç‰¹å¾æ±‡æ€»**ï¼šå…¨å±€ä¸Šä¸‹æ–‡æå–å™¨é€šè¿‡**å…¨å±€å¹³å‡æ± åŒ–**ï¼ˆGlobal Average Pooling, GAPï¼‰æ¥èšåˆæ•´ä¸ªç‰¹å¾å›¾çš„å…¨å±€ä¿¡æ¯ã€‚è¿™ä¸ªæ­¥éª¤äº§ç”Ÿä¸€ä¸ªå…¨å±€ç‰¹å¾å‘é‡ï¼Œå®ƒæ•è·äº†è¾“å…¥å›¾åƒä¸­æ¯ä¸ªé€šé“çš„å¹³å‡å“åº”ã€‚

**2\. å¤šå±‚æ„ŸçŸ¥æœºå¤„ç†**ï¼šå…¨å±€ç‰¹å¾å‘é‡éšåé€šè¿‡ä¸€ä¸ª**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMultilayer Perceptron, MLPï¼‰è¿›ä¸€æ­¥å¤„ç†ã€‚MLPèƒ½å¤Ÿå­¦ä¹ ç‰¹å¾é—´çš„å¤æ‚éçº¿æ€§å…³ç³»ï¼Œè¿›ä¸€æ­¥ç»†åŒ–å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚

**3\. ç‰¹å¾é‡æ ‡å®š**ï¼šæå–çš„å…¨å±€ä¸Šä¸‹æ–‡é€šè¿‡**ç¼©æ”¾å±‚**ï¼ˆscale layerï¼‰ä¸è”åˆç‰¹å¾ç»“åˆã€‚è¿™ä¸ªæ“ä½œç›¸å½“äºå°†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ä½œä¸ºæƒé‡ï¼Œé€šé“çº§åˆ«åœ°é‡æ–°æ ‡å®šè”åˆç‰¹å¾ï¼Œå¼ºè°ƒæœ‰ç”¨çš„ç‰¹å¾éƒ¨åˆ†ï¼ŒæŠ‘åˆ¶ä¸é‡è¦çš„ç‰¹å¾éƒ¨åˆ†ã€‚

**4\. è‡ªé€‚åº”æ€§**ï¼šå…¨å±€ä¸Šä¸‹æ–‡æå–å™¨çš„æ“ä½œæ˜¯è‡ªé€‚åº”çš„ï¼Œå› ä¸ºæå–çš„å…¨å±€ä¸Šä¸‹æ–‡æ˜¯æ ¹æ®è¾“å…¥å›¾åƒç”Ÿæˆçš„ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒçš„å›¾åƒç”Ÿæˆå®šåˆ¶åŒ–çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚

**5\. æé«˜åˆ†å‰²å‡†ç¡®æ€§**ï¼šåœ¨æ¶ˆèç ”ç©¶ä¸­ï¼Œä½¿ç”¨å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨å¯ä»¥æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚è¿™è¯æ˜äº†å…¨å±€ä¸Šä¸‹æ–‡åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„ä»·å€¼ã€‚

æä¾›äº†**ä¸Šä¸‹æ–‡å¼•å¯¼å—ï¼ˆContext Guided blockï¼‰çš„æ¦‚è§ˆ**ã€‚åœ¨å›¾ä¸­çš„å…¨å±€ä¸Šä¸‹æ–‡æå–å™¨${f_{glo}(*)}$éƒ¨åˆ†ï¼Œå±•ç¤ºäº†ä½¿ç”¨**å…¨å±€å¹³å‡æ± åŒ–**ï¼ˆGAPï¼‰æ¥æå–å…¨å›¾çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç„¶åé€šè¿‡**ä¸¤ä¸ªå…¨è¿æ¥å±‚**ï¼ˆFCï¼‰å¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†ã€‚è¿™æœ‰åŠ©äºç½‘ç»œç†è§£æ•´ä¸ªå›¾åƒçš„å…¨å±€ä¿¡æ¯ï¼Œè¿™å¯¹äºåˆ†ç±»å›¾åƒä¸­çš„å±€éƒ¨åŒºåŸŸç‰¹åˆ«é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨è¿™äº›å±€éƒ¨åŒºåŸŸçš„ç±»åˆ«å¯èƒ½ä¾èµ–äºå…¨å±€ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e12ec53c9fc8498a90cba0b1bc97fb58.png)â€‹

è¿™äº›ç»„ä»¶å…±åŒå·¥ä½œï¼Œæé«˜äº†ç½‘ç»œå¯¹å¤æ‚åœºæ™¯ä¸­å„ç§å°ºåº¦çš„ç‰¹å¾çš„ç†è§£èƒ½åŠ›ï¼Œä½¿å¾—CGNetèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚é€šè¿‡è¿™æ ·çš„è®¾è®¡ï¼ŒCGNetèƒ½å¤Ÿåœ¨å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¹‹é—´å»ºç«‹è”ç³»ï¼Œè¿™å¯¹äºå‡†ç¡®åˆ†ç±»å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è‡³å…³é‡è¦ã€‚

* * *

ä¸‰ã€**ContextGuidedçš„æ ¸å¿ƒä»£ç **
------------------------

æˆ‘ä»¬å¤åˆ¶ä¸‹é¢çš„ä»£ç 'ultralytics/nn/modules'åˆ°è¯¥ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶ç²˜è´´è¿›å»ï¼Œæˆ‘è¿™é‡Œèµ·åä¸ºContextGuided**(è¿™é‡Œæœ‰ä¸€ä¸ªæ³¨æ„ç‚¹æ˜¯ä¸è¦å’Œå®šä¹‰çš„ç±»é‡åæœ‰æ—¶å€™ä¼šæŠ¥é”™)ã€‚

```python
import torch
import torch.nn as nn
 
__all__ = ['C2f_Context', 'ContextGuidedBlock_Down']
 
class ConvBNPReLU(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1):
        """
        args:
            nIn: number of input channels
            nOut: number of output channels
            kSize: kernel size
            stride: stride rate for down-sampling. Default is 1
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2)
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)
        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)
        self.act = nn.PReLU(nOut)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        output = self.bn(output)
        output = self.act(output)
        return output
 
 
class BNPReLU(nn.Module):
    def __init__(self, nOut):
        """
        args:
           nOut: channels of output feature maps
        """
        super().__init__()
        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)
        self.act = nn.PReLU(nOut)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: normalized and thresholded feature map
        """
        output = self.bn(input)
        output = self.act(output)
        return output
 
 
class ConvBN(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1):
        """
        args:
           nIn: number of input channels
           nOut: number of output channels
           kSize: kernel size
           stride: optinal stide for down-sampling
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2)
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)
        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        output = self.bn(output)
        return output
 
 
class Conv(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1):
        """
        args:
            nIn: number of input channels
            nOut: number of output channels
            kSize: kernel size
            stride: optional stride rate for down-sampling
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2)
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        return output
 
 
class ChannelWiseConv(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1):
        """
        Args:
            nIn: number of input channels
            nOut: number of output channels, default (nIn == nOut)
            kSize: kernel size
            stride: optional stride rate for down-sampling
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2)
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups=nIn,
                              bias=False)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        return output
 
 
class DilatedConv(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1, d=1):
        """
        args:
           nIn: number of input channels
           nOut: number of output channels
           kSize: kernel size
           stride: optional stride rate for down-sampling
           d: dilation rate
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2) * d
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False,
                              dilation=d)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        return output
 
 
class ChannelWiseDilatedConv(nn.Module):
    def __init__(self, nIn, nOut, kSize, stride=1, d=1):
        """
        args:
           nIn: number of input channels
           nOut: number of output channels, default (nIn == nOut)
           kSize: kernel size
           stride: optional stride rate for down-sampling
           d: dilation rate
        """
        super().__init__()
        if isinstance(kSize, tuple):
            kSize = kSize[0]
        padding = int((kSize - 1) / 2) * d
        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups=nIn,
                              bias=False, dilation=d)
 
    def forward(self, input):
        """
        args:
           input: input feature map
           return: transformed feature map
        """
        output = self.conv(input)
        return output
 
class FGlo(nn.Module):
    """
    the FGlo class is employed to refine the joint feature of both local feature and surrounding context.
    """
 
    def __init__(self, channel, reduction=16):
        super(FGlo, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )
 
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y
 
class ContextGuidedBlock_Down(nn.Module):
    """
    the size of feature map divided 2, (H,W,C)---->(H/2, W/2, 2C)
    """
 
    def __init__(self, nIn, dilation_rate=2, reduction=16):
        """
        args:
           nIn: the channel of input feature map
           nOut: the channel of output feature map, and nOut=2*nIn
        """
        super().__init__()
 
        nOut = nIn
 
        self.conv1x1 = ConvBNPReLU(nIn, nOut, 3, 2)  # size/2, channel: nIn--->nOut
 
        self.F_loc = ChannelWiseConv(nOut, nOut, 3, 1)
        self.F_sur = ChannelWiseDilatedConv(nOut, nOut, 3, 1, dilation_rate)
 
        self.bn = nn.BatchNorm2d(2 * nOut, eps=1e-3)
        self.act = nn.PReLU(2 * nOut)
        self.reduce = Conv(2 * nOut, nOut, 1, 1)  # reduce dimension: 2*nOut--->nOut
 
        self.F_glo = FGlo(nOut, reduction)
 
    def forward(self, input):
        output = self.conv1x1(input)
        loc = self.F_loc(output)
        sur = self.F_sur(output)
 
        joi_feat = torch.cat([loc, sur], 1)  # the joint feature
        joi_feat = self.bn(joi_feat)
        joi_feat = self.act(joi_feat)
        joi_feat = self.reduce(joi_feat)  # channel= nOut
 
        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature
 
        return output
 
 
class ContextGuidedBlock(nn.Module):
    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16, add=True):
        """
        args:
           nIn: number of input channels
           nOut: number of output channels,
           add: if true, residual learning
        """
        super().__init__()
        n = int(nOut / 2)
        self.conv1x1 = ConvBNPReLU(nIn, n, 1, 1)  # 1x1 Conv is employed to reduce the computation
        self.F_loc = ChannelWiseConv(n, n, 3, 1)  # local feature
        self.F_sur = ChannelWiseDilatedConv(n, n, 3, 1, dilation_rate)  # surrounding context
        self.bn_prelu = BNPReLU(nOut)
        self.add = add
        self.F_glo = FGlo(nOut, reduction)
 
    def forward(self, input):
        output = self.conv1x1(input)
        loc = self.F_loc(output)
        sur = self.F_sur(output)
 
        joi_feat = torch.cat([loc, sur], 1)
 
        joi_feat = self.bn_prelu(joi_feat)
 
        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature
        # if residual version
        if self.add:
            output = input + output
        return output
 
 
class Bottleneck_Context(nn.Module):
    """Standard bottleneck."""
 
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
        expansion.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = ContextGuidedBlock_Down(c_)
        self.add = shortcut and c1 == c2
 
    def forward(self, x):
        """'forward()' applies the YOLO FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
 
class C2f_Context(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""
 
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(ContextGuidedBlock(self.c, self.c) for _ in range(n))
 
    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
 
    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
```

* * *

å››ã€ æ‰‹æŠŠæ‰‹æ•™ä½ æ·»åŠ ContextGuided(æ³¨æ„çœ‹æ­¤å¤„)
------------------------------

æœ¬æ–‡çš„å†…å®¹æ˜¯æ›¿æ¢ä¼ ç»Ÿå·ç§¯çš„ä¸‹é‡‡æ ·æ–¹æ³•ï¼Œå…¶ä½¿ç”¨æ–¹å¼å’Œä¹‹å‰çš„éƒ½ä¸å¤ªä¸€æ ·ï¼Œæ‰€ä»¥ä¸‹é¢çš„ä¿®æ”¹æ•™ç¨‹å¤§å®¶éœ€è¦ä»”ç»†çš„çœ‹ä¸€ä¸‹**(å¤§éƒ¨åˆ†éƒ½ä¸€æ ·åªæ˜¯éœ€è¦å¤šæ·»åŠ ä¸€è¡Œä»£ç )**ã€‚

### **4.1 ä¿®æ”¹ä¸€**

æˆ‘ä»¬å¤åˆ¶ä¸‹é¢çš„ä»£ç 'ultralytics/nn/modules'åˆ°è¯¥ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶ç²˜è´´è¿›å»ï¼Œæˆ‘è¿™é‡Œèµ·åä¸ºContextGuideï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º->

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6b75e374b09f48e081c3a245b14f5a29.png)



* * *

### 4.2 ä¿®æ”¹äºŒÂ 

æ‰¾åˆ°å¦‚ä¸‹çš„æ–‡ä»¶''ultralytics/nn/tasks.py''ï¼Œåœ¨æ–‡ä»¶çš„å¼€å¤´å¤„å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/3e86ac6ec6844eb89522ea0ea1de95be.png)â€‹



### 4.3 ä¿®æ”¹ä¸‰Â 

æ‰¾åˆ°parse\_modelçš„æ–¹æ³•ï¼Œåœ¨ä¸‹é¢æ³¨å†Œæˆ‘ä»¬çš„æ¨¡å—ï¼ŒæŒ‰ç…§å›¾ç‰‡è¿›è¡Œä¸‰å¤„ä¿®æ”¹ï¼ï¼ï¼ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/826eff16146843598bdee99f23addf72.png)â€‹

**åˆ°æ­¤å°±æ·»åŠ æˆåŠŸäº†ï¼Œåªéœ€è¦é…ç½®yamlæ–‡ä»¶ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å—å³å¯ã€‚**

* * *

äº”ã€ContextGuidedçš„yamlæ–‡ä»¶
----------------------

### 5.1 yamlæ–‡ä»¶1(å®éªŒç‰ˆæœ¬)

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f_Context, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f_Context, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f_Context, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f_Context, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 5.2 yamlæ–‡ä»¶2Â 

æ­¤ç‰ˆæœ¬çš„æ•ˆæœæˆ‘æ²¡æœ‰è¯•éªŒè¿‡ï¼Œå…·ä½“æƒ…å†µä¸çŸ¥é“ï¼Œå»ºè®®å¤§å®¶å…ˆä½¿ç”¨yamlæ–‡ä»¶ä¸€éªŒè¯æ•ˆæœä¹‹åï¼Œåœ¨æ¢ç´¢yamlæ–‡ä»¶2çš„æ•ˆæœã€‚

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, ContextGuidedBlock_Down, []]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, ContextGuidedBlock_Down, []]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, ContextGuidedBlock_Down, []]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, ContextGuidedBlock_Down, []]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

å…­ã€æˆåŠŸè¿è¡Œè®°å½•Â 
---------

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4c9c1c67dbc24c5a9b195e9454c9fd7b.png)â€‹

