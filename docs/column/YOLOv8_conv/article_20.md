 # GELANæ¨¡å—ï¼ˆ:100:ï¼‰

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

æœ¬æ–‡ç»™å¤§å®¶å¸¦æ¥çš„æ”¹è¿›æœºåˆ¶æ˜¯åˆ©ç”¨2024/02/21å·æœ€æ–°å‘å¸ƒçš„**YOLOv9å…¶ä¸­æå‡ºçš„GELANæ¨¡å—æ¥æ”¹è¿›YOLOv8ä¸­çš„C2f**ï¼ŒGELANèåˆäº†CSPNetå’ŒELANæœºåˆ¶åŒæ—¶å…¶ä¸­åˆ©ç”¨åˆ°äº†RepConvåœ¨è·å–æ›´å¤šæœ‰æ•ˆç‰¹å¾çš„åŒæ—¶åœ¨æ¨ç†æ—¶ä¸“ç”¨å•åˆ†æ”¯ç»“æ„ä»è€Œä¸å½±å“æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶æœ¬æ–‡çš„å†…å®¹æä¾›äº†ä¸¤ç§ç‰ˆæœ¬ä¸€ç§æ˜¯å‚æ•°é‡æ›´ä½æ¶¨ç‚¹æ•ˆæœç•¥å¾®å¼±ä¸€äº›çš„ç‰ˆæœ¬ï¼ˆå‚æ•°é‡V8nä¸‹é™80wï¼Œè®¡ç®—é‡ä¸º6.1GFLOPsï¼‰ï¼Œå¦ä¸€ç§æ˜¯å‚æ•°é‡ç¨å¤šä¸€äº›ä½†æ˜¯æ•ˆæœè¦ä¸å‚æ•°é‡ä½çš„æ•ˆæœè¦å¥½ä¸€äº›ï¼ˆå‡ä¸ºæˆ‘ä¸ªäººæ•´ç†ï¼‰ï¼Œæä¾›ä¸¤ç§ç‰ˆæœ¬æ˜¯ä¸ºäº†é€‚é…ä¸åŒéœ€æ±‚çš„è¯»è€…ï¼Œå…·ä½“é€‰æ‹©é‚£ç§å¤§å®¶å¯ä»¥æ ¹æ®è‡ªèº«çš„éœ€æ±‚æ¥é€‰æ‹©å³å¯ï¼Œæ–‡ç« ä¸­æˆ‘éƒ½å‡å·²æä¾›ï¼Œ**åŒæ—¶æœ¬æ–‡çš„ç»“æ„å­˜åœ¨å¤§é‡çš„äºŒæ¬¡åˆ›æ–°æœºä¼šï¼Œåé¢æˆ‘ä¹Ÿä¼šæä¾›ã€‚**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/59e47e866f3b47db9c9adfc786da0736.png)

* * *

äºŒã€GELANçš„åŸç†
----------

### 2.1Â Generalized ELAN

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†æå‡ºçš„æ–°ç½‘ç»œæ¶æ„ - GELANã€‚é€šè¿‡ç»“åˆä¸¤ç§ç¥ç»ç½‘ç»œæ¶æ„CSPNetå’ŒELANï¼Œè¿™ä¸¤ç§æ¶æ„éƒ½æ˜¯ä»¥æ¢¯åº¦è·¯å¾„è§„åˆ’è®¾è®¡çš„ï¼Œæˆ‘ä»¬è®¾è®¡äº†è€ƒè™‘äº†è½»é‡çº§ã€æ¨ç†é€Ÿåº¦å’Œå‡†ç¡®æ€§çš„å¹¿ä¹‰é«˜æ•ˆå±‚èšåˆç½‘ç»œï¼ˆGELANï¼‰ã€‚å…¶æ•´ä½“æ¶æ„å¦‚å›¾4æ‰€ç¤ºã€‚æˆ‘ä»¬æ¨å¹¿äº†ELANçš„èƒ½åŠ›ï¼ŒELANåŸæœ¬åªä½¿ç”¨å·ç§¯å±‚çš„å †å ï¼Œåˆ°ä¸€ä¸ªæ–°çš„æ¶æ„ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•è®¡ç®—å—ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/46b1bc49cbc8418a92eb97c8cf9acca9.png)

> è¿™å¼ å›¾ï¼ˆå›¾4ï¼‰å±•ç¤ºäº†å¹¿ä¹‰é«˜æ•ˆå±‚èšåˆç½‘ç»œï¼ˆGELANï¼‰çš„æ¶æ„ï¼Œä»¥åŠå®ƒæ˜¯å¦‚ä½•ä»CSPNetå’ŒELANè¿™ä¸¤ç§ç¥ç»ç½‘ç»œæ¶æ„æ¼”å˜è€Œæ¥çš„ã€‚è¿™ä¸¤ç§æ¶æ„éƒ½è®¾è®¡æœ‰æ¢¯åº¦è·¯å¾„è§„åˆ’ã€‚
> 
> **a) CSPNet**ï¼šåœ¨CSPNetçš„æ¶æ„ä¸­ï¼Œè¾“å…¥é€šè¿‡ä¸€ä¸ªè½¬æ¢å±‚è¢«åˆ†å‰²ä¸ºä¸¤éƒ¨åˆ†ï¼Œç„¶ååˆ†åˆ«é€šè¿‡ä»»æ„çš„è®¡ç®—å—ã€‚ä¹‹åï¼Œè¿™äº›åˆ†æ”¯è¢«é‡æ–°åˆå¹¶ï¼ˆé€šè¿‡concatenationï¼‰ï¼Œå¹¶å†æ¬¡é€šè¿‡è½¬æ¢å±‚ã€‚
> 
> **b) ELAN**ï¼šä¸CSPNetç›¸æ¯”ï¼ŒELANé‡‡ç”¨äº†å †å çš„å·ç§¯å±‚ï¼Œå…¶ä¸­æ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šä¸ä¸‹ä¸€å±‚çš„è¾“å…¥ç›¸ç»“åˆï¼Œå†ç»è¿‡å·ç§¯å¤„ç†ã€‚
> 
> **c) GELAN**ï¼šç»“åˆäº†CSPNetå’ŒELANçš„è®¾è®¡ï¼Œæå‡ºäº†GELANã€‚å®ƒé‡‡ç”¨äº†CSPNetçš„åˆ†å‰²å’Œé‡ç»„çš„æ¦‚å¿µï¼Œå¹¶åœ¨æ¯ä¸€éƒ¨åˆ†å¼•å…¥äº†ELANçš„å±‚çº§å·ç§¯å¤„ç†æ–¹å¼ã€‚ä¸åŒä¹‹å¤„åœ¨äºGELANä¸ä»…ä½¿ç”¨å·ç§¯å±‚ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä»»ä½•è®¡ç®—å—ï¼Œä½¿å¾—ç½‘ç»œæ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„åº”ç”¨éœ€æ±‚å®šåˆ¶ã€‚
> 
> GELANçš„è®¾è®¡è€ƒè™‘åˆ°äº†è½»é‡åŒ–ã€æ¨ç†é€Ÿåº¦å’Œç²¾ç¡®åº¦ï¼Œä»¥æ­¤æ¥æé«˜æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚å›¾ä¸­æ˜¾ç¤ºçš„æ¨¡å—å’Œåˆ†åŒºçš„å¯é€‰æ€§è¿›ä¸€æ­¥å¢åŠ äº†ç½‘ç»œçš„é€‚åº”æ€§å’Œå¯å®šåˆ¶æ€§ã€‚GELANçš„è¿™ç§ç»“æ„å…è®¸å®ƒæ”¯æŒå¤šç§ç±»å‹çš„è®¡ç®—å—ï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥æ›´å¥½åœ°é€‚åº”å„ç§ä¸åŒçš„è®¡ç®—éœ€æ±‚å’Œç¡¬ä»¶çº¦æŸã€‚
> 
> æ€»çš„æ¥è¯´ï¼ŒGELANçš„æ¶æ„æ˜¯ä¸ºäº†æä¾›ä¸€ä¸ªæ›´åŠ é€šç”¨å’Œé«˜æ•ˆçš„ç½‘ç»œï¼Œå¯ä»¥é€‚åº”ä»è½»é‡çº§åˆ°å¤æ‚çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒæˆ–å¢å¼ºè®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGELANæ—¨åœ¨è§£å†³ç°æœ‰æ¶æ„çš„é™åˆ¶ï¼Œæä¾›ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥é€‚åº”æœªæ¥æ·±åº¦å­¦ä¹ çš„å‘å±•ã€‚
> 
> **å¤§å®¶çœ‹å›¾ç‰‡ä¸€çœ¼å°±èƒ½çœ‹å‡ºæ¥å®ƒèåˆäº†ä»€ä¹ˆï¼Œå°±æ˜¯å°†CSPHetçš„anyBlockæ¨¡å—å †å çš„æ–¹å¼å’ŒELANèåˆåˆ°äº†ä¸€èµ·ã€‚**

* * *

### 2.2 Generalized ELANç»“æ„å›¾

YOLOv9æœ€ä¸»è¦çš„åˆ›æ–°ç›®å‰èƒ½å¤Ÿå¾—åˆ°çš„å°±æ˜¯å…¶ä¸­çš„**GELANç»“æ„ï¼Œæˆ‘ä¹Ÿæ˜¯åˆ†æå…¶ä»£ç æ ¹æ®è®ºæ–‡å°†å…¶ç»“æ„å›¾ç»˜ç”»å‡ºæ¥ã€‚**

ä¸‹é¢çš„æ–‡ä»¶ä¸ºYOLOv9çš„yamlæ–‡ä»¶ã€‚å¯ä»¥çœ‹åˆ°çš„æ˜¯å…¶æå‡ºäº†ä¸€ç§ç»“æ„åå­—RepNCSPELAN4ï¼Œå…¶ä¸­çš„ç»“æ„å›¾concatåçš„é€šé“æ•°æˆ‘æ²¡æœ‰ç”»æ˜¯å› ä¸ºå®ƒæœ‰è®¡ç®—ä¸­é—´çš„å‚æ•°çš„å˜é‡æ˜¯æ ¹æ®ä¸ªäººè®¾ç½®æ¥çš„ã€‚Â 

**å…¶ä»£ç å’Œç»“æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼**

```python
class RepNCSPELAN4(nn.Module):
    # csp-elan
    def __init__(self, c1, c2, c5=1):  # c5 = repeat
        super().__init__()
        c3 = int(c2 / 2)
        c4 = int(c3 / 2)
        self.c = c3 // 2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = nn.Sequential(RepNCSP(c3 // 2, c4, c5), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1)
 
    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))
 
    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))
```

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/47a3f90614554f0693deda34df848c40.png)

* * *

ä¸‰ã€æ ¸å¿ƒä»£ç Â 
-------

**æ ¸å¿ƒä»£ç çš„ä½¿ç”¨æ–¹å¼çœ‹ç« èŠ‚å››ï¼**

```python
import torch
import torch.nn as nn
import numpy as np
 
__all__ = ['RepNCSPELAN4_low', 'RepNCSPELAN4_high']
 
 
class RepConvN(nn.Module):
    """RepConv is a basic rep-style block, including training and deploy status
    This code is based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    """
    default_act = nn.SiLU()  # default activation
 
    def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):
        super().__init__()
        assert k == 3 and p == 1
        self.g = g
        self.c1 = c1
        self.c2 = c2
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
 
        self.bn = None
        self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)
        self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False)
 
    def forward_fuse(self, x):
        """Forward process"""
        return self.act(self.conv(x))
 
    def forward(self, x):
        """Forward process"""
        id_out = 0 if self.bn is None else self.bn(x)
        return self.act(self.conv1(x) + self.conv2(x) + id_out)
 
    def get_equivalent_kernel_bias(self):
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)
        kernelid, biasid = self._fuse_bn_tensor(self.bn)
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid
 
    def _avg_to_3x3_tensor(self, avgp):
        channels = self.c1
        groups = self.g
        kernel_size = avgp.kernel_size
        input_dim = channels // groups
        k = torch.zeros((channels, input_dim, kernel_size, kernel_size))
        k[np.arange(channels), np.tile(np.arange(input_dim), groups), :, :] = 1.0 / kernel_size ** 2
        return k
 
    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])
 
    def _fuse_bn_tensor(self, branch):
        if branch is None:
            return 0, 0
        if isinstance(branch, Conv):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        elif isinstance(branch, nn.BatchNorm2d):
            if not hasattr(self, 'id_tensor'):
                input_dim = self.c1 // self.g
                kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.c1):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std
 
    def fuse_convs(self):
        if hasattr(self, 'conv'):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.conv = nn.Conv2d(in_channels=self.conv1.conv.in_channels,
                              out_channels=self.conv1.conv.out_channels,
                              kernel_size=self.conv1.conv.kernel_size,
                              stride=self.conv1.conv.stride,
                              padding=self.conv1.conv.padding,
                              dilation=self.conv1.conv.dilation,
                              groups=self.conv1.conv.groups,
                              bias=True).requires_grad_(False)
        self.conv.weight.data = kernel
        self.conv.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__('conv1')
        self.__delattr__('conv2')
        if hasattr(self, 'nm'):
            self.__delattr__('nm')
        if hasattr(self, 'bn'):
            self.__delattr__('bn')
        if hasattr(self, 'id_tensor'):
            self.__delattr__('id_tensor')
 
 
class RepNBottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, kernels, groups, expand
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = RepConvN(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2
 
    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
 
 
class RepNCSP(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(RepNBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))
 
    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))
 
 
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
 
 
class Conv(nn.Module):
    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)
    default_act = nn.SiLU()  # default activation
 
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
 
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))
 
    def forward_fuse(self, x):
        return self.act(self.conv(x))
 
 
class RepNCSPELAN4_low(nn.Module):
    # csp-elan
    def __init__(self, c1, c2, c5=1):  # c5 = repeat
        super().__init__()
        c3 = int(c2 / 2)
        c4 = int(c3 / 2)
        self.c = c3 // 2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv3 = nn.Sequential(RepNCSP(c3, c3, c5))
        self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1)
 
    def forward(self, x):
        temp = self.cv1(x)
        temp3 = self.cv3(temp)
        y = list(temp.chunk(2, 1))
        y.append(temp3)
        temp2 = torch.cat(y, 1)
        return self.cv4(temp2)
 
    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))
 
 
class RepNCSPELAN4_high(nn.Module):
    # csp-elan
    def __init__(self, c1, c2, c5=1):  # c5 = repeat
        super().__init__()
        c3 = c2
        c4 = int(c3 / 2)
        self.c = c3 // 2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = nn.Sequential(RepNCSP(c3 // 2, c4, c5), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3 + (2 * c4), c2, 1, 1)
 
    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))
 
    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))
 
 
if __name__ == "__main__":
    # Generating Sample image
    image_size = (1, 24, 224, 224)
    image = torch.rand(*image_size)
 
    # Model
    mobilenet_v1 = RepNCSPELAN4_low(24, 24)
 
    out = mobilenet_v1(image)
    print(out.size())
```

å››ã€æ‰‹æŠŠæ‰‹æ•™ä½ æ·»åŠ GELANæœºåˆ¶Â 
-----------------

### 4.1 ä¿®æ”¹ä¸€

ç¬¬ä¸€è¿˜æ˜¯å»ºç«‹æ–‡ä»¶ï¼Œæˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹ultralytics/nn/modulesæ–‡ä»¶å¤¹ä¸‹å»ºç«‹ä¸€ä¸ªç›®å½•åå­—å‘¢å°±æ˜¯'Addmodules'æ–‡ä»¶å¤¹ï¼Œç„¶ååœ¨å…¶å†…éƒ¨å»ºç«‹ä¸€ä¸ªæ–°çš„pyæ–‡ä»¶å°†æ ¸å¿ƒä»£ç å¤åˆ¶ç²˜è´´è¿›å»å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d8757e8e481944c89dfb2da45af91a79.png)



* * *

### 4.2 ä¿®æ”¹äºŒÂ 

ç¬¬äºŒæ­¥æˆ‘ä»¬åœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–°çš„pyæ–‡ä»¶åå­—ä¸º'\_\_init\_\_.py'ï¼Œç„¶ååœ¨å…¶å†…éƒ¨å¯¼å…¥æˆ‘ä»¬çš„æ£€æµ‹å¤´å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/4a7ce9e6a2bc488b8ced324d0a4e1347.png)

* * *

### 4.3 ä¿®æ”¹ä¸‰Â 

ç¬¬ä¸‰æ­¥æˆ‘é—¨ä¸­åˆ°å¦‚ä¸‹æ–‡ä»¶'ultralytics/nn/tasks.py'è¿›è¡Œå¯¼å…¥å’Œæ³¨å†Œæˆ‘ä»¬çš„æ¨¡å—ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/67b28bda87e44d3285f0241acd165256.png)

* * *

### 4.4 ä¿®æ”¹å››Â 

æŒ‰ç…§æˆ‘çš„æ·»åŠ åœ¨parse\_modelé‡Œæ·»åŠ å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/fadfc24073624eaea6d52acc3425cc74.png)

**åˆ°æ­¤å°±ä¿®æ”¹å®Œæˆäº†ï¼Œå¤§å®¶å¯ä»¥å¤åˆ¶ä¸‹é¢çš„yamlæ–‡ä»¶è¿è¡Œã€‚**

* * *

äº”ã€GELANçš„yamlæ–‡ä»¶å’Œè¿è¡Œè®°å½•
-------------------

### 5.1 GELANä½å‚æ•°é‡ç‰ˆæœ¬çš„yamlæ–‡ä»¶

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales:  # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 1, RepNCSPELAN4_low, [128]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 1, RepNCSPELAN4_low, [256]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 1, RepNCSPELAN4_low, [512]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 1, RepNCSPELAN4_low, [1024]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 1, RepNCSPELAN4_low, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 1, RepNCSPELAN4_low, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 1, RepNCSPELAN4_low, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 1, RepNCSPELAN4_low, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 5.2Â  GELANé«˜å‚æ•°é‡ç‰ˆæœ¬çš„yamlæ–‡ä»¶

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales:  # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 1, RepNCSPELAN4_high, [128]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 1, RepNCSPELAN4_high, [256]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 1, RepNCSPELAN4_high, [512]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 1, RepNCSPELAN4_high, [1024]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 1, RepNCSPELAN4_high, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 1, RepNCSPELAN4_high, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 1, RepNCSPELAN4_high, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 1, RepNCSPELAN4_high, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 5.3 è®­ç»ƒä»£ç Â 

å¤§å®¶å¯ä»¥åˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶å°†æˆ‘ç»™çš„ä»£ç å¤åˆ¶ç²˜è´´è¿›å»ï¼Œé…ç½®å¥½è‡ªå·±çš„æ–‡ä»¶è·¯å¾„å³å¯è¿è¡Œã€‚

```python
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO
 
if __name__ == '__main__':
    model = YOLO('ultralytics/cfg/models/v8/yolov8-C2f-FasterBlock.yaml')
    # model.load('yolov8n.pt') # loading pretrain weights
    model.train(data=r'æ›¿æ¢æ•°æ®é›†yamlæ–‡ä»¶åœ°å€',
                # å¦‚æœå¤§å®¶ä»»åŠ¡æ˜¯å…¶å®ƒçš„'ultralytics/cfg/default.yaml'æ‰¾åˆ°è¿™é‡Œä¿®æ”¹taskå¯ä»¥æ”¹æˆdetect, segment, classify, pose
                cache=False,
                imgsz=640,
                epochs=150,
                single_cls=False,  # æ˜¯å¦æ˜¯å•ç±»åˆ«æ£€æµ‹
                batch=4,
                close_mosaic=10,
                workers=0,
                device='0',
                optimizer='SGD', # using SGD
                # resume='', # å¦‚è¿‡æƒ³ç»­è®­å°±è®¾ç½®last.ptçš„åœ°å€
                amp=False,  # å¦‚æœå‡ºç°è®­ç»ƒæŸå¤±ä¸ºNanå¯ä»¥å…³é—­amp
                project='runs/train',
                name='exp',
                )
```

* * *

### 5.3 GELANçš„è®­ç»ƒè¿‡ç¨‹æˆªå›¾Â 

#### 5.3.1 ä½å‚æ•°é‡ç‰ˆæœ¬

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b05ed798fd4e401c9a21f442d565be7a.png)

* * *

#### 5.3.2 é«˜å‚æ•°é‡ç‰ˆæœ¬

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/0099ad91b85f432abe25bec68f00d1d8.png)

