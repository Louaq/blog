# OREPA 

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

æœ¬æ–‡ç»™å¤§å®¶å¸¦æ¥çš„æ”¹è¿›æœºåˆ¶æ˜¯ä¸€ç§é‡å‚æ•°åŒ–çš„å·ç§¯æ¨¡å—**OREPA**ï¼Œè¿™ç§é‡å‚æ•°åŒ–æ¨¡å—éå¸¸é€‚åˆç”¨äºäºŒæ¬¡åˆ›æ–°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ›¿æ¢ç½‘ç»œä¸­çš„å…¶å®ƒå·ç§¯æ¨¡å—å¯ä»¥ä¸å½±å“æ¨ç†é€Ÿåº¦çš„åŒæ—¶è®©æ¨¡å‹å­¦ä¹ åˆ°æ›´å¤šçš„ç‰¹å¾ã€‚**OREPA**æ˜¯é€šè¿‡**åœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ï¼ˆOnline Convolutional Re-parameterizationï¼‰**æ¥å‡å°‘æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•ä¸»è¦åŒ…æ‹¬**ä¸¤ä¸ªé˜¶æ®µ**ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ä¸€ä¸ªç‰¹æ®Šçš„çº¿æ€§ç¼©æ”¾å±‚æ¥ä¼˜åŒ–åœ¨çº¿å—çš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å°†å¤æ‚çš„è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„å·ç§¯æ¥å‡å°‘è®­ç»ƒå¼€é”€ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1a1a6f9f899049f296a3ed14f1c8eacb.png)

äºŒã€OREPAåŸç†
---------

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/6b348fa0f29449d38dbbd218e526e060.png)

**è®ºæ–‡åœ°å€ï¼š[å®˜æ–¹è®ºæ–‡åœ°å€](https://arxiv.org/pdf/2204.00826.pdf "å®˜æ–¹è®ºæ–‡åœ°å€")**

**ä»£ç åœ°å€ï¼š[å®˜æ–¹ä»£ç åœ°å€](http://https//github.com/JUGGHM/OREPA_CVPR2022. "å®˜æ–¹ä»£ç åœ°å€")**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/71b305411a0e456999fc169ca370e710.png)

### 2.1Â Â OREPAçš„åŸºæœ¬åŸç†

**OREPA**æ˜¯é€šè¿‡**åœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ï¼ˆOnline Convolutional Re-parameterizationï¼‰**æ¥å‡å°‘æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚è¿™ç§æ–¹æ³•ä¸»è¦åŒ…æ‹¬**ä¸¤ä¸ªé˜¶æ®µ**ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ä¸€ä¸ªç‰¹æ®Šçš„çº¿æ€§ç¼©æ”¾å±‚æ¥ä¼˜åŒ–åœ¨çº¿å—çš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å°†å¤æ‚çš„è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„å·ç§¯æ¥å‡å°‘è®­ç»ƒå¼€é”€ã€‚OREPAèƒ½å¤Ÿæ˜¾è‘—é™ä½è®­ç»ƒæ—¶çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶åœ¨æ ‡å‡†çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¦‚å›¾åƒåˆ†ç±»ã€ç‰©ä½“æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

**OREPAï¼ˆåœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ï¼‰çš„åŸºæœ¬åŸç†å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š**

**1\. ä¸¤é˜¶æ®µæµç¨‹**ï¼šOREPAé‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µæ¥å®ç°å…¶ç›®æ ‡ã€‚é¦–å…ˆæ˜¯åœ¨çº¿ä¼˜åŒ–é˜¶æ®µï¼Œå…¶æ¬¡æ˜¯å‹ç¼©è®­ç»ƒæ—¶æ¨¡å—é˜¶æ®µã€‚

**2\. çº¿æ€§ç¼©æ”¾å±‚**ï¼šå¼•å…¥çº¿æ€§ç¼©æ”¾å±‚æ¥ä¼˜åŒ–åœ¨çº¿æ¨¡å—ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œè®­ç»ƒã€‚

**3\. è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©**ï¼šå°†å¤æ‚çš„è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©æˆä¸€ä¸ªå•ä¸€çš„å·ç§¯æ“ä½œï¼Œä»¥å‡å°‘è®­ç»ƒå¼€é”€ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/55abba5b333a44db9baa2a1abc161093.png)

è¿™å¼ å›¾å±•ç¤ºäº†**åœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ï¼ˆOREPAï¼‰æµç¨‹çš„å…·ä½“æ­¥éª¤ï¼š**

**1\. ç§»é™¤ï¼ˆRemoveï¼‰**ï¼šé¦–å…ˆç§»é™¤åŸå‹å—ä¸­çš„éçº¿æ€§å±‚ï¼Œå¦‚ReLUå’Œæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰ã€‚

**2\. æ·»åŠ ç¼©æ”¾ï¼ˆAdd Scalingï¼‰**ï¼šå¼•å…¥ç¼©æ”¾å±‚æ¥ä¼˜åŒ–å·ç§¯å±‚çš„è¾“å‡ºã€‚

**3\. æ·»åŠ å½’ä¸€åŒ–ï¼ˆAdd Normï¼‰**ï¼šæœ€åï¼Œæ·»åŠ å½’ä¸€åŒ–æ¥è¿›è¡Œæ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–å’Œæå‡ã€‚

é€šè¿‡è¿™ä¸€ç³»åˆ—æ­¥éª¤ï¼ŒOREPAæ–¹æ³•åœ¨è®­ç»ƒé˜¶æ®µæœ‰æ•ˆåœ°ç®€åŒ–äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œä¼˜åŒ–äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸ºé«˜æ€§èƒ½æ¨¡å‹çš„æ„å»ºæä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•è®ºã€‚

* * *

### 2.2Â **ä¸¤é˜¶æ®µæµç¨‹**

ä¸‹é¢ä¸ºå¤§å®¶å±•ç¤ºäº†**åœ¨çº¿é‡å‚æ•°åŒ–ï¼ˆOREPAï¼‰çš„ä¸¤ä¸ªé˜¶æ®µï¼š**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/b11e070188eb43ebaffdf53f7012159b.png)

**1\. å—çº¿æ€§åŒ–ï¼ˆBlock Linearizationï¼‰**ï¼šåœ¨è¿™ä¸ªé˜¶æ®µï¼ŒåŸå‹é‡å‚æ•°åŒ–å—ä¸­çš„æ‰€æœ‰éçº¿æ€§ç»„ä»¶è¢«ç§»é™¤ï¼Œåªç•™ä¸‹å·ç§¯å’Œæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰å±‚ï¼Œå¹¶åŠ å…¥ç¼©æ”¾å±‚ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

**2\. å—æŒ¤å‹ï¼ˆBlock Squeezingï¼‰**ï¼šéšåï¼Œè¿™ä¸ªè¿‡ç¨‹å°†ä¸Šè¿°çº¿æ€§åŒ–çš„å—åˆå¹¶æˆä¸€ä¸ªå•ç‹¬çš„å·ç§¯å±‚ï¼ˆOREPA Convï¼‰ï¼Œè¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚

* * *

### 2.3Â **çº¿æ€§ç¼©æ”¾å±‚**

**çº¿æ€§ç¼©æ”¾å±‚**åœ¨OREPAä¸­æ˜¯ä¸€ä¸ªå…³é”®çš„ç»„æˆéƒ¨åˆ†ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæ›´æœ‰æ•ˆçš„æƒé‡æ›´æ–°ã€‚è¿™ä¸€å±‚çš„ç›®çš„æ˜¯åœ¨**ä¿æŒè®­ç»ƒæ—¶å¤æ‚æ€§ç®¡ç†**çš„åŒæ—¶ï¼Œæé«˜æ¨¡å‹åœ¨å­¦ä¹ ç‰¹å¾æ—¶çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡å¯¹æƒé‡è¿›è¡Œåˆé€‚çš„ç¼©æ”¾ï¼Œè¿™ä¸€å±‚æœ‰åŠ©äºæ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŒç¨³å®šçš„æ¢¯åº¦æµåŠ¨ï¼Œè¿™å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–è‡³å…³é‡è¦ã€‚è¿™ç§çº¿æ€§ç¼©æ”¾å±‚é€šè¿‡åœ¨è®­ç»ƒæ—¶å¯¹å·ç§¯å±‚çš„è¾“å‡ºè¿›è¡Œç¼©æ”¾ï¼Œä½¿æ¨¡å‹å¯ä»¥æ›´å¿«åœ°æ”¶æ•›ï¼ŒåŒæ—¶å‡å°‘èµ„æºçš„æ¶ˆè€—ã€‚

ä¸‹é¢è¿™å¼ å›¾å±•ç¤ºäº†**åœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ï¼ˆOREPAï¼‰æ¡†æ¶ä¸­æå‡ºçš„å››ä¸ªç»„ä»¶ï¼š**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/e3b8e6ccfcef417bafb623cdfe9dc3c5.png)

Â **(a) é¢‘ç‡å…ˆéªŒè¿‡æ»¤å™¨**ï¼šä¸€ä¸ªç»“åˆäº†1x1å·ç§¯å’Œ3x3é¢‘ç‡å…ˆéªŒæ»¤æ³¢å™¨çš„ç»“æ„ï¼Œéšåæ˜¯ä¸€ä¸ªç¼©æ”¾å±‚ã€‚  
Â **(b) çº¿æ€§æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆLinear DW sepconvï¼‰**ï¼šåŒ…å«äº†3x3æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆDW Convï¼‰ï¼Œåé¢è·Ÿç€1x1é€ç‚¹å·ç§¯ï¼ˆPW Convï¼‰å’Œç¼©æ”¾å±‚ã€‚  
Â **(c) é‡å‚æ•°1x1å·ç§¯**ï¼šç”±ä¸¤ä¸ª1x1å·ç§¯å±‚ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªå±‚åéƒ½æ¥æœ‰ä¸€ä¸ªç¼©æ”¾å±‚ï¼Œè¿™äº›å±‚è¢«ç”¨æ¥é‡å‚æ•°åŒ–æ›´å¤§çš„å·ç§¯ç»“æ„ã€‚

Â **(d) çº¿æ€§æ·±å±‚å¹²ç»†èƒï¼ˆLinear deep stemï¼‰**ï¼šç”±ä¸‰ä¸ª3x3å·ç§¯å±‚ç»„æˆï¼Œè¿™ç§ç»“æ„æ—¨åœ¨å¤„ç†è¾“å…¥æ•°æ®çš„æœ€åˆå‡ å±‚ï¼Œä»¥æé«˜æ¨¡å‹çš„åˆæœŸç‰¹å¾æå–èƒ½åŠ›ã€‚

* * *

### 2.4Â **è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©**

åœ¨OREPAæ¡†æ¶ä¸­ï¼Œ**è®­ç»ƒæ—¶æ¨¡å—å‹ç¼©**æŒ‡çš„æ˜¯åœ¨è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡**çº¿æ€§åŒ–å¤„ç†å’Œç»“æ„ç®€åŒ–**å°†åŸæœ¬å¤æ‚çš„å·ç§¯ç½‘ç»œå—è½¬æ¢ä¸ºä¸€ä¸ªç®€å•çš„å·ç§¯å±‚ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†æ¨¡å‹è®­ç»ƒæ—¶çš„å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ¶‰åŠåˆ°ç§»é™¤éçº¿æ€§æ¿€æ´»å‡½æ•°ã€åˆå¹¶å¤šä¸ªå·ç§¯å±‚ï¼Œå¹¶å¼•å…¥ç¼©æ”¾å±‚æ¥è°ƒæ•´å·ç§¯å±‚æƒé‡çš„è§„æ¨¡ã€‚

Â è¿™å¼ å›¾å±•ç¤ºäº†**ä¸åŒå·ç§¯å±‚ç»“æ„åœ¨è®­ç»ƒé˜¶æ®µå’Œæ¨ç†é˜¶æ®µçš„å¯¹æ¯”**ï¼š

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/befc20de61f24f7f9307b727423daf0a.png)

Â **(a)** æ˜¯ä¸€ä¸ªæ ‡å‡†çš„å·ç§¯å±‚ï¼Œæ²¡æœ‰é‡å‚æ•°åŒ–ï¼ˆNo Re-paramï¼‰ã€‚  
Â **(b)** å±•ç¤ºäº†ä¸€ä¸ªå…¸å‹çš„ç»“æ„é‡å‚æ•°åŒ–å—ï¼ˆStructural Re-paramï¼‰ã€‚  
Â **(c)** æ˜¯è®ºæ–‡ä¸­æå‡ºçš„åœ¨çº¿é‡å‚æ•°åŒ–å—ï¼ˆOREPAï¼‰ï¼Œå³åœ¨çº¿å·ç§¯é‡å‚æ•°åŒ–ã€‚  
Â **(d)** æ˜¯éƒ¨ç½²æ—¶çš„ç»“æ„ï¼Œæ‰€æœ‰è®­ç»ƒé˜¶æ®µçš„ç»“æ„åœ¨æ¨ç†æ—¶éƒ½è½¬æ¢ä¸ºè¿™ä¸ªç®€å•çš„å·ç§¯ç»“æ„ã€‚

> åœ¨è®­ç»ƒé˜¶æ®µï¼ŒOREPAçš„è®¾è®¡å…è®¸å°†å¤šä¸ªå·ç§¯å±‚å’Œæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰å±‚é€šè¿‡é‡å‚æ•°åŒ–åˆå¹¶ä¸ºä¸€ä¸ªå·ç§¯å±‚ï¼Œä»¥å‡å°‘è®­ç»ƒæ—¶çš„å¤æ‚æ€§å’Œå†…å­˜éœ€æ±‚ã€‚åœ¨æ¨ç†æ—¶ï¼Œä¸è®ºè®­ç»ƒæ—¶çš„ç»“æ„å¦‚ä½•å¤æ‚ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½è¢«ç®€åŒ–ä¸ºå•ä¸€çš„å·ç§¯å±‚ï¼Œè¿™æœ‰åŠ©äºæé«˜æ¨ç†é€Ÿåº¦å¹¶é™ä½æ¨ç†æ—¶çš„èµ„æºæ¶ˆè€—ã€‚

ä¸‹å›¾ä¸ºå¤§å®¶å±•ç¤ºçš„æ˜¯**OREPAå—çš„è®¾è®¡**ï¼Œå®ƒåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¯¹åº”äºä¸€ä¸ª**3x3çš„å·ç§¯**ã€‚è¿™ä¸ªè®¾è®¡é€šè¿‡å°†ä¸€ç³»åˆ—å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œé¢‘ç‡å…ˆéªŒå±‚ï¼Œå¹¶å¸¦æœ‰ç¼©æ”¾å±‚ï¼Œæœ€ç»ˆé€šè¿‡ä¸€ä¸ªæŒ¤å‹æ“ä½œå°†è¿™äº›å±‚åˆå¹¶æˆä¸€ä¸ªå•ç‹¬çš„OREPAå·ç§¯å±‚ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d1fc248718954403b05a8041d139cd43.png)

ä¸‰ã€OREPAçš„æ ¸å¿ƒä»£ç 
------------

**ä»£ç çš„ä½¿ç”¨æ–¹å¼çœ‹ç« èŠ‚å››ï¼**

```python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.nn import init
 
__all__ = ['OREPA', 'C2f_OREPA']
 
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
 
 
class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""
    default_act = nn.SiLU()  # default activation
 
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
 
    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))
 
    def forward_fuse(self, x):
        """Perform transposed convolution of 2D data."""
        return self.act(self.conv(x))
 
def transI_fusebn(kernel, bn):
    gamma = bn.weight
    std = (bn.running_var + bn.eps).sqrt()
    return kernel * ((gamma / std).reshape(-1, 1, 1, 1)), bn.bias - bn.running_mean * gamma / std
 
 
def transVI_multiscale(kernel, target_kernel_size):
    H_pixels_to_pad = (target_kernel_size - kernel.size(2)) // 2
    W_pixels_to_pad = (target_kernel_size - kernel.size(3)) // 2
    return F.pad(kernel, [W_pixels_to_pad, W_pixels_to_pad, H_pixels_to_pad, H_pixels_to_pad])
 
 
class OREPA(nn.Module):
    def __init__(self,
                 in_channels,
                 kernel_size=3,
                 stride=1,
                 padding=None,
                 groups=1,
                 dilation=1,
                 act=True,
                 internal_channels_1x1_3x3=None,
                 deploy=False,
                 single_init=False,
                 weight_only=False,
                 init_hyper_para=1.0, init_hyper_gamma=1.0):
        super(OREPA, self).__init__()
        self.deploy = deploy
        out_channels = in_channels
        self.nonlinear = Conv.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
        self.weight_only = weight_only
 
        self.kernel_size = kernel_size
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.groups = groups
 
        self.stride = stride
        padding = autopad(kernel_size, padding, dilation)
        self.padding = padding
        self.dilation = dilation
 
        if deploy:
            self.orepa_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,
                                           stride=stride,
                                           padding=padding, dilation=dilation, groups=groups, bias=True)
 
        else:
 
            self.branch_counter = 0
 
            self.weight_orepa_origin = nn.Parameter(
                torch.Tensor(out_channels, int(in_channels / self.groups), kernel_size, kernel_size))
            init.kaiming_uniform_(self.weight_orepa_origin, a=math.sqrt(0.0))
            self.branch_counter += 1
 
            self.weight_orepa_avg_conv = nn.Parameter(
                torch.Tensor(out_channels, int(in_channels / self.groups), 1,
                             1))
            self.weight_orepa_pfir_conv = nn.Parameter(
                torch.Tensor(out_channels, int(in_channels / self.groups), 1,
                             1))
            init.kaiming_uniform_(self.weight_orepa_avg_conv, a=0.0)
            init.kaiming_uniform_(self.weight_orepa_pfir_conv, a=0.0)
            self.register_buffer(
                'weight_orepa_avg_avg',
                torch.ones(kernel_size,
                           kernel_size).mul(1.0 / kernel_size / kernel_size))
            self.branch_counter += 1
            self.branch_counter += 1
 
            self.weight_orepa_1x1 = nn.Parameter(
                torch.Tensor(out_channels, int(in_channels / self.groups), 1,
                             1))
            init.kaiming_uniform_(self.weight_orepa_1x1, a=0.0)
            self.branch_counter += 1
 
            if internal_channels_1x1_3x3 is None:
                internal_channels_1x1_3x3 = in_channels if groups <= 4 else 2 * in_channels
 
            if internal_channels_1x1_3x3 == in_channels:
                self.weight_orepa_1x1_kxk_idconv1 = nn.Parameter(
                    torch.zeros(in_channels, int(in_channels / self.groups), 1, 1))
                id_value = np.zeros(
                    (in_channels, int(in_channels / self.groups), 1, 1))
                for i in range(in_channels):
                    id_value[i, i % int(in_channels / self.groups), 0, 0] = 1
                id_tensor = torch.from_numpy(id_value).type_as(
                    self.weight_orepa_1x1_kxk_idconv1)
                self.register_buffer('id_tensor', id_tensor)
 
            else:
                self.weight_orepa_1x1_kxk_idconv1 = nn.Parameter(
                    torch.zeros(internal_channels_1x1_3x3,
                                int(in_channels / self.groups), 1, 1))
                id_value = np.zeros(
                    (internal_channels_1x1_3x3, int(in_channels / self.groups), 1, 1))
                for i in range(internal_channels_1x1_3x3):
                    id_value[i, i % int(in_channels / self.groups), 0, 0] = 1
                id_tensor = torch.from_numpy(id_value).type_as(
                    self.weight_orepa_1x1_kxk_idconv1)
                self.register_buffer('id_tensor', id_tensor)
                # init.kaiming_uniform_(
                # self.weight_orepa_1x1_kxk_conv1, a=math.sqrt(0.0))
            self.weight_orepa_1x1_kxk_conv2 = nn.Parameter(
                torch.Tensor(out_channels,
                             int(internal_channels_1x1_3x3 / self.groups),
                             kernel_size, kernel_size))
            init.kaiming_uniform_(self.weight_orepa_1x1_kxk_conv2, a=math.sqrt(0.0))
            self.branch_counter += 1
 
            expand_ratio = 8
            self.weight_orepa_gconv_dw = nn.Parameter(
                torch.Tensor(in_channels * expand_ratio, 1, kernel_size,
                             kernel_size))
            self.weight_orepa_gconv_pw = nn.Parameter(
                torch.Tensor(out_channels, int(in_channels * expand_ratio / self.groups), 1, 1))
            init.kaiming_uniform_(self.weight_orepa_gconv_dw, a=math.sqrt(0.0))
            init.kaiming_uniform_(self.weight_orepa_gconv_pw, a=math.sqrt(0.0))
            self.branch_counter += 1
 
            self.vector = nn.Parameter(torch.Tensor(self.branch_counter, self.out_channels))
            if weight_only is False:
                self.bn = nn.BatchNorm2d(self.out_channels)
 
            self.fre_init()
 
            init.constant_(self.vector[0, :], 0.25 * math.sqrt(init_hyper_gamma))  # origin
            init.constant_(self.vector[1, :], 0.25 * math.sqrt(init_hyper_gamma))  # avg
            init.constant_(self.vector[2, :], 0.0 * math.sqrt(init_hyper_gamma))  # prior
            init.constant_(self.vector[3, :], 0.5 * math.sqrt(init_hyper_gamma))  # 1x1_kxk
            init.constant_(self.vector[4, :], 1.0 * math.sqrt(init_hyper_gamma))  # 1x1
            init.constant_(self.vector[5, :], 0.5 * math.sqrt(init_hyper_gamma))  # dws_conv
 
            self.weight_orepa_1x1.data = self.weight_orepa_1x1.mul(init_hyper_para)
            self.weight_orepa_origin.data = self.weight_orepa_origin.mul(init_hyper_para)
            self.weight_orepa_1x1_kxk_conv2.data = self.weight_orepa_1x1_kxk_conv2.mul(init_hyper_para)
            self.weight_orepa_avg_conv.data = self.weight_orepa_avg_conv.mul(init_hyper_para)
            self.weight_orepa_pfir_conv.data = self.weight_orepa_pfir_conv.mul(init_hyper_para)
 
            self.weight_orepa_gconv_dw.data = self.weight_orepa_gconv_dw.mul(math.sqrt(init_hyper_para))
            self.weight_orepa_gconv_pw.data = self.weight_orepa_gconv_pw.mul(math.sqrt(init_hyper_para))
 
            if single_init:
                #   Initialize the vector.weight of origin as 1 and others as 0. This is not the default setting.
                self.single_init()
 
    def fre_init(self):
        prior_tensor = torch.Tensor(self.out_channels, self.kernel_size,
                                    self.kernel_size)
        half_fg = self.out_channels / 2
        for i in range(self.out_channels):
            for h in range(3):
                for w in range(3):
                    if i < half_fg:
                        prior_tensor[i, h, w] = math.cos(math.pi * (h + 0.5) *
                                                         (i + 1) / 3)
                    else:
                        prior_tensor[i, h, w] = math.cos(math.pi * (w + 0.5) *
                                                         (i + 1 - half_fg) / 3)
 
        self.register_buffer('weight_orepa_prior', prior_tensor)
 
    def weight_gen(self):
        weight_orepa_origin = torch.einsum('oihw,o->oihw',
                                           self.weight_orepa_origin,
                                           self.vector[0, :])
 
        weight_orepa_avg = torch.einsum('oihw,hw->oihw', self.weight_orepa_avg_conv, self.weight_orepa_avg_avg)
        weight_orepa_avg = torch.einsum(
            'oihw,o->oihw',
            torch.einsum('oi,hw->oihw', self.weight_orepa_avg_conv.squeeze(3).squeeze(2),
                         self.weight_orepa_avg_avg), self.vector[1, :])
 
        weight_orepa_pfir = torch.einsum(
            'oihw,o->oihw',
            torch.einsum('oi,ohw->oihw', self.weight_orepa_pfir_conv.squeeze(3).squeeze(2),
                         self.weight_orepa_prior), self.vector[2, :])
 
        weight_orepa_1x1_kxk_conv1 = None
        if hasattr(self, 'weight_orepa_1x1_kxk_idconv1'):
            weight_orepa_1x1_kxk_conv1 = (self.weight_orepa_1x1_kxk_idconv1 +
                                          self.id_tensor).squeeze(3).squeeze(2)
        elif hasattr(self, 'weight_orepa_1x1_kxk_conv1'):
            weight_orepa_1x1_kxk_conv1 = self.weight_orepa_1x1_kxk_conv1.squeeze(3).squeeze(2)
        else:
            raise NotImplementedError
        weight_orepa_1x1_kxk_conv2 = self.weight_orepa_1x1_kxk_conv2
 
        if self.groups > 1:
            g = self.groups
            t, ig = weight_orepa_1x1_kxk_conv1.size()
            o, tg, h, w = weight_orepa_1x1_kxk_conv2.size()
            weight_orepa_1x1_kxk_conv1 = weight_orepa_1x1_kxk_conv1.view(
                g, int(t / g), ig)
            weight_orepa_1x1_kxk_conv2 = weight_orepa_1x1_kxk_conv2.view(
                g, int(o / g), tg, h, w)
            weight_orepa_1x1_kxk = torch.einsum('gti,gothw->goihw',
                                                weight_orepa_1x1_kxk_conv1,
                                                weight_orepa_1x1_kxk_conv2).reshape(
                o, ig, h, w)
        else:
            weight_orepa_1x1_kxk = torch.einsum('ti,othw->oihw',
                                                weight_orepa_1x1_kxk_conv1,
                                                weight_orepa_1x1_kxk_conv2)
        weight_orepa_1x1_kxk = torch.einsum('oihw,o->oihw', weight_orepa_1x1_kxk, self.vector[3, :])
 
        weight_orepa_1x1 = 0
        if hasattr(self, 'weight_orepa_1x1'):
            weight_orepa_1x1 = transVI_multiscale(self.weight_orepa_1x1,
                                                  self.kernel_size)
            weight_orepa_1x1 = torch.einsum('oihw,o->oihw', weight_orepa_1x1,
                                            self.vector[4, :])
 
        weight_orepa_gconv = self.dwsc2full(self.weight_orepa_gconv_dw,
                                            self.weight_orepa_gconv_pw,
                                            self.in_channels, self.groups)
        weight_orepa_gconv = torch.einsum('oihw,o->oihw', weight_orepa_gconv,
                                          self.vector[5, :])
 
        weight = weight_orepa_origin + weight_orepa_avg + weight_orepa_1x1 + weight_orepa_1x1_kxk + weight_orepa_pfir + weight_orepa_gconv
 
        return weight
 
    def dwsc2full(self, weight_dw, weight_pw, groups, groups_conv=1):
 
        t, ig, h, w = weight_dw.size()
        o, _, _, _ = weight_pw.size()
        tg = int(t / groups)
        i = int(ig * groups)
        ogc = int(o / groups_conv)
        groups_gc = int(groups / groups_conv)
        weight_dw = weight_dw.view(groups_conv, groups_gc, tg, ig, h, w)
        weight_pw = weight_pw.squeeze().view(ogc, groups_conv, groups_gc, tg)
 
        weight_dsc = torch.einsum('cgtihw,ocgt->cogihw', weight_dw, weight_pw)
        return weight_dsc.reshape(o, int(i / groups_conv), h, w)
 
    def forward(self, inputs=None):
        if hasattr(self, 'orepa_reparam'):
            return self.nonlinear(self.orepa_reparam(inputs))
 
        weight = self.weight_gen()
 
        if self.weight_only is True:
            return weight
 
        out = F.conv2d(
            inputs,
            weight,
            bias=None,
            stride=self.stride,
            padding=self.padding,
            dilation=self.dilation,
            groups=self.groups)
        return self.nonlinear(self.bn(out))
 
    def get_equivalent_kernel_bias(self):
        return transI_fusebn(self.weight_gen(), self.bn)
 
    def switch_to_deploy(self):
        if hasattr(self, 'or1x1_reparam'):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.orepa_reparam = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels,
                                       kernel_size=self.kernel_size, stride=self.stride,
                                       padding=self.padding, dilation=self.dilation, groups=self.groups, bias=True)
        self.orepa_reparam.weight.data = kernel
        self.orepa_reparam.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__('weight_orepa_origin')
        self.__delattr__('weight_orepa_1x1')
        self.__delattr__('weight_orepa_1x1_kxk_conv2')
        if hasattr(self, 'weight_orepa_1x1_kxk_idconv1'):
            self.__delattr__('id_tensor')
            self.__delattr__('weight_orepa_1x1_kxk_idconv1')
        elif hasattr(self, 'weight_orepa_1x1_kxk_conv1'):
            self.__delattr__('weight_orepa_1x1_kxk_conv1')
        else:
            raise NotImplementedError
        self.__delattr__('weight_orepa_avg_avg')
        self.__delattr__('weight_orepa_avg_conv')
        self.__delattr__('weight_orepa_pfir_conv')
        self.__delattr__('weight_orepa_prior')
        self.__delattr__('weight_orepa_gconv_dw')
        self.__delattr__('weight_orepa_gconv_pw')
 
        self.__delattr__('bn')
        self.__delattr__('vector')
 
    def init_gamma(self, gamma_value):
        init.constant_(self.vector, gamma_value)
 
    def single_init(self):
        self.init_gamma(0.0)
        init.constant_(self.vector[0, :], 1.0)
 
 
 
class Bottleneck(nn.Module):
    # Standard bottleneck with DCN
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
 
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = OREPA(c_, k[1], 1, groups=g)
        self.add = shortcut and c1 == c2
 
    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
 
 
class C2f_OREPA(nn.Module):
    # CSP Bottleneck with 2 convolutions
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))
 
    def forward(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
 
 
```

* * *

å››ã€æ‰‹æŠŠæ‰‹æ•™ä½ æ·»åŠ OREPAæ¨¡å—
----------------

### 4.1 ä¿®æ”¹ä¸€

ç¬¬ä¸€è¿˜æ˜¯å»ºç«‹æ–‡ä»¶ï¼Œæˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹ultralytics/nn/modulesæ–‡ä»¶å¤¹ä¸‹å»ºç«‹ä¸€ä¸ªç›®å½•åå­—å‘¢å°±æ˜¯'Addmodules'æ–‡ä»¶å¤¹ï¼Œç„¶ååœ¨å…¶å†…éƒ¨å»ºç«‹ä¸€ä¸ªæ–°çš„pyæ–‡ä»¶å°†æ ¸å¿ƒä»£ç å¤åˆ¶ç²˜è´´è¿›å»å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/58a4c688b17c4e429ac35c99e42e4fb8.png)

* * *

### 4.2 ä¿®æ”¹äºŒÂ 

ç¬¬äºŒæ­¥æˆ‘ä»¬åœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–°çš„pyæ–‡ä»¶åå­—ä¸º'\_\_init\_\_.py'ï¼Œç„¶ååœ¨å…¶å†…éƒ¨å¯¼å…¥æˆ‘ä»¬çš„æ£€æµ‹å¤´å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/196c97d41ad543b4a414587b24f23d3b.png)

* * *

### 4.3 ä¿®æ”¹ä¸‰Â 

ç¬¬ä¸‰æ­¥æˆ‘é—¨ä¸­åˆ°å¦‚ä¸‹æ–‡ä»¶'ultralytics/nn/tasks.py'è¿›è¡Œå¯¼å…¥å’Œæ³¨å†Œæˆ‘ä»¬çš„æ¨¡å—ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/67b28bda87e44d3285f0241acd165256.png)

* * *

### 4.4 ä¿®æ”¹å››Â 

æŒ‰ç…§æˆ‘çš„æ·»åŠ åœ¨parse\_modelé‡Œæ·»åŠ å³å¯ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/42098d5101794e01ad1776b146ea7885.png)

**åˆ°æ­¤å°±ä¿®æ”¹å®Œæˆäº†ï¼Œå¤§å®¶å¯ä»¥å¤åˆ¶ä¸‹é¢çš„yamlæ–‡ä»¶è¿è¡Œã€‚**

* * *

äº”ã€OREPAçš„yamlæ–‡ä»¶å’Œè¿è¡Œè®°å½•
-------------------

### 5.1 OREPAçš„yamlæ–‡ä»¶ä¸€

**ä¸‹é¢çš„æ·»åŠ **OREPA**æ˜¯æˆ‘å®éªŒç»“æœçš„ç‰ˆæœ¬ã€‚**

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f_OREPA, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f_OREPA, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f_OREPA, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f_OREPA, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 5.2 OREPAçš„yamlæ–‡ä»¶äºŒ

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, OREPA, [3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, OREPA, [3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, OREPA, [3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, OREPA, [3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 5.3 OREPAçš„è®­ç»ƒè¿‡ç¨‹æˆªå›¾Â 

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/bc6f35a1c3ea4f03af80b2a3a2e2748f.png)

