# åŠ¨æ€è›‡å½¢å·ç§¯ï¼ˆ:100:ï¼‰

ä¸€ã€æœ¬æ–‡ä»‹ç»
------

åŠ¨æ€è›‡å½¢å·ç§¯çš„çµæ„Ÿæ¥æºäºå¯¹ç®¡çŠ¶ç»“æ„çš„ç‰¹æ®Šæ€§çš„è§‚å¯Ÿå’Œç†è§£ï¼Œåœ¨åˆ†å‰²æ‹“æ‰‘ç®¡çŠ¶ç»“æ„ã€è¡€ç®¡å’Œé“è·¯ç­‰ç±»å‹çš„ç®¡çŠ¶ç»“æ„æ—¶ï¼Œä»»åŠ¡çš„å¤æ‚æ€§å¢åŠ ï¼Œå› ä¸ºè¿™äº›ç»“æ„çš„å±€éƒ¨ç»“æ„å¯èƒ½éå¸¸ç»†é•¿å’Œè¿‚å›ï¼Œè€Œæ•´ä½“å½¢æ€ä¹Ÿå¯èƒ½å¤šå˜ã€‚  
å› æ­¤ä¸ºäº†åº”å¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œä½œè€…ç ”ç©¶å›¢é˜Ÿæ³¨æ„åˆ°äº†**ç®¡çŠ¶ç»“æ„çš„ç‰¹æ®Šæ€§**ï¼Œå¹¶æå‡ºäº†åŠ¨æ€è›‡å½¢å·ç§¯ï¼ˆDynamic Snake Convolutionï¼‰è¿™ä¸ªæ–¹æ³•ã€‚åŠ¨æ€è›‡å½¢å·ç§¯é€šè¿‡è‡ªé€‚åº”åœ°èšç„¦äºç»†é•¿å’Œè¿‚å›çš„å±€éƒ¨ç»“æ„ï¼Œå‡†ç¡®åœ°æ•æ‰ç®¡çŠ¶ç»“æ„çš„ç‰¹å¾ã€‚è¿™ç§å·ç§¯æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œ**é€šè¿‡åŠ¨æ€å½¢çŠ¶çš„å·ç§¯æ ¸æ¥å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œé’ˆå¯¹ç®¡çŠ¶ç»“æ„çš„ç‰¹å¾æå–è¿›è¡Œä¼˜åŒ–ã€‚**

æ€»ä¹‹åŠ¨æ€è›‡å½¢å·ç§¯æ˜¯ä¸€ç§é’ˆå¯¹ç®¡çŠ¶ç»“æ„åˆ†å‰²ä»»åŠ¡çš„åˆ›æ–°æ–¹æ³•ï¼Œ**åœ¨è®¸å¤šæ¨¡å‹ä¸Šæ·»åŠ é’ˆå¯¹ä¸€äº›æ•°æ®é›†éƒ½èƒ½å¤Ÿæœ‰æ•ˆçš„æ¶¨ç‚¹**ï¼Œå…¶å…·æœ‰é‡è¦æ€§å’Œå¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚

> åŠ¨æ€è›‡å½¢å·ç§¯(Dynamic Snake Convolution)é€‚ç”¨äºå¤šç§æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤šç§æ¨¡å‹ä¸Šæ·»åŠ æˆ–æ›¿æ¢è¯¥å·ç§¯ï¼Œæœ¬æ–‡ä¸»è¦é’ˆå¯¹çš„æ”¹è¿›æ¨¡å‹æ˜¯YOLOv8æ¨¡å‹ï¼Œå¹¶ä¿®å¤åŠ¨æ€è›‡å½¢å·ç§¯å®˜æ–¹ä»£ç ä¸­å­˜åœ¨çš„BUGä¾‹å¦‚ï¼š Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!ä¿®å¤ï¼ŒåŒæ—¶ä»¥æ­¤æ¥è¿›è¡Œç¤ºä¾‹å¸®åŠ©å¤§å®¶ç†è§£å’ŒæŒæ¡åŠ¨æ€è›‡å½¢å·ç§¯å’ŒYOLOv8æ¨¡å‹ã€‚
>

äºŒã€åŠ¨æ€è›‡å½¢å·ç§¯èƒŒæ™¯å’ŒåŸç†
-------------

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/178b091169894ca6b784ba30558103f0.png)

**è®ºæ–‡ä»£ç åœ°å€ï¼š[åŠ¨æ€è›‡å½¢å·ç§¯å®˜æ–¹ä»£ç ä¸‹è½½åœ°å€](https://github.com/YaoleiQi/DSCNet "åŠ¨æ€è›‡å½¢å·ç§¯å®˜æ–¹ä»£ç ä¸‹è½½åœ°å€")  
è®ºæ–‡åœ°å€ï¼š[åŠ¨æ€è›‡å½¢å·ç§¯(DynamicSnakeConvolution)](https://download.csdn.net/download/java1314777/88489535 "åŠ¨æ€è›‡å½¢å·ç§¯(DynamicSnakeConvolution)")**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a03e5908d12a4c55a45509b6a5cf538a.png)

**èƒŒæ™¯**ï¼šåŠ¨æ€è›‡å½¢å·ç§¯(Dynamic Snake Convolution)æ¥æºäºä¸´åºŠåŒ»å­¦ï¼Œæ¸…æ™°å‹¾ç”»è¡€ç®¡æ˜¯è®¡ç®—æµä½“åŠ›å­¦ç ”ç©¶çš„å…³é”®å‰æï¼Œå¹¶èƒ½ååŠ©æ”¾å°„ç§‘åŒ»å¸ˆè¿›è¡Œè¯Šæ–­å’Œå®šä½ç—…å˜ã€‚åœ¨é¥æ„Ÿåº”ç”¨ä¸­ï¼Œå®Œæ•´çš„é“è·¯åˆ†å‰²ä¸ºè·¯å¾„è§„åˆ’æä¾›äº†åšå®çš„åŸºç¡€ã€‚æ— è®ºæ˜¯å“ªä¸ªé¢†åŸŸï¼Œè¿™äº›ç»“æ„éƒ½å…·æœ‰ç»†é•¿å’Œæ›²æŠ˜çš„å…±åŒç‰¹å¾ï¼Œä½¿å¾—å®ƒä»¬å¾ˆéš¾åœ¨å›¾åƒä¸­æ•æ‰åˆ°ï¼Œå› ä¸ºå®ƒä»¬åœ¨å›¾åƒä¸­çš„æ¯”ä¾‹å¾ˆå°ã€‚å› æ­¤ï¼Œ**è¿«åˆ‡éœ€è¦æå‡å¯¹ç»†é•¿ç®¡çŠ¶ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›**ï¼Œæ‰€ä»¥åœ¨è¿™ä¸€èƒŒæ™¯ä¸‹ä½œè€…æå‡ºäº†åŠ¨æ€è›‡å½¢å·ç§¯(Dynamic Snake Convolution)ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/cf037049fdd34b5dbd8d4ee47783dd66.png)

**åŸç†->**ä¸Šå›¾å±•ç¤ºäº†ä¸€ä¸ª**ä¸‰ç»´å¿ƒè„è¡€ç®¡æ•°æ®é›†**å’Œä¸€ä¸ª**äºŒç»´è¿œç¨‹é“è·¯æ•°æ®é›†**ã€‚è¿™ä¸¤ä¸ªæ•°æ®é›†æ—¨åœ¨æå–ç®¡çŠ¶ç»“æ„ï¼Œä½†ç”±äº**è„†å¼±çš„å±€éƒ¨ç»“æ„å’Œå¤æ‚çš„æ•´ä½“å½¢æ€**ï¼Œè¿™ä¸ªä»»åŠ¡é¢ä¸´ç€æŒ‘æˆ˜ã€‚æ ‡å‡†çš„**å·ç§¯æ ¸**æ—¨åœ¨æå–å±€éƒ¨ç‰¹å¾ã€‚åŸºäºæ­¤ï¼Œè®¾è®¡äº†å¯å˜å½¢å·ç§¯æ ¸ä»¥ä¸°å¯Œå®ƒä»¬çš„åº”ç”¨ï¼Œå¹¶é€‚åº”ä¸åŒç›®æ ‡çš„å‡ ä½•å˜å½¢ã€‚ç„¶è€Œï¼Œç”±äºå‰é¢æåˆ°çš„æŒ‘æˆ˜ï¼Œæœ‰æ•ˆåœ°èšç„¦äºç»†å°çš„ç®¡çŠ¶ç»“æ„æ˜¯å›°éš¾çš„ã€‚

ç”±äºä»¥ä¸‹å›°éš¾ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼š

1.  **ç»†å°è€Œè„†å¼±çš„å±€éƒ¨ç»“æ„**ï¼šå¦‚ä¸Šé¢çš„å›¾æ‰€ç¤ºï¼Œç»†å°çš„ç»“æ„ä»…å æ•´ä½“å›¾åƒçš„ä¸€å°éƒ¨åˆ†ï¼Œå¹¶ä¸”ç”±äºåƒç´ ç»„æˆæœ‰é™ï¼Œè¿™äº›ç»“æ„å®¹æ˜“å—åˆ°å¤æ‚èƒŒæ™¯çš„å¹²æ‰°ï¼Œä½¿æ¨¡å‹éš¾ä»¥ç²¾ç¡®åœ°åŒºåˆ†ç›®æ ‡çš„ç»†å¾®å˜åŒ–ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯èƒ½éš¾ä»¥åŒºåˆ†è¿™äº›ç»“æ„ï¼Œå¯¼è‡´åˆ†å‰²ç»“æœå‡ºç°æ–­è£‚ã€‚
    
2.  **å¤æ‚è€Œå¤šå˜çš„æ•´ä½“å½¢æ€**ï¼šä¸Šé¢çš„å›¾ç‰‡å±•ç¤ºäº†ç»†å°ç®¡çŠ¶ç»“æ„çš„å¤æ‚å’Œå¤šå˜å½¢æ€ï¼Œå³ä½¿åœ¨åŒä¸€å›¾åƒä¸­ä¹Ÿå¦‚æ­¤ã€‚ä¸åŒåŒºåŸŸä¸­çš„ç›®æ ‡å‘ˆç°å‡ºå½¢æ€ä¸Šçš„å˜åŒ–ï¼ŒåŒ…æ‹¬åˆ†æ”¯æ•°é‡ã€åˆ†å‰ä½ç½®å’Œè·¯å¾„é•¿åº¦ç­‰ã€‚å½“æ•°æ®å‘ˆç°å‡ºå‰æ‰€æœªè§çš„å½¢æ€ç»“æ„æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿‡åº¦æ‹Ÿåˆå·²ç»è§è¿‡çš„ç‰¹å¾ï¼Œå¯¼è‡´åœ¨æ–°çš„å½¢æ€ç»“æ„ä¸‹æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚

ä¸ºäº†åº”å¯¹ä¸Šè¿°éšœç¢ï¼Œæå‡ºäº†å¦‚ä¸‹è§£å†³æ–¹æ¡ˆï¼Œ**å…¶ä¸­åŒ…æ‹¬ç®¡çŠ¶æ„ŸçŸ¥å·ç§¯æ ¸ã€å¤šè§†è§’ç‰¹å¾èåˆç­–ç•¥å’Œæ‹“æ‰‘è¿ç»­æ€§çº¦æŸæŸå¤±å‡½æ•°**ã€‚å…·ä½“å¦‚ä¸‹ï¼š

Â Â Â Â Â Â Â Â 1. **é’ˆå¯¹ç»†å°ä¸”è„†å¼±çš„å±€éƒ¨ç»“æ„æ‰€å æ¯”ä¾‹å°ä¸”éš¾ä»¥èšç„¦çš„æŒ‘æˆ˜**ï¼Œæå‡ºäº†åŠ¨æ€è›‡å½¢å·ç§¯ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°èšç„¦äºç®¡çŠ¶ç»“æ„çš„ç»†é•¿æ›²çº¿å±€éƒ¨ç‰¹å¾ï¼Œå¢å¼ºå¯¹å‡ ä½•ç»“æ„çš„æ„ŸçŸ¥ã€‚ä¸å¯å˜å½¢å·ç§¯ä¸åŒï¼ŒDSConvè€ƒè™‘åˆ°ç®¡çŠ¶ç»“æ„çš„è›‡å½¢å½¢æ€ï¼Œå¹¶é€šè¿‡çº¦æŸè¡¥å……è‡ªç”±å­¦ä¹ è¿‡ç¨‹ï¼Œæœ‰é’ˆå¯¹æ€§åœ°å¢å¼ºå¯¹ç®¡çŠ¶ç»“æ„çš„æ„ŸçŸ¥ã€‚

Â Â Â Â Â Â Â Â 2. **é’ˆå¯¹å¤æ‚å’Œå¤šå˜çš„æ•´ä½“å½¢æ€çš„æŒ‘æˆ˜**ï¼Œæå‡ºäº†ä¸€ç§å¤šè§†è§’ç‰¹å¾èåˆç­–ç•¥ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼ŒåŸºäºDSConvç”Ÿæˆå¤šä¸ªå½¢æ€å­¦å·ç§¯æ ¸æ¨¡æ¿ï¼Œä»ä¸åŒè§’åº¦è§‚å¯Ÿç›®æ ‡çš„ç»“æ„ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ€»ç»“å…¸å‹çš„é‡è¦ç‰¹å¾å®ç°é«˜æ•ˆçš„ç‰¹å¾èåˆã€‚

Â Â Â Â Â Â Â Â 3. **é’ˆå¯¹ç®¡çŠ¶ç»“æ„åˆ†å‰²å®¹æ˜“å‡ºç°æ–­è£‚çš„é—®é¢˜**ï¼Œæå‡ºäº†åŸºäºæŒä¹…åŒè°ƒï¼ˆPersistent Homologyï¼ŒPHï¼‰çš„æ‹“æ‰‘è¿ç»­æ€§çº¦æŸæŸå¤±å‡½æ•°ï¼ˆTCLossï¼‰ã€‚PHæ˜¯ä¸€ç§ä»å‡ºç°åˆ°æ¶ˆå¤±çš„æ‹“æ‰‘ç‰¹å¾å“åº”è¿‡ç¨‹ï¼Œèƒ½å¤Ÿä»å˜ˆæ‚çš„é«˜ç»´æ•°æ®ä¸­è·å–è¶³å¤Ÿçš„æ‹“æ‰‘ä¿¡æ¯ã€‚ç›¸å…³çš„è´è’‚æ•°æ˜¯æè¿°æ‹“æ‰‘ç©ºé—´è¿é€šæ€§çš„ä¸€ç§æ–¹å¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œ**TCLosså°†PHä¸ç‚¹é›†ç›¸ä¼¼æ€§ç›¸ç»“åˆ**ï¼Œå¼•å¯¼ç½‘ç»œå…³æ³¨å…·æœ‰å¼‚å¸¸åƒç´ /ä½“ç´ åˆ†å¸ƒçš„æ–­è£‚åŒºåŸŸï¼Œä»æ‹“æ‰‘è§’åº¦å®ç°è¿ç»­æ€§çº¦æŸã€‚

> **æ€»ç»“**:ä¸ºäº†å…‹æœæŒ‘æˆ˜ï¼Œæå‡ºäº†DSCNetæ¡†æ¶ï¼ŒåŒ…æ‹¬ç®¡çŠ¶æ„ŸçŸ¥å·ç§¯æ ¸ã€å¤šè§†è§’ç‰¹å¾èåˆç­–ç•¥å’Œæ‹“æ‰‘è¿ç»­æ€§çº¦æŸæŸå¤±å‡½æ•°ã€‚DSConvå¢å¼ºäº†å¯¹ç»†é•¿æ›²çº¿ç‰¹å¾çš„æ„ŸçŸ¥ï¼Œå¤šè§†è§’ç‰¹å¾èåˆç­–ç•¥æé«˜äº†å¯¹å¤æ‚æ•´ä½“å½¢æ€çš„å¤„ç†èƒ½åŠ›ï¼Œè€ŒTCLossåŸºäºæŒä¹…åŒè°ƒå®ç°äº†ä»æ‹“æ‰‘è§’åº¦çš„è¿ç»­æ€§çº¦æŸã€‚

ä¸‰ã€åŠ¨æ€è›‡å½¢å·ç§¯çš„ä¼˜åŠ¿
-----------

ä¸ºäº†æé«˜å¯¹ç®¡çŠ¶ç»“æ„çš„æ€§èƒ½ï¼Œå·²ç»æå‡ºäº†å„ç§æ–¹æ³•ï¼Œæ ¹æ®ç®¡çŠ¶ç»“æ„çš„å½¢æ€è®¾è®¡äº†ç‰¹å®šçš„ç½‘ç»œæ¶æ„å’Œæ¨¡å—ã€‚å…·ä½“å¦‚ä¸‹ï¼š

**1\. åŸºäºå·ç§¯æ ¸è®¾è®¡çš„æ–¹æ³•**ï¼šè‘—åçš„æ‰©å¼ å·ç§¯ï¼ˆdilated convolutionï¼‰å’Œå¯å˜å½¢å·ç§¯ï¼ˆdeformable convolutionï¼‰ç­‰æ–¹æ³•è¢«æå‡ºæ¥å¤„ç†å·ç§¯ç¥ç»ç½‘ç»œä¸­å›ºæœ‰çš„å‡ ä½•å˜æ¢é™åˆ¶ï¼Œå¹¶åœ¨å¤æ‚çš„æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ã€‚è¿™äº›æ–¹æ³•è¿˜è¢«è®¾è®¡ç”¨äºåŠ¨æ€æ„ŸçŸ¥å¯¹è±¡çš„å‡ ä½•ç‰¹å¾ï¼Œä»¥é€‚åº”å…·æœ‰å¯å˜å½¢æ€çš„ç»“æ„ã€‚ä¾‹å¦‚ï¼ŒDUNetã€‚

**2\. åŸºäºå½¢æ€å­¦çš„æ–¹æ³•**ï¼šä¸€äº›æ–¹æ³•ä¸“æ³¨äºåˆ©ç”¨å½¢æ€å­¦ä¿¡æ¯æ¥å¤„ç†ç®¡çŠ¶ç»“æ„ã€‚ä¾‹å¦‚ï¼Œå½¢æ€å­¦é‡å»ºç½‘ç»œï¼ˆMorphological Reconstruction Networkï¼‰åˆ©ç”¨å½¢æ€å­¦é‡å»ºæ“ä½œæ¥é‡å»ºç®¡çŠ¶ç»“æ„ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„åˆ†å‰²ã€‚å¦å¤–ï¼Œå½¢æ€å­¦æ“ä½œå¦‚å¼€è¿ç®—å’Œé—­è¿ç®—ä¹Ÿè¢«å¹¿æ³›åº”ç”¨äºå¤„ç†ç®¡çŠ¶ç»“æ„ã€‚

**3\. åŸºäºæ‹“æ‰‘å­¦çš„æ–¹æ³•**ï¼šæ‹“æ‰‘å­¦æ–¹æ³•è¢«ç”¨æ¥å¤„ç†ç®¡çŠ¶ç»“æ„çš„æ‹“æ‰‘ç‰¹å¾ã€‚ä¾‹å¦‚ï¼ŒåŸºäºæŒä¹…åŒè°ƒï¼ˆPersistent Homologyï¼‰çš„æ–¹æ³•å¯ä»¥ä»é«˜ç»´æ•°æ®ä¸­è·å–æ‹“æ‰‘ä¿¡æ¯ï¼Œå¹¶ç”¨äºåˆ†æç®¡çŠ¶ç»“æ„çš„è¿é€šæ€§å’Œå½¢æ€ç‰¹å¾ã€‚

> æ€»ç»“ï¼šä¸ºäº†å¤„ç†ç®¡çŠ¶ç»“æ„ï¼Œå·²ç»æå‡ºäº†å¤šç§æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬åŸºäºå·ç§¯æ ¸è®¾è®¡çš„æ–¹æ³•ã€åŸºäºå½¢æ€å­¦çš„æ–¹æ³•å’ŒåŸºäºæ‹“æ‰‘å­¦çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•çš„ç›®æ ‡æ˜¯é€šè¿‡è®¾è®¡é€‚åº”ç®¡çŠ¶ç»“æ„å½¢æ€çš„ç½‘ç»œæ¶æ„å’Œæ¨¡å—ï¼Œæé«˜å¯¹ç®¡çŠ¶ç»“æ„çš„æ£€æµ‹å’Œåˆ†å‰²æ€§èƒ½ã€‚

**ä¼˜åŠ¿**ï¼šä»¥ä¸Šæ‰€è¿°çš„æ–¹æ³•éƒ½åªæ˜¯ä»å•ä¸€çš„è§’åº¦å»åˆ†æï¼ŒDSConvæå‡ºäº†ä¸€ç§å¤šè§’åº¦ç‰¹å¾èåˆç­–ç•¥ï¼Œä»å¤šä¸ªè§’åº¦è¡¥å……å¯¹é‡è¦ç‰¹å¾çš„å…³æ³¨ã€‚åœ¨è¿™ä¸ªç­–ç•¥ä¸­ï¼ŒåŸºäºåŠ¨æ€è›‡å½¢å·ç§¯ï¼ˆDSConvï¼‰ç”Ÿæˆå¤šä¸ªå½¢æ€å­¦å·ç§¯æ ¸æ¨¡æ¿ï¼Œä»å¤šä¸ªè§’åº¦è§‚å¯Ÿç›®æ ‡çš„ç»“æ„ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ€»ç»“å…³é”®çš„æ ‡å‡†ç‰¹å¾å®ç°ç‰¹å¾èåˆï¼Œä»è€Œæé«˜æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ã€‚

å››ã€å®éªŒå’Œç»“æœ
-------

### 4.1 æ•°æ®é›†

ä½¿ç”¨äº†ä¸‰ä¸ªæ•°æ®é›†æ¥éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸€ä¸ªå†…éƒ¨æ•°æ®é›†ã€‚åœ¨2Dæ–¹é¢ï¼Œè¯„ä¼°äº†DRIVEè§†ç½‘è†œæ•°æ®é›†å’Œé©¬è¨è¯¸å¡é“è·¯æ•°æ®é›†ã€‚åœ¨3Dæ–¹é¢ï¼Œä½¿ç”¨äº†ä¸€ä¸ªåä¸ºCardiac CCTA Dataçš„æ•°æ®é›†ã€‚

### 4.2 å®éªŒ

è¿›è¡Œäº†æ¯”è¾ƒå®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜DSCNçš„ä¼˜åŠ¿ã€‚ä¸ç»å…¸çš„åˆ†å‰²ç½‘ç»œU-Net å’Œ2021å¹´æå‡ºçš„ç”¨äºè¡€ç®¡åˆ†å‰²çš„CS2-Net è¿›è¡Œæ¯”è¾ƒï¼Œä»¥éªŒè¯å‡†ç¡®æ€§ã€‚ä¸ºäº†éªŒè¯ç½‘ç»œè®¾è®¡æ€§èƒ½ï¼Œå°†2022å¹´æå‡ºçš„ç”¨äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„DCU-Net è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†éªŒè¯ç‰¹å¾èåˆçš„ä¼˜åŠ¿ï¼Œå°†2021å¹´æå‡ºçš„ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„Transunet è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†éªŒè¯æŸå¤±å‡½æ•°çº¦æŸï¼Œå°†2021å¹´æå‡ºçš„clDiceå’ŒåŸºäºWassersteinè·ç¦»çš„TCLoss LWTCè¿›è¡Œäº†æ¯”è¾ƒã€‚è¿™äº›æ¨¡å‹åœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿›è¡Œäº†ç²¾ç¡®çš„å®ç°ï¼Œé€šè¿‡ä»¥ä¸‹æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯é’ˆå¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œè®¡ç®—å¹¶æ±‚å¹³å‡ã€‚

> **1\. ä½“ç§¯å¾—åˆ†**ï¼šä½¿ç”¨å¹³å‡Diceç³»æ•°ï¼ˆDiceï¼‰ã€ç›¸å¯¹Diceç³»æ•°ï¼ˆRDiceï¼‰ã€ä¸­å¿ƒçº¿Diceï¼ˆclDiceï¼‰ã€å‡†ç¡®åº¦ï¼ˆACCï¼‰å’ŒAUCæ¥è¯„ä¼°ç»“æœçš„æ€§èƒ½ã€‚  
> **2\. æ‹“æ‰‘é”™è¯¯**ï¼šè®¡ç®—åŸºäºæ‹“æ‰‘çš„å¾—åˆ†ï¼ŒåŒ…æ‹¬Bettiæ•°Î²0å’ŒÎ²1çš„Bettié”™è¯¯ã€‚åŒæ—¶ï¼Œä¸ºäº†å®¢è§‚éªŒè¯å† çŠ¶åŠ¨è„‰åˆ†å‰²çš„è¿ç»­æ€§ï¼Œä½¿ç”¨ç›´åˆ°ç¬¬ä¸€ä¸ªé”™è¯¯çš„é‡å ï¼ˆOFï¼‰æ¥è¯„ä¼°æå–çš„ä¸­å¿ƒçº¿çš„å®Œæ•´æ€§ã€‚  
> **3\. è·ç¦»é”™è¯¯**ï¼šHausdorffè·ç¦»ï¼ˆHDï¼‰ä¹Ÿè¢«å¹¿æ³›ç”¨äºæè¿°ä¸¤ç»„ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¨èç”¨äºè¯„ä¼°è–„ç®¡çŠ¶ç»“æ„çš„ç›¸ä¼¼æ€§ã€‚

### 4.3 å®éªŒç»“æœ

åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­å±•ç¤ºäº†DSCNetæ–¹æ³•åœ¨æ¯ä¸ªæŒ‡æ ‡ä¸Šçš„ä¼˜åŠ¿ï¼Œç»“æœè¡¨æ˜æå‡ºçš„DSCNetåœ¨2Då’Œ3Dæ•°æ®é›†ä¸Šå–å¾—äº†æ›´å¥½çš„ç»“æœã€‚

> åœ¨DRIVEæ•°æ®é›†ä¸Šçš„è¯„ä¼°ä¸­ï¼ŒDSCNetåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œæ‹“æ‰‘è¿ç»­æ€§æ–¹é¢å‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒDSCNetåœ¨ä½“ç§¯å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æœ€ä½³çš„åˆ†å‰²ç»“æœï¼ŒDiceç³»æ•°ä¸º82.06%ï¼ŒRDiceç³»æ•°ä¸º90.17%ï¼ŒclDiceç³»æ•°ä¸º82.07%ï¼Œå‡†ç¡®åº¦ä¸º96.87%ï¼ŒAUCä¸º90.27%ã€‚åŒæ—¶ï¼Œä»æ‹“æ‰‘çš„è§’åº¦æ¥çœ‹ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒDSCNetåœ¨æ‹“æ‰‘è¿ç»­æ€§ä¸Šå–å¾—äº†æœ€å¥½çš„ç»“æœï¼ŒÎ²0é”™è¯¯ä¸º0.998ï¼ŒÎ²1é”™è¯¯ä¸º0.803ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDSCNetæ–¹æ³•æ›´å¥½åœ°æ•æ‰äº†è–„ç®¡çŠ¶ç»“æ„çš„ç‰¹å¾ï¼Œå¹¶å±•ç°å‡ºæ›´å‡†ç¡®çš„åˆ†å‰²æ€§èƒ½å’Œæ›´è¿ç»­çš„æ‹“æ‰‘ç»“æ„ã€‚æ­£å¦‚è¡¨æ ¼1ä¸­ç¬¬6è¡Œåˆ°ç¬¬12è¡Œæ‰€ç¤ºï¼Œåœ¨å¼•å…¥TCLossåï¼Œä¸åŒçš„æ¨¡å‹åœ¨åˆ†å‰²çš„æ‹“æ‰‘è¿ç»­æ€§æ–¹é¢å‡æœ‰æ‰€æ”¹å–„ã€‚ç»“æœè¡¨æ˜ï¼ŒTCLossèƒ½å¤Ÿå‡†ç¡®åœ°çº¦æŸæ¨¡å‹å…³æ³¨å¤±å»æ‹“æ‰‘è¿ç»­æ€§çš„è–„ç®¡çŠ¶ç»“æ„ã€‚åœ¨ROADSæ•°æ®é›†ä¸Šçš„è¯„ä¼°ä¸­ï¼ŒDSCNetåŒæ ·å–å¾—äº†æœ€ä½³ç»“æœã€‚å¦‚è¡¨æ ¼1æ‰€ç¤ºï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„å¸¦æœ‰TCLossçš„DSCNetåœ¨åˆ†å‰²ç»“æœä¸Šå–å¾—äº†æœ€ä½³çš„æ•ˆæœï¼ŒDiceç³»æ•°ä¸º78.21%ï¼ŒRDiceç³»æ•°ä¸º85.85%ï¼ŒclDiceç³»æ•°ä¸º87.64%ã€‚ä¸ç»å…¸çš„åˆ†å‰²ç½‘ç»œUNetçš„ç»“æœç›¸æ¯”ï¼ŒDSCNetçš„æ–¹æ³•åœ¨Diceç³»æ•°ã€RDiceç³»æ•°å’ŒclDiceç³»æ•°ä¸Šåˆ†åˆ«æ”¹å–„äº†æœ€å¤š1.31%ã€1.78%å’Œ0.77%ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒDSCNetçš„æ¨¡å‹åœ¨ç»“æ„å¤æ‚ä¸”å½¢æ€å¤šå˜çš„é“è·¯æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

### ![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/59a589c44020482d9a5045635fd302ed.png)

> åœ¨CORONARYæ•°æ®é›†ä¸Šçš„è¯„ä¼°ä¸­ï¼ŒéªŒè¯äº†DSCNetåœ¨3Dè–„ç®¡çŠ¶ç»“æ„åˆ†å‰²ä»»åŠ¡ä¸Šä»ç„¶å–å¾—äº†æœ€ä½³ç»“æœã€‚å¦‚ä¸‹é¢çš„è¡¨æ ¼æ‰€ç¤ºï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„DSCNetåœ¨åˆ†å‰²ç»“æœä¸Šå–å¾—äº†æœ€ä½³çš„æ•ˆæœï¼ŒDiceç³»æ•°ä¸º80.27%ï¼ŒRDiceç³»æ•°ä¸º86.37%ï¼ŒclDiceç³»æ•°ä¸º85.26%ã€‚ä¸ç»å…¸çš„åˆ†å‰²ç½‘ç»œUNetçš„ç»“æœç›¸æ¯”ï¼ŒDSCNetæ–¹æ³•åœ¨Diceç³»æ•°ã€RDiceç³»æ•°å’ŒclDiceç³»æ•°ä¸Šåˆ†åˆ«æ”¹å–„äº†æœ€å¤š3.40%ã€1.89%å’Œ3.83%ã€‚åŒæ—¶ï¼Œä½¿ç”¨OFæŒ‡æ ‡æ¥è¯„ä¼°åˆ†å‰²çš„è¿ç»­æ€§ã€‚ä½¿ç”¨DSCNetçš„æ–¹æ³•ï¼ŒLADçš„OFæŒ‡æ ‡æå‡äº†6.00%ï¼ŒLCXçš„OFæŒ‡æ ‡æå‡äº†3.78%ï¼Œè€ŒRCAçš„OFæŒ‡æ ‡æå‡äº†3.30%

### ![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/03031346d4854644a8ae8a10da3ae3ab.png)4.4 æœ‰æ•ˆæ€§å±•ç¤º

DSCNetå’ŒTCLossåœ¨å„ä¸ªæ–¹é¢éƒ½å…·æœ‰å†³å®šæ€§çš„è§†è§‰ä¼˜åŠ¿ã€‚

(1) ä¸ºäº†å±•ç¤ºDSCNetçš„æœ‰æ•ˆæ€§ä¸‹é¢çš„å›¾ç‰‡ä¸­ã€‚ä»å·¦åˆ°å³ï¼Œç¬¬ä¸‰åˆ°ç¬¬äº”åˆ—å±•ç¤ºäº†ä¸åŒç½‘ç»œåœ¨åˆ†å‰²å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°ã€‚ç”±äºDSConvèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ„ŸçŸ¥å…³é”®ç‰¹å¾ï¼ŒDSCNetçš„æ–¹æ³•åœ¨åˆ†å‰²ç»“æœä¸Šè¡¨ç°å‡ºè‰²ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒDSCNetçš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å’Œä¿ç•™è–„ç®¡çŠ¶ç»“æ„çš„ç»†èŠ‚ã€‚

(2) ä¸ºäº†éªŒè¯DSCNetçš„TCLossçš„æœ‰æ•ˆæ€§ï¼Œç¬¬å…­åˆ—å±•ç¤ºäº†åœ¨æ²¡æœ‰ä½¿ç”¨TCLossçš„æƒ…å†µä¸‹çš„åˆ†å‰²ç»“æœã€‚å¯ä»¥çœ‹å‡ºï¼Œæ²¡æœ‰TCLossçš„æ–¹æ³•åœ¨æ‹“æ‰‘è¿ç»­æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„é—®é¢˜ï¼Œè€ŒDSCNetçš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡TCLosså‡†ç¡®åœ°çº¦æŸåˆ†å‰²ç»“æœçš„æ‹“æ‰‘ç»“æ„ï¼Œä½¿å¾—åˆ†å‰²ç»“æœæ›´åŠ è¿ç»­ã€‚

(3) åœ¨ç¬¬ä¸ƒåˆ—å’Œç¬¬å…«åˆ—ä¸­ï¼Œå±•ç¤ºäº†DSCNetåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„åˆ†å‰²ç»“æœã€‚å¯ä»¥çœ‹åˆ°ï¼ŒDSCNetåœ¨DRIVEå’ŒROADSæ•°æ®é›†ä¸Šéƒ½èƒ½å–å¾—å‡†ç¡®ä¸”è¿ç»­çš„åˆ†å‰²ç»“æœï¼Œè¿›ä¸€æ­¥è¯æ˜äº†DSCNetçš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚

> æ€»çš„æ¥è¯´ï¼Œä»å›¾6å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°æˆ‘ä»¬çš„DSCNetå’ŒTCLossåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œæ‹“æ‰‘è¿ç»­æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/a8eede075bba486f9ec56fe788d85528.png)

DSConvèƒ½å¤ŸåŠ¨æ€åœ°é€‚åº”ç®¡çŠ¶ç»“æ„çš„å½¢çŠ¶ï¼Œå¹¶ä¸”æ³¨æ„åŠ›èƒ½å¤Ÿå¾ˆå¥½åœ°é€‚é…ç›®æ ‡ã€‚

(1) é€‚åº”ç®¡çŠ¶ç»“æ„çš„å½¢çŠ¶ã€‚ä¸‹é¢çš„å›¾ç‰‡ä¸­çš„é¡¶éƒ¨æ˜¾ç¤ºäº†å·ç§¯æ ¸çš„ä½ç½®å’Œå½¢çŠ¶ã€‚å¯è§†åŒ–ç»“æœæ˜¾ç¤ºï¼ŒDSConvèƒ½å¤Ÿå¾ˆå¥½åœ°é€‚åº”ç®¡çŠ¶ç»“æ„å¹¶ä¿æŒå½¢çŠ¶ï¼Œè€Œå¯å˜å½¢å·ç§¯åˆ™åœ¨ç›®æ ‡å¤–éƒ¨æ¸¸èµ°ã€‚

(2) å…³æ³¨ç®¡çŠ¶ç»“æ„çš„ä½ç½®ã€‚ä¸‹é¢çš„å›¾ç‰‡çš„åº•éƒ¨æ˜¾ç¤ºäº†ç»™å®šç‚¹çš„æ³¨æ„åŠ›çƒ­åŠ›å›¾ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDSConvæœ€äº®çš„åŒºåŸŸé›†ä¸­åœ¨ç®¡çŠ¶ç»“æ„ä¸Šï¼Œè¿™è¡¨ç¤ºDSConvå¯¹ç®¡çŠ¶ç»“æ„æ›´åŠ æ•æ„Ÿã€‚

> è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DSConvèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å’Œå…³æ³¨ç®¡çŠ¶ç»“æ„ï¼Œä»è€Œä½¿å¾—DSCNetèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å’Œåˆ†å‰²è¿™äº›ç»“æ„ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/cb2f1bcbfb01402b95a00796ec3f43ed.png)

* * *

äº”ã€æ ¸å¿ƒä»£ç 
------

**ä½¿ç”¨æ–¹å¼çœ‹ç« èŠ‚å…­**

```python
import torch
import torch.nn as nn
from ..modules.conv import Conv
 
 
class DySnakeConv(nn.Module):
    def __init__(self, inc, ouc, k=3) -> None:
        super().__init__()
 
        self.conv_0 = Conv(inc, ouc, k)
        self.conv_x = DSConv(inc, ouc, 0, k)
        self.conv_y = DSConv(inc, ouc, 1, k)
 
    def forward(self, x):
        return torch.cat([self.conv_0(x), self.conv_x(x), self.conv_y(x)], dim=1)
 
 
class DSConv(nn.Module):
    def __init__(self, in_ch, out_ch, morph, kernel_size=3, if_offset=True, extend_scope=1):
        """
        The Dynamic Snake Convolution
        :param in_ch: input channel
        :param out_ch: output channel
        :param kernel_size: the size of kernel
        :param extend_scope: the range to expand (default 1 for this method)
        :param morph: the morphology of the convolution kernel is mainly divided into two types
                        along the x-axis (0) and the y-axis (1) (see the paper for details)
        :param if_offset: whether deformation is required, if it is False, it is the standard convolution kernel
        """
        super(DSConv, self).__init__()
        # use the <offset_conv> to learn the deformable offset
        self.offset_conv = nn.Conv2d(in_ch, 2 * kernel_size, 3, padding=1)
        self.bn = nn.BatchNorm2d(2 * kernel_size)
        self.kernel_size = kernel_size
 
        # two types of the DSConv (along x-axis and y-axis)
        self.dsc_conv_x = nn.Conv2d(
            in_ch,
            out_ch,
            kernel_size=(kernel_size, 1),
            stride=(kernel_size, 1),
            padding=0,
        )
        self.dsc_conv_y = nn.Conv2d(
            in_ch,
            out_ch,
            kernel_size=(1, kernel_size),
            stride=(1, kernel_size),
            padding=0,
        )
 
        self.gn = nn.GroupNorm(out_ch // 4, out_ch)
        self.act = Conv.default_act
 
        self.extend_scope = extend_scope
        self.morph = morph
        self.if_offset = if_offset
 
    def forward(self, f):
        offset = self.offset_conv(f)
        offset = self.bn(offset)
        # We need a range of deformation between -1 and 1 to mimic the snake's swing
        offset = torch.tanh(offset)
        input_shape = f.shape
        dsc = DSC(input_shape, self.kernel_size, self.extend_scope, self.morph)
        deformed_feature = dsc.deform_conv(f, offset, self.if_offset)
        if self.morph == 0:
            x = self.dsc_conv_x(deformed_feature.type(f.dtype))
            x = self.gn(x)
            x = self.act(x)
            return x
        else:
            x = self.dsc_conv_y(deformed_feature.type(f.dtype))
            x = self.gn(x)
            x = self.act(x)
            return x
 
 
# Core code, for ease of understanding, we mark the dimensions of input and output next to the code
class DSC(object):
    def __init__(self, input_shape, kernel_size, extend_scope, morph):
        self.num_points = kernel_size
        self.width = input_shape[2]
        self.height = input_shape[3]
        self.morph = morph
        self.extend_scope = extend_scope  # offset (-1 ~ 1) * extend_scope
 
        # define feature map shape
        """
        B: Batch size  C: Channel  W: Width  H: Height
        """
        self.num_batch = input_shape[0]
        self.num_channels = input_shape[1]
 
    """
    input: offset [B,2*K,W,H]  K: Kernel size (2*K: 2D image, deformation contains <x_offset> and <y_offset>)
    output_x: [B,1,W,K*H]   coordinate map
    output_y: [B,1,K*W,H]   coordinate map
    """
 
    def _coordinate_map_3D(self, offset, if_offset):
        device = offset.device
        # offset
        y_offset, x_offset = torch.split(offset, self.num_points, dim=1)
 
        y_center = torch.arange(0, self.width).repeat([self.height])
        y_center = y_center.reshape(self.height, self.width)
        y_center = y_center.permute(1, 0)
        y_center = y_center.reshape([-1, self.width, self.height])
        y_center = y_center.repeat([self.num_points, 1, 1]).float()
        y_center = y_center.unsqueeze(0)
 
        x_center = torch.arange(0, self.height).repeat([self.width])
        x_center = x_center.reshape(self.width, self.height)
        x_center = x_center.permute(0, 1)
        x_center = x_center.reshape([-1, self.width, self.height])
        x_center = x_center.repeat([self.num_points, 1, 1]).float()
        x_center = x_center.unsqueeze(0)
 
        if self.morph == 0:
            """
            Initialize the kernel and flatten the kernel
                y: only need 0
                x: -num_points//2 ~ num_points//2 (Determined by the kernel size)
                !!! The related PPT will be submitted later, and the PPT will contain the whole changes of each step
            """
            y = torch.linspace(0, 0, 1)
            x = torch.linspace(
                -int(self.num_points // 2),
                int(self.num_points // 2),
                int(self.num_points),
            )
 
            y, x = torch.meshgrid(y, x)
            y_spread = y.reshape(-1, 1)
            x_spread = x.reshape(-1, 1)
 
            y_grid = y_spread.repeat([1, self.width * self.height])
            y_grid = y_grid.reshape([self.num_points, self.width, self.height])
            y_grid = y_grid.unsqueeze(0)  # [B*K*K, W,H]
 
            x_grid = x_spread.repeat([1, self.width * self.height])
            x_grid = x_grid.reshape([self.num_points, self.width, self.height])
            x_grid = x_grid.unsqueeze(0)  # [B*K*K, W,H]
 
            y_new = y_center + y_grid
            x_new = x_center + x_grid
 
            y_new = y_new.repeat(self.num_batch, 1, 1, 1).to(device)
            x_new = x_new.repeat(self.num_batch, 1, 1, 1).to(device)
 
            y_offset_new = y_offset.detach().clone()
 
            if if_offset:
                y_offset = y_offset.permute(1, 0, 2, 3)
                y_offset_new = y_offset_new.permute(1, 0, 2, 3)
                center = int(self.num_points // 2)
 
                # The center position remains unchanged and the rest of the positions begin to swing
                # This part is quite simple. The main idea is that "offset is an iterative process"
                y_offset_new[center] = 0
                for index in range(1, center):
                    y_offset_new[center + index] = (y_offset_new[center + index - 1] + y_offset[center + index])
                    y_offset_new[center - index] = (y_offset_new[center - index + 1] + y_offset[center - index])
                y_offset_new = y_offset_new.permute(1, 0, 2, 3).to(device)
                y_new = y_new.add(y_offset_new.mul(self.extend_scope))
 
            y_new = y_new.reshape(
                [self.num_batch, self.num_points, 1, self.width, self.height])
            y_new = y_new.permute(0, 3, 1, 4, 2)
            y_new = y_new.reshape([
                self.num_batch, self.num_points * self.width, 1 * self.height
            ])
            x_new = x_new.reshape(
                [self.num_batch, self.num_points, 1, self.width, self.height])
            x_new = x_new.permute(0, 3, 1, 4, 2)
            x_new = x_new.reshape([
                self.num_batch, self.num_points * self.width, 1 * self.height
            ])
            return y_new, x_new
 
        else:
            """
            Initialize the kernel and flatten the kernel
                y: -num_points//2 ~ num_points//2 (Determined by the kernel size)
                x: only need 0
            """
            y = torch.linspace(
                -int(self.num_points // 2),
                int(self.num_points // 2),
                int(self.num_points),
            )
            x = torch.linspace(0, 0, 1)
 
            y, x = torch.meshgrid(y, x)
            y_spread = y.reshape(-1, 1)
            x_spread = x.reshape(-1, 1)
 
            y_grid = y_spread.repeat([1, self.width * self.height])
            y_grid = y_grid.reshape([self.num_points, self.width, self.height])
            y_grid = y_grid.unsqueeze(0)
 
            x_grid = x_spread.repeat([1, self.width * self.height])
            x_grid = x_grid.reshape([self.num_points, self.width, self.height])
            x_grid = x_grid.unsqueeze(0)
 
            y_new = y_center + y_grid
            x_new = x_center + x_grid
 
            y_new = y_new.repeat(self.num_batch, 1, 1, 1)
            x_new = x_new.repeat(self.num_batch, 1, 1, 1)
 
            y_new = y_new.to(device)
            x_new = x_new.to(device)
            x_offset_new = x_offset.detach().clone()
 
            if if_offset:
                x_offset = x_offset.permute(1, 0, 2, 3)
                x_offset_new = x_offset_new.permute(1, 0, 2, 3)
                center = int(self.num_points // 2)
                x_offset_new[center] = 0
                for index in range(1, center):
                    x_offset_new[center + index] = (x_offset_new[center + index - 1] + x_offset[center + index])
                    x_offset_new[center - index] = (x_offset_new[center - index + 1] + x_offset[center - index])
                x_offset_new = x_offset_new.permute(1, 0, 2, 3).to(device)
                x_new = x_new.add(x_offset_new.mul(self.extend_scope))
 
            y_new = y_new.reshape(
                [self.num_batch, 1, self.num_points, self.width, self.height])
            y_new = y_new.permute(0, 3, 1, 4, 2)
            y_new = y_new.reshape([
                self.num_batch, 1 * self.width, self.num_points * self.height
            ])
            x_new = x_new.reshape(
                [self.num_batch, 1, self.num_points, self.width, self.height])
            x_new = x_new.permute(0, 3, 1, 4, 2)
            x_new = x_new.reshape([
                self.num_batch, 1 * self.width, self.num_points * self.height
            ])
            return y_new, x_new
 
    """
    input: input feature map [N,C,D,W,H]ï¼›coordinate map [N,K*D,K*W,K*H] 
    output: [N,1,K*D,K*W,K*H]  deformed feature map
    """
 
    def _bilinear_interpolate_3D(self, input_feature, y, x):
        device = input_feature.device
        y = y.reshape([-1]).float()
        x = x.reshape([-1]).float()
 
        zero = torch.zeros([]).int()
        max_y = self.width - 1
        max_x = self.height - 1
 
        # find 8 grid locations
        y0 = torch.floor(y).int()
        y1 = y0 + 1
        x0 = torch.floor(x).int()
        x1 = x0 + 1
 
        # clip out coordinates exceeding feature map volume
        y0 = torch.clamp(y0, zero, max_y)
        y1 = torch.clamp(y1, zero, max_y)
        x0 = torch.clamp(x0, zero, max_x)
        x1 = torch.clamp(x1, zero, max_x)
 
        input_feature_flat = input_feature.flatten()
        input_feature_flat = input_feature_flat.reshape(
            self.num_batch, self.num_channels, self.width, self.height)
        input_feature_flat = input_feature_flat.permute(0, 2, 3, 1)
        input_feature_flat = input_feature_flat.reshape(-1, self.num_channels)
        dimension = self.height * self.width
 
        base = torch.arange(self.num_batch) * dimension
        base = base.reshape([-1, 1]).float()
 
        repeat = torch.ones([self.num_points * self.width * self.height
                             ]).unsqueeze(0)
        repeat = repeat.float()
 
        base = torch.matmul(base, repeat)
        base = base.reshape([-1])
 
        base = base.to(device)
 
        base_y0 = base + y0 * self.height
        base_y1 = base + y1 * self.height
 
        # top rectangle of the neighbourhood volume
        index_a0 = base_y0 - base + x0
        index_c0 = base_y0 - base + x1
 
        # bottom rectangle of the neighbourhood volume
        index_a1 = base_y1 - base + x0
        index_c1 = base_y1 - base + x1
 
        # get 8 grid values
        value_a0 = input_feature_flat[index_a0.type(torch.int64)].to(device)
        value_c0 = input_feature_flat[index_c0.type(torch.int64)].to(device)
        value_a1 = input_feature_flat[index_a1.type(torch.int64)].to(device)
        value_c1 = input_feature_flat[index_c1.type(torch.int64)].to(device)
 
        # find 8 grid locations
        y0 = torch.floor(y).int()
        y1 = y0 + 1
        x0 = torch.floor(x).int()
        x1 = x0 + 1
 
        # clip out coordinates exceeding feature map volume
        y0 = torch.clamp(y0, zero, max_y + 1)
        y1 = torch.clamp(y1, zero, max_y + 1)
        x0 = torch.clamp(x0, zero, max_x + 1)
        x1 = torch.clamp(x1, zero, max_x + 1)
 
        x0_float = x0.float()
        x1_float = x1.float()
        y0_float = y0.float()
        y1_float = y1.float()
 
        vol_a0 = ((y1_float - y) * (x1_float - x)).unsqueeze(-1).to(device)
        vol_c0 = ((y1_float - y) * (x - x0_float)).unsqueeze(-1).to(device)
        vol_a1 = ((y - y0_float) * (x1_float - x)).unsqueeze(-1).to(device)
        vol_c1 = ((y - y0_float) * (x - x0_float)).unsqueeze(-1).to(device)
 
        outputs = (value_a0 * vol_a0 + value_c0 * vol_c0 + value_a1 * vol_a1 +
                   value_c1 * vol_c1)
 
        if self.morph == 0:
            outputs = outputs.reshape([
                self.num_batch,
                self.num_points * self.width,
                1 * self.height,
                self.num_channels,
            ])
            outputs = outputs.permute(0, 3, 1, 2)
        else:
            outputs = outputs.reshape([
                self.num_batch,
                1 * self.width,
                self.num_points * self.height,
                self.num_channels,
            ])
            outputs = outputs.permute(0, 3, 1, 2)
        return outputs
 
    def deform_conv(self, input, offset, if_offset):
        y, x = self._coordinate_map_3D(offset, if_offset)
        deformed_feature = self._bilinear_interpolate_3D(input, y, x)
        return deformed_feature
 
 
class Bottleneck(nn.Module):
    """Standard bottleneck."""
 
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        """Initializes a bottleneck module with given input/output channels, shortcut option, group, kernels, and
        expansion.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2
 
    def forward(self, x):
        """'forward()' applies the YOLO FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
 
 
class Bottleneck_DySnakeConv(Bottleneck):
    """Standard bottleneck with DySnakeConv."""
 
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv2 = DySnakeConv(c_, c2, k[1])
        self.cv3 = Conv(c2 * 3, c2, k=1)
 
    def forward(self, x):
        """'forward()' applies the YOLOv5 FPN to input data."""
        return x + self.cv3(self.cv2(self.cv1(x))) if self.add else self.cv3(self.cv2(self.cv1(x)))
 
 
class C2f_DSConv(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""
 
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        """Initialize CSP bottleneck layer with two convolutions with arguments ch_in, ch_out, number, shortcut, groups,
        expansion.
        """
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck_DySnakeConv(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))
 
    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
 
    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))
 
```

* * *

å…­ã€éœ€è¦æ”¹åŠ¨ä»£ç çš„åœ°æ–¹
-----------

### 6.1 æ·»åŠ æ­¥éª¤

#### 6.1.1 æ­¥éª¤ä¸€

é¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„ç›®å½•'ultralytics/nn/modules'ï¼Œç„¶ååœ¨è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpyæ–‡ä»¶ï¼Œåå­—å¯ä»¥æ ¹æ®ä½ è‡ªå·±çš„ä¹ æƒ¯èµ·ï¼Œç„¶åå°†DSConvçš„æ ¸å¿ƒä»£ç å¤åˆ¶è¿›å»ã€‚

#### 6.1.2 æ­¥éª¤äºŒ

ä¹‹åæˆ‘ä»¬æ‰¾åˆ°'ultralytics/nn/tasks.py'æ–‡ä»¶ï¼Œåœ¨å…¶ä¸­æ³¨å†Œæˆ‘ä»¬çš„MLCAå’ŒC2f\_MLCAæ¨¡å—ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦åœ¨æ–‡ä»¶çš„å¼€å¤´å¯¼å…¥æˆ‘ä»¬çš„MLCAå’ŒC2f\_MLCAæ¨¡å—ï¼Œ**å¦‚ä¸‹å›¾æ‰€ç¤º->**

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/62dac780291345528f2ed3bd23bbbd95.png)

#### 6.1.3 æ­¥éª¤ä¸‰

æˆ‘ä»¬æ‰¾åˆ°parse\_modelè¿™ä¸ªæ–¹æ³•ï¼Œå¯ä»¥ç”¨æœç´¢ä¹Ÿå¯ä»¥è‡ªå·±æ‰‹åŠ¨æ‰¾ï¼Œå¤§æ¦‚åœ¨å…­ç™¾å¤šè¡Œå§ã€‚Â æˆ‘ä»¬æ‰¾åˆ°å¦‚ä¸‹çš„åœ°æ–¹ï¼Œç„¶åå°†æˆ‘å¯¼å…¥çš„æ¨¡å—æ·»åŠ è¿›å»å³å¯ï¼Œæ¨¡ä»¿æˆ‘æ·»åŠ å³å¯(éœ€è¦æ³¨æ„çš„æ˜¯è¯¥å·ç§¯ä¸å¯æ›¿æ¢ä¸»å¹²ä¸Šçš„å·ç§¯ï¼Œè¯¥å·ç§¯çš„æ„é€ æ¯”è¾ƒç‰¹æ®Š)ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/1d7195860e40448a8a08180ce8b1e628.png)

**åˆ°æ­¤æˆ‘ä»¬å°±æ³¨å†ŒæˆåŠŸäº†ï¼Œå¯ä»¥ä¿®æ”¹yamlæ–‡ä»¶ä¸­è¾“å…¥MLCAå’ŒC2f\_MLCAä½¿ç”¨è¿™ä¸ªæ¨¡å—äº†ã€‚**

* * *

ä¸ƒã€DSConvçš„yamlæ–‡ä»¶å’Œè¿è¡Œè®°å½•
--------------------

### 7.1 DSConvçš„yamlæ–‡ä»¶ä¸€

**ä¸‹é¢çš„æ·»åŠ **DSConv**æ˜¯æˆ‘å®éªŒç»“æœçš„ç‰ˆæœ¬ã€‚**

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f_DSConv, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f_DSConv, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f_DSConv, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f_DSConv, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f_DSConv, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f_DSConv, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f_DSConv, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 7.2 DSConvçš„yamlæ–‡ä»¶äºŒ

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f_DSConv, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f_DSConv, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f_DSConv, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

### 7.3 DSConvçš„yamlæ–‡ä»¶ä¸‰

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect
 
# Parameters
nc: 80  # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  n: [0.33, 0.25, 1024]  # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  m: [0.67, 0.75, 768]   # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  l: [1.00, 1.00, 512]   # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  x: [1.00, 1.25, 512]   # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
 
# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9
 
# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12
 
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f_DSConv, [256]]  # 15 (P3/8-small)
 
  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f_DSConv, [512]]  # 18 (P4/16-medium)
 
  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f_DSConv, [1024]]  # 21 (P5/32-large)
 
  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)
```

* * *

### 7.4 DSConvçš„è®­ç»ƒè¿‡ç¨‹æˆªå›¾Â 

**ä¸‹é¢æ˜¯æ·»åŠ äº†**DSConv**çš„è®­ç»ƒæˆªå›¾ã€‚**

å¤§å®¶å¯ä»¥çœ‹ä¸‹é¢çš„è¿è¡Œç»“æœå’Œæ·»åŠ çš„ä½ç½®æ‰€ä»¥ä¸å­˜åœ¨æˆ‘å‘çš„ä»£ç ä¸å…¨æˆ–è€…è¿è¡Œä¸äº†çš„é—®é¢˜å¤§å®¶æœ‰é—®é¢˜ä¹Ÿå¯ä»¥åœ¨è¯„è®ºåŒºè¯„è®ºæˆ‘çœ‹åˆ°éƒ½ä¼šä¸ºå¤§å®¶è§£ç­”(æˆ‘çŸ¥é“çš„)ã€‚

![](https://yangyang666.oss-cn-chengdu.aliyuncs.com/typoraImages/d6672df181b949978077151be9ae165f.png)

