## 上采样

> 可以将较小的 feature 映射回一个较大的 feature map，这样的操作称为上采样，常用的上采样包括转置卷积，反池化，插值等操作。

![Snipaste_2025-02-24_14-54-56](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-54-56.png)





![Snipaste_2025-02-24_14-55-44](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_14-55-44.png)







**小特征图 -> 大特征图**

可学习的上采样：转置卷积





![Snipaste_2025-02-24_15-01-27](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-24_15-01-27.png)

转置卷积，也叫反卷积，它并不是正向卷积的完全逆过程，用一句话来解释：

> 反卷积是一种特殊的正向卷积，先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行普通的卷积。

> 卷积核大小：kernelsize
>
> 卷积步长：stride
>
> 特征图填充宽度：padding

对于普通的 kernelsize=(3,3),strides=(2,2) 的卷积，其过程如下图所示



![padding_strides](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides.gif)





kernelsize=(3,3),strides=(1,1) 情况下的反卷积则体现为：





![padding_strides_transposed](https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/padding_strides_transposed.gif)

我们可以发现，对于反卷积而言，补 0 主要是通过输入边缘的 padding 和输入内部插 0 实现。

具体的说，padding 的层数为 $kernelsize−stride$，而对于输入的内部插 0，其插入的 0 的数量为$stride−1$。



**接口与参数说明**

**torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)**

1. **in_channels**(int) – 输入信号的通道数
2. **out_channels** (int) – 卷积产生的通道数
3. **kerner_size** (int or tuple) - 卷积核的大小
4. **stride** (int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。
5. **padding** (int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding
6. **output_padding** (int or tuple, optional) - 输出边补充0的层数，高宽都增加padding
7. **groups** (int, optional) – 从输入通道到输出通道的阻塞连接数
8. **bias** (bool, optional) - 如果bias=True，添加偏置
9. **dilation** (int or tuple, optional) – 卷积核元素之间的间距

输出尺寸的计算公式为:
$$
H_{out}=(H_{in}-1)*stride[0]-2*padding[0]+kernelsize[0]+outputpadding[0]\\W_{out}=(W_{in}-1)*stride[1]-2*padding[1]+kernelsize[1]+outputpadding[1]
$$






















