<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Prompting Multi-Modal Image Segmentation with Semantic Grouping | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.CobjoJX1.css" as="style">
    
    <script type="module" src="/blog/assets/app.CEKjKXZT.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.D6Cah3zj.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.Db4DMmXV.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_Prompting_Multi-Moda_Segmetation.md.BmkSDMDZ.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Pytorch/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>Pytorch笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/deepLearning/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>深度学习笔记</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_Prompting_Multi-Moda_Segmetation" data-v-35102dec><div><h1 id="prompting-multi-modal-image-segmentation-with-semantic-grouping" tabindex="-1">Prompting Multi-Modal Image Segmentation with Semantic Grouping <a class="header-anchor" href="#prompting-multi-modal-image-segmentation-with-semantic-grouping" aria-label="Permalink to &quot;Prompting Multi-Modal Image Segmentation with Semantic Grouping&quot;">​</a></h1><h2 id="university-of-chinese-academy-of-sciences" tabindex="-1">University of Chinese Academy of Sciences <a class="header-anchor" href="#university-of-chinese-academy-of-sciences" aria-label="Permalink to &quot;University of Chinese Academy of Sciences&quot;">​</a></h2><h2 id="摘要" tabindex="-1"><strong>摘要：</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要：**&quot;">​</a></h2><p>Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework(GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propaga- tion by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore, an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only &lt; 1% model parameters.</p><h2 id="翻译" tabindex="-1"><strong>翻译：</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译：**&quot;">​</a></h2><p>多模态图像分割技术是计算机视觉领域的关键挑战。这项技术的核心难点在于如何有效融合不同模态（如图像、红外等）的共性特征，同时保留各模态独有的特征模式。当前主流方法主要通过对基于可见光（RGB）预训练模型进行全局微调，以继承基础模型的强大特征提取能力。但这种方法存在明显局限：一方面模型可迁移性较弱，另一方面下游任务标注数据往往匮乏。受大语言模型中提示学习方法取得突破的启发，我们研发了分组提示调优框架（GoPT），通过语义分组机制学习模态专属提示，使冻结的预训练模型能灵活适配多种多模态分割任务。该框架包含两大创新模块：首先是<strong>类感知单模态提示器</strong>，通过聚类同类模态特征，在保留模态内独特空间信息的同时，促进跨模态语义对齐；其次是<strong>对齐引导的跨模态提示器</strong>，通过共享提示参数聚合不同模态的类特征表示，有效捕捉多模态数据的共有统计规律。实验数据显示，GoPT 仅需微调模型不足 1% 的参数，就在多个多模态图像分割基准任务中刷新了最高性能记录，展现出显著优势。</p><h2 id="研究背景" tabindex="-1"><strong>研究背景：</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景：**&quot;">​</a></h2><ol><li><strong>多模态融合的重要性</strong>：语义分割旨在为场景中每个像素分配语义类别，随着传感器技术发展，多模态融合用于分割成为图像解释核心问题。深度学习推动下，深度多模态融合展现出比单模态分割更显著的优势。</li><li><strong>现存方法的挑战</strong>：现有多模态分割方法主要分为基于对齐和基于聚合的融合，但面临诸多挑战。一方面，不同成像机制的模态存在异质差距，基于对齐的融合因信息交换弱，常提供无效的跨模态融合；另一方面，不同模态有效信息不同，基于聚合的融合易忽略模态内传播，导致模态间知识共享和模态内信息处理失衡。</li><li><strong>全微调方法的局限</strong>：多模态方法常采用基于RGB的预训练分割器，全微调虽有效，但效率低、参数存储负担大，且因样本标注有限，无法充分利用预训练模型知识获得通用表示。</li></ol><h2 id="研究现状" tabindex="-1"><strong>研究现状：</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状：**&quot;">​</a></h2><ul><li><strong>多模态图像分割主流方法</strong>：以深度多模态融合为主，旨在利用多数据源增强细粒度细节和像素级语义，主要分为基于对齐和基于聚合的融合方法。前者通过条件损失对齐子网络嵌入，后者运用特定算子组合多模态子网络。</li><li><strong>模型训练方式</strong>：因缺乏大规模多模态训练集，现有方法通常先加载基于RGB的预训练模型参数，再在特定下游任务数据集上微调。</li><li><strong>视觉提示学习应用</strong>：提示调优作为新范式，在自然语言处理中表现出色，近期也开始应用于视觉任务。</li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型：</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型：**&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_14-56-04.png" alt="Snipaste_2025-03-06_14-56-04" loading="lazy"></p><p>本文提出了<strong>分组提示调优框架（Grouping Prompt Tuning Framework，GoPT）</strong>，用于<strong>多模态图像分割</strong>任务，以下是该模型的详细介绍：</p><ol><li><strong>核心思想</strong>：引入显式语义分组机制到提示学习中，通过冻结预训练的基础模型，仅微调少量视觉提示参数，使模型适应各种下游多模态分割任务，以解决现有方法在整合模态间信息和保留各模态特定模式方面的挑战。</li><li><strong>整体架构</strong><ul><li><strong>输入处理</strong>：将RGB图像和辅助模态图像输入到补丁嵌入层，生成对应的RGB标记和辅助模态标记。</li><li><strong>提示生成</strong>：把标记送入分组提示器，生成特定于模态的提示。</li><li><strong>特征融合</strong>：将学习到的提示作为残差添加到原始RGB流中，再输入到基础模型的下一层。</li></ul></li><li><strong>主要组件</strong><ul><li><strong>类感知单模态提示器（Class-Aware Uni-Modal Prompter，CUP）</strong>：通过引入特定于模态的类标记，对辅助模态的视觉概念进行分层渐进分组，平衡模态内和模态间的语义传播，提高空间信息的适应性。</li><li><strong>对齐诱导跨模态提示器（Alignment-Induced Cross-Modal Prompter，ACP）</strong>：根据显式分组的语义相似性，聚合辅助模态的类感知表示，将其他数据源的关键模式整合到RGB流中，生成新的跨模态对齐诱导提示，辅助建模模态公共统计信息。</li></ul></li><li><strong>优化策略</strong>：使用基于RGB的预训练基础模型的参数初始化多模态分割模型，在提示调优过程中，仅更新分组提示器和分割头的梯度值，以少量提示参数促进模型快速收敛，并有效继承预训练基础模型的先验知识。</li><li><strong>实验验证</strong>：在多个下游多模态图像分割任务（RGB - D、RGB - T、RGB - SAR分割）上进行了广泛实验，结果表明GoPT仅训练不到1%的模型参数，就能在各项指标上取得优于现有方法的性能，展现出高效性和优越性。</li></ol><h2 id="实验过程-与sota方法的对比" tabindex="-1"><strong>实验过程（与SOTA方法的对比）：</strong> <a class="header-anchor" href="#实验过程-与sota方法的对比" aria-label="Permalink to &quot;**实验过程（与SOTA方法的对比）：**&quot;">​</a></h2><p>数据集：NYUDv2、SUN RGB-D、MFNet、PST900、WHU-OS</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-02-29.png" alt="Snipaste_2025-03-06_15-02-29" loading="lazy"></p><h2 id="实验过程-消融实验" tabindex="-1"><strong>实验过程（消融实验）：</strong> <a class="header-anchor" href="#实验过程-消融实验" aria-label="Permalink to &quot;**实验过程（消融实验）：**&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-06_15-05-05.png" alt="Snipaste_2025-03-06_15-05-05" loading="lazy"></p><p>通过消融实验，证明了GoPT已经达到了SOTA的标准</p><ul><li>Effectiveness of Prompter Structure （<strong>table 4，row 7 and row 8</strong>）</li><li>Impact of Multi-Modal Information （<strong>table 4</strong>）</li><li>Number of Grouping Prompter <strong>（Figure 6）</strong></li><li>Hard vs. Soft Assignment <strong>（Figure 7）</strong></li></ul><p><strong>结论：</strong></p><p>作者提出了用于<strong>多模态图像分割</strong>的参数高效视觉调优框架GoPT，通过在提示学习中引入显式语义分组，使冻结的预训练基础模型适应各种下游多模态分割任务。具体而言，设计了类感知单模态提示器（CUP），通过对特定模态的类令牌进行分组，平衡了模态内和模态间的语义传播；引入了对齐诱导的跨模态提示器（ACP），聚合类感知表示并辅助建模公共统计信息。大量下游任务实验表明，GoPT在准确性和效率上达到了最佳平衡，仅训练不到1%的模型参数，就在多个下游多模态图像分割任务中取得了SOTA性能，证明了该框架的优越性和泛化性。</p></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/Prompting_Multi-Moda_Segmetation.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-03-08T02:49:02.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/PAT.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>提示词迁移的少样本分割</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>基于Transformer的自适应原型匹配网络</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"DrOP7NeH\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"BzoGmork\",\"column_paper_dgss.md\":\"Btn7YmH7\",\"column_paper_high_quality_segmentation.md\":\"Di55dXsf\",\"column_paper_night-time_semantic_segmentation.md\":\"nnWds54Y\",\"column_paper_pat.md\":\"Cx6baS9V\",\"column_paper_prompting_multi-moda_segmetation.md\":\"BmkSDMDZ\",\"column_paper_sed.md\":\"C0HVy77-\",\"column_paper_segment anything.md\":\"Cg0c77u-\",\"column_paper_visual studio code latex.md\":\"SnkorJ51\",\"column_paper_index.md\":\"CT5DSzbL\",\"column_pytorch_index.md\":\"DzoTzck4\",\"column_deeplearning_index.md\":\"CjxoPop6\",\"column_image_segmentation_20250224-语义分割概述.md\":\"C6LNQ-AV\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"yWx3I2u_\",\"column_image_segmentation_fcn模型讲解.md\":\"cW_w4Wq9\",\"column_image_segmentation_index.md\":\"IS_eT4Hu\",\"column_image_segmentation_图像分割基础.md\":\"0gJc10SB\",\"column_image_segmentation_语义分割基础模型.md\":\"DoXyfXmt\",\"index.md\":\"CbFdN_3u\",\"markdown-examples.md\":\"BJN8Gt8L\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"Pytorch笔记\",\"link\":\"/column/Pytorch/\"},{\"text\":\"深度学习笔记\",\"link\":\"/column/deepLearning/\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>