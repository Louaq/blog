<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/blog/assets/style.aP5EKgV3.css" as="style">
    <link rel="preload stylesheet" href="/blog/vp-icons.css" as="style">
    
    <script type="module" src="/blog/assets/app.g4FfhqZ8.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.dzp_CulL.js">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.Bw1ez5nK.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.md.BX_WuxG-.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9bcec609><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b775d67b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b775d67b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-9bcec609 data-v-389f03b4><div class="VPNavBar" data-v-389f03b4 data-v-f2c85656><div class="wrapper" data-v-f2c85656><div class="container" data-v-f2c85656><div class="title" data-v-f2c85656><div class="VPNavBarTitle has-sidebar" data-v-f2c85656 data-v-02609f48><a class="title" href="/blog/" data-v-02609f48><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b1.png" alt data-v-52246203><!--]--><span data-v-02609f48>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-f2c85656><div class="content-body" data-v-f2c85656><!--[--><!--]--><div class="VPNavBarSearch search" data-v-f2c85656><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-f2c85656 data-v-33ef0f58><span id="main-nav-aria-label" class="visually-hidden" data-v-33ef0f58> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-f2c85656 data-v-b431cf34><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b431cf34 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-f2c85656 data-v-661149a6 data-v-965a88c0><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-965a88c0><span class="vpi-more-horizontal icon" data-v-965a88c0></span></button><div class="menu" data-v-965a88c0><div class="VPMenu" data-v-965a88c0 data-v-979bc427><!----><!--[--><!--[--><!----><div class="group" data-v-661149a6><div class="item appearance" data-v-661149a6><p class="label" data-v-661149a6>深浅模式</p><div class="appearance-action" data-v-661149a6><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-661149a6 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-f2c85656 data-v-057c9ada><span class="container" data-v-057c9ada><span class="top" data-v-057c9ada></span><span class="middle" data-v-057c9ada></span><span class="bottom" data-v-057c9ada></span></span></button></div></div></div></div><div class="divider" data-v-f2c85656><div class="divider-line" data-v-f2c85656></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-9bcec609 data-v-bb57d500><div class="container" data-v-bb57d500><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-bb57d500><span class="vpi-align-left menu-icon" data-v-bb57d500></span><span class="menu-text" data-v-bb57d500>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-bb57d500 data-v-600f5a1a><button data-v-600f5a1a>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-9bcec609 data-v-ff6a08f7><div class="curtain" data-v-ff6a08f7></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-ff6a08f7><span class="visually-hidden" id="sidebar-aria-label" data-v-ff6a08f7> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible collapsed has-active" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于原型的像素级再分类提高弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>C-CAM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Rolling-Unet%20Revitalizing%20MLP%20Ability%20to%20Efficiently%20Extract%20Long-Distance%20Dependencies%20for%20Medical%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：Rolling-Net</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-9bcec609 data-v-b233c28d><div class="VPDoc has-sidebar has-aside" data-v-b233c28d data-v-4e3ca6fa><!--[--><!--]--><div class="container" data-v-4e3ca6fa><div class="aside" data-v-4e3ca6fa><div class="aside-curtain" data-v-4e3ca6fa></div><div class="aside-container" data-v-4e3ca6fa><div class="aside-content" data-v-4e3ca6fa><div class="VPDocAside" data-v-4e3ca6fa data-v-99a4d02b><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-99a4d02b data-v-6eb44d9d><div class="content" data-v-6eb44d9d><div class="outline-marker" data-v-6eb44d9d></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-6eb44d9d>当前大纲</div><ul class="VPDocOutlineItem root" data-v-6eb44d9d data-v-da2b5471><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-99a4d02b></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-4e3ca6fa><div class="content-container" data-v-4e3ca6fa><!--[--><!--]--><main class="main" data-v-4e3ca6fa><div style="position:relative;" class="vp-doc _blog_column_Paper_Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence" data-v-4e3ca6fa><div><h1 id="scaling-up-multi-domain-semantic-segmentation-with-sentence-embeddings" tabindex="-1">Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings <a class="header-anchor" href="#scaling-up-multi-domain-semantic-segmentation-with-sentence-embeddings" aria-label="Permalink to &quot;Scaling Up Multi-domain Semantic Segmentation with Sentence Embeddings&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/30/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>The <strong>state-of-the-art semantic segmentation methods</strong> have achieved impressive performance on predefined close-set individual datasets, but their generalization to zero-shot domains and unseen categories is limited. Labeling a large-scale dataset is challenging and expensive, Training a robust semantic segmentation model on multi-domains has drawn much attention. However, inconsistent taxonomies hinder the naive merging of current publicly available annotations. <strong>To address this, we propose a simple solution to scale up the multi-domain semantic segmentation dataset with less human effort</strong>. We replace each class label with a sentence embedding, which is a vector-valued embedding of a sentence describing the class. This approach enables the merging of multiple datasets from different domains, each with varying class labels and semantics. We merged publicly available noisy and weak annotations with the most finely annotated data, over 2 million images, which enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. Instead of manually tuning a consistent label space, we utilized a vector-valued embedding of short paragraphs to describe the classes. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 (Silberman et al., in: European conference on computer vision, Springer, pp 746–760, 2012) and PASCAL-context (Everingham et al. in Int J Comput Visi 111(1):98–136, 2015) at 60% and 65% mIoU, respectively. Our method can segment unseen labels based on the closeness of language embeddings, showing strong generalization to unseen image domains and labels. Additionally, it enables impressive performance improvements in some adaptation applications, such as depth estimation and instance segmentation. Code is available at <a href="https://github.com/YvanYin/SSIW" target="_blank" rel="noreferrer">https://github.com/YvanYin/SSIW</a>.</p><h2 id="翻译" tabindex="-1">翻译 <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;翻译&quot;">​</a></h2><p>当前最先进的语义分割方法在预设的封闭数据集上表现出色，但其在零样本（zero-shot）领域和未见过类别上的泛化能力仍然有限。由于大规模数据标注既困难又昂贵，开发跨领域通用的鲁棒语义分割模型成为研究热点。然而，现有公开数据集的不同分类标准阻碍了它们的直接融合。为此，我们提出了一种高效扩展多领域语义分割数据集的方法：用文本嵌入（text embedding）替代传统类别标签，这种向量化的语义表示可以融合不同领域、不同标签体系的数据。通过整合包含 200 万张图像的精细标注数据与公开的带噪声弱标注数据，我们训练的模型在 7 个主流测试集上达到了监督学习的顶尖水平，尽管完全没有使用这些测试集的训练图像。与人工统一标签体系不同，我们通过语言模型生成短文本描述来表征类别语义。在标准数据集微调后，模型在 NYUD-V2 (Silberman et al., 2012) 和 PASCAL-context (Everingham et al., 2015) 上分别取得 60% 和 65% 的平均交并比（mIoU），显著超越现有监督方法。该方法通过计算语义相似度实现未见过标签的分割，展现出优异的跨领域泛化能力，**同时在深度估计、实例分割等下游任务中带来显著性能提升。**代码已开源：<a href="https://github.com/YvanYin/SSIW" target="_blank" rel="noreferrer">https://github.com/YvanYin/SSIW</a></p><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><p>语义分割是计算机视觉的基础任务，在自动驾驶、农业机器人和医学等领域应用广泛。当前语义分割方法虽在预定义封闭集数据集上表现出色，但存在明显局限：</p><ol><li><strong>泛化能力不足</strong>：这些方法假设测试集中的所有类别都在训练时出现，然而现实场景并非如此，且模型受限于训练数据集的图像领域，难以泛化到新领域和标签。</li><li><strong>数据集合并难题</strong>：训练多领域语义分割模型是提升模型鲁棒性和泛化能力的自然途径，但直接合并不同领域的数据集会导致标签分类体系冲突，手动统一标签集和重新标注的方法不仅费力，在开放集场景下也存在局限性。</li><li><strong>现有零样本方法的缺陷</strong>：现有解决开放集问题的方法多在小数据集上实验，限制了其在现实场景中的应用潜力。</li></ol><h2 id="研究现状" tabindex="-1">研究现状 <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;研究现状&quot;">​</a></h2><ul><li><strong>语义分割</strong>：深度学习方法在特定高质量数据集上取得显著成果，但泛化能力受限。如FCN开启全卷积方法，后续ResNet、Transformer等推动性能提升。</li><li><strong>零样本语义分割</strong>：分为判别式和生成式方法，前者如Xian等将像素特征转换到语义词嵌入空间，后者如ZS3Net用生成模型生成像素特征。</li><li><strong>跨领域密集预测</strong>：有方法合并分割数据集提升性能和泛化能力，如Ros合并六个驾驶数据集，Lambert提出统一分类法合并多领域数据集。</li><li><strong>零样本学习标签编码</strong>：许多方法为类别标签生成语义嵌入，如Bucher用Word2Vec编码标签，Lseg用语言嵌入监督类别标签。</li></ul><h2 id="提出的模型" tabindex="-1">提出的模型 <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;提出的模型&quot;">​</a></h2><ol><li><ul><li><p><strong>创建语言嵌入</strong>：从Wikipedia收集每个类别的简短描述，使用CLIP - ViT语言模型将这些描述编码为向量值的句子嵌入。这种方式能保留标签之间的语义关系，相比单字标签嵌入，更能反映类别间的语义相似性，有助于零样本标签的分割。</p></li><li><p><strong>混合数据的异构约束</strong>：为解决合并数据集中标注质量不平衡的问题，提出了异构损失函数。</p></li></ul></li></ol><ul><li><strong>高质量标注数据集</strong>：对所有样本施加像素级损失。 <ul><li><strong>粗标注数据集（如OpenImages）</strong>：通过自适应阈值，对高置信度样本施加损失，忽略噪声较大的部分。</li></ul></li><li><strong>弱标注数据集（如Objects365）</strong>：采用蒸馏方法，利用CLIP分类模型的知识，对分割模型进行监督。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-18_09-38-37.png" alt="Snipaste_2025-03-18_09-38-37" loading="lazy"></p><h2 id="实验-compared-with-sota-and-ablation-experiment" tabindex="-1">实验（Compared with SOTA and ablation experiment） <a class="header-anchor" href="#实验-compared-with-sota-and-ablation-experiment" aria-label="Permalink to &quot;实验（Compared with SOTA and ablation experiment）&quot;">​</a></h2><h3 id="_1-数据集与实现细节" tabindex="-1">1. 数据集与实现细节 <a class="header-anchor" href="#_1-数据集与实现细节" aria-label="Permalink to &quot;1. 数据集与实现细节&quot;">​</a></h3><ul><li><strong>训练数据</strong>：合并了7个高质量语义分割数据集（<strong>ADE20K、Mapillary、COCO Panoptic、IDD、BDD100K、Cityscapes、SUNRGBD</strong>），并从OpenImagesV6和Objects365中采样部分数据用于训练。这些数据集涵盖了不同的标注风格和图像领域，总训练图像约<strong>200万张</strong>。</li><li>测试数据 <ul><li><strong>语义分割测试</strong>：在8个零样本数据集（CamVid、KITTI、Pascal VOC、Pascal Context、ScanNet、WildDash1、WildDash2、YoutubeVIS）上进行测试，并在NYUv2和Pascal Context上微调模型以评估性能。</li><li><strong>下游应用测试</strong>：在零样本数据集上创建伪语义标签，用于提升实例分割和单目深度估计的性能。实例分割在COCO数据集上进行测试，深度估计在NYUDv2、KITTI、DIODE、ScanNet和Sintel等数据集上进行评估。</li></ul></li><li>评估指标 <ul><li><strong>语义分割</strong>：使用平均交并比（mIoU）进行评估。</li><li><strong>实例分割</strong>：使用平均精度（AP）进行评估。</li><li><strong>深度估计</strong>：采用绝对相对误差（AbsRel）和满足特定条件的像素百分比（δτ）进行评估。</li></ul></li><li><strong>多尺度评估</strong>：在评估语义分割性能时，将测试图像调整为多个尺度（0.5 - 1.75，步长为0.25）输入模型，然后平均得分作为最终预测结果。</li><li><strong>实现细节</strong>：使用HRNet - W48和Segformer两种网络架构进行实验。训练时，采用不同的优化器和学习率衰减策略，并对图像进行数据增强处理。推理时，将图像短边调整为三种分辨率（480/720/1080），并根据需要采用多尺度或单尺度测试。</li></ul><h3 id="_2-实验内容" tabindex="-1">2. 实验内容 <a class="header-anchor" href="#_2-实验内容" aria-label="Permalink to &quot;2. 实验内容&quot;">​</a></h3><ul><li><p><strong>语义分割评估</strong></p><p>在15个数据集上进行评估，以验证模型的鲁棒性和有效性。</p><ul><li><strong>鲁棒性评估</strong>：与现有最先进的方法在6个零样本数据集上进行比较，结果表明该方法在CamViD、ScanNet和WildDash1上达到了最先进的性能，并且在混合数据集上训练的模型比在单个数据集上训练的HRNet更具鲁棒性。</li><li><strong>Wilddash2评估</strong>：在Wilddash2基准测试中，该方法取得了最先进的性能。</li><li><strong>异质损失聚合效果评估</strong>：提出异质损失来监督合并的数据集，实验结果表明，在聚合OpenImages和Objects365时，使用异质损失可以持续提高所有零样本数据集的性能。</li><li><strong>未见标签泛化能力评估</strong>：通过在YoutubeVIS数据集上采样具有5个零样本标签的约2100张图像进行实验，结果表明该方法比Mseg和JoEm更具鲁棒性，并且使用句子编码可以获得更好的性能。</li><li><strong>小数据集微调评估</strong>：在NYUv2和Pascal Context上微调模型，与其他预训练权重相比，该方法的预训练权重可以显著提升性能，超过现有最先进的方法。</li><li><strong>蒸馏效果评估</strong>：使用CLIP对模型进行知识蒸馏，实验结果表明，即使只在裁剪的边界框区域进行粗略的知识蒸馏，性能仍然可以得到显著提升。</li><li><strong>语言嵌入合并训练数据效果评估</strong>：比较了两种合并训练数据标签的方法，结果表明，使用句子嵌入来表示标签可以更好地解决标签冲突问题，从而获得更好的性能。</li><li><strong>与Lseg比较</strong>：与Lseg方法在不同粒度的标签集上进行比较，结果表明，在细化标签集上，该方法的分割结果更优。</li></ul></li><li><p><strong>下游应用提升评估</strong></p><ul><li><strong>单目深度估计</strong>：在多个零样本深度数据集上创建伪语义标签，将其转换为像素级语言嵌入并输入到深度预测网络中。实验结果表明，与基线方法LeReS相比，添加创建的嵌入可以持续提高所有数据集的性能。</li><li><strong>实例分割</strong>：在采样的Objects365上创建伪实例掩码，用于训练CondInst模型。实验结果表明，使用仅25%的COCO数据，该方法可以达到与基线方法相当的性能，并且在使用完整的COCO数据进行微调时，性能比基线方法高约4% AP。</li></ul></li></ul><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>作者提出一种语义分割方法，能在多个零样本跨域数据集上取得良好性能。具体结论如下：</p><ol><li><strong>数据融合</strong>：从维基百科收集标签简短描述，编码为向量嵌入替代标签，可轻松合并多数据集，得到强大鲁棒的分割模型。</li><li><strong>损失函数</strong>：提出异质损失，利用噪声和弱标注数据集。</li><li><strong>性能表现</strong>：在7个跨域数据集上，性能优于或与当前最先进方法相当，模型能分割零样本标签。</li><li><strong>下游应用</strong>：该模型显著提升单目深度估计和实例分割等下游应用的性能。不过，模型性能可能受语言模型表示限制，对训练语言空间外的类别泛化能力不足，但增加数据类别和改进语言模型有望解决。</li></ol></div></div></main><footer class="VPDocFooter" data-v-4e3ca6fa data-v-92f5315a><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-92f5315a><div class="edit-link" data-v-92f5315a><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.md" target="_blank" rel="noreferrer" data-v-92f5315a><!--[--><span class="vpi-square-pen edit-link-icon" data-v-92f5315a></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-92f5315a><p class="VPLastUpdated" data-v-92f5315a data-v-8bddb0e8>最后更新于: <time datetime="2025-04-30T15:17:56.000Z" data-v-8bddb0e8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-92f5315a><span class="visually-hidden" id="doc-footer-aria-label" data-v-92f5315a>Pager</span><div class="pager" data-v-92f5315a><a class="VPLink link pager-link prev" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>上一页</span><span class="title" data-v-92f5315a>医学图像分割：基于解耦特征查询</span><!--]--></a></div><div class="pager" data-v-92f5315a><a class="VPLink link pager-link next" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>下一页</span><span class="title" data-v-92f5315a>基于涂鸦的弱监督语义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-9bcec609 data-v-93274d57><div class="container" data-v-93274d57><p class="message" data-v-93274d57>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-93274d57>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"z19mZNku\",\"column_image_segmentation_20250224-语义分割概述.md\":\"iuS4dJhu\",\"column_image_segmentation_20250312-pytorch教程.md\":\"BaWbulcg\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"CVQDP8QM\",\"column_image_segmentation_fcn模型讲解.md\":\"CE8bEZdZ\",\"column_image_segmentation_index.md\":\"lnBprgqY\",\"column_image_segmentation_segment algrothm.md\":\"Dr-YMOmv\",\"column_image_segmentation_图像分割基础.md\":\"BmGD68I9\",\"column_image_segmentation_语义分割基础模型.md\":\"gixyT3Vv\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"54AzNJk9\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"CSSeQOKg\",\"column_paper_c-cam causal cam for weakly supervised semantic segmentation on medical image.md\":\"DEOKIhFN\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"DEsrsnA7\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"Bo14D9-A\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"DSfv6MJN\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"CgRAYt2-\",\"column_paper_dgss.md\":\"DSYavnQF\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"CZoquaSX\",\"column_paper_high_quality_segmentation.md\":\"CC8DfBn1\",\"column_paper_index.md\":\"Synlh4NI\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"BeUtBo0E\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"DVJSSNdD\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"BJ_F3svD\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"DTxxhFdx\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"3J1B893t\",\"column_paper_night-time_semantic_segmentation.md\":\"BRrk_XVi\",\"column_paper_pat.md\":\"BptPSP_H\",\"column_paper_pixel-wise reclassification with prototypes for enhancing weakly supervised semantic segmentation.md\":\"DdsmXG9W\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"B5sgGZt8\",\"column_paper_prompting_multi-moda_segmetation.md\":\"DUQOziMy\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"CXgHjlnn\",\"column_paper_rolling-unet revitalizing mlp ability to efficiently extract long-distance dependencies for medical image segmentation.md\":\"CJUaFy27\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"BX_WuxG-\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"Dj1NeIuT\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"DZ8y7O_I\",\"column_paper_sed.md\":\"m07QoKIc\",\"column_paper_segment anything.md\":\"B92nF2Yh\",\"column_paper_self-supervised_vit.md\":\"CH5QKIeK\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"BeaNaAZm\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"T6SYAXM0\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"CCyCHAAw\",\"column_paper_visual studio code latex.md\":\"BCPb2vsp\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"DOyuAc6K\",\"column_puruse_index.md\":\"CFTXr3a1\",\"column_puruse_template.md\":\"BXMZIw1p\",\"index.md\":\"BfZhzBoj\",\"markdown-examples.md\":\"Crq_jGJN\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b1.png\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于原型的像素级再分类提高弱监督语义分割\",\"link\":\"/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation\"},{\"text\":\"C-CAM\",\"link\":\"/column/Paper/C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image\"},{\"text\":\"医学图像分割：Rolling-Net\",\"link\":\"/column/Paper/Rolling-Unet Revitalizing MLP Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>