<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS） | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.CobjoJX1.css" as="style">
    
    <script type="module" src="/blog/assets/app.BFVcPL9O.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.D6Cah3zj.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.DeP9SCVM.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_DGSS.md.CYN0IrfE.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Pytorch/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>Pytorch笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/deepLearning/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>深度学习笔记</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_DGSS" data-v-35102dec><div><h1 id="stronger-fewer-superior-harnessing-vision-foundation-models-for-domain-generalized-semantic-segmentation-dgss" tabindex="-1">Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS） <a class="header-anchor" href="#stronger-fewer-superior-harnessing-vision-foundation-models-for-domain-generalized-semantic-segmentation-dgss" aria-label="Permalink to &quot;Stronger, Fewer, &amp; Superior Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation（DGSS）&quot;">​</a></h1><h2 id="中国科学技术大学-上海人工智能实验室" tabindex="-1"><strong>中国科学技术大学，上海人工智能实验室</strong> <a class="header-anchor" href="#中国科学技术大学-上海人工智能实验室" aria-label="Permalink to &quot;**中国科学技术大学，上海人工智能实验室**&quot;">​</a></h2><p><a href="https://github.com/w1oves/Rein.git" target="_blank" rel="noreferrer">https://github.com/w1oves/Rein.git</a></p><blockquote><p>摘要：In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generaliz- ability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at <a href="https://github.com/w1oves/Rein.git" target="_blank" rel="noreferrer">https://github.com/w1oves/Rein.git</a>.</p></blockquote><blockquote><p>翻译：在本文中，我们首先在领域泛化语义分割（DGSS）任务中，评估并应用了多种视觉基础模型（VFM）。我们提出的动机是：“通过利用更强大的预训练模型和更少的可训练参数，获得更好的泛化能力”。基于此，我们提出了一种高效的微调方法——“Rein”，该方法能够以参数高效的方式利用VFM来解决DGSS任务。Rein方法依赖于一组可训练的标记，每个标记与特定实例对应，能够精确地细化并将特征图从每一层传递到骨干网络的下一层。这样，Rein能够在单张图像中为不同的类别生成多样化的细化结果。通过减少可训练的参数，Rein在微调VFM时，效果出乎意料地优于完全参数微调。通过广泛的实验验证，Rein显著超越了现有的最先进方法。值得一提的是，仅在冻结的骨干网络中增加1%的可训练参数，Rein便在Cityscapes数据集上达到了78.4%的mIoU，而且无需使用任何真实的城市场景数据集。代码已发布，您可以通过<a href="https://github.com/w1oves/Rein.git%E8%AE%BF%E9%97%AE%E3%80%82" target="_blank" rel="noreferrer">https://github.com/w1oves/Rein.git访问。</a></p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-37.png" alt="Snipaste_2025-02-26_21-13-37" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-02-26_21-13-59.png" alt="Snipaste_2025-02-26_21-13-59" loading="lazy"></p><p>​ <strong>模型结构图</strong></p><p><strong>本文的研究背景：</strong></p><ul><li><strong>传统DGSS方法的局限</strong>：以往DGSS方法着重提升模型在多未见领域的预测准确性，但多采用VGGNet、MobileNetV2和ResNet等经典骨干网络，且依赖复杂数据增强和领域不变特征提取策略，对更强的VFMs在DGSS中的效能探索不足。</li><li><strong>VFMs的潜力与挑战</strong>：近年来，CLIP、MAE、SAM等大规模VFMs显著提升了计算机视觉任务的性能，其在不同未知场景下展现出强大泛化能力。然而，将VFMs用于DGSS任务存在挑战，常用数据集规模远小于ImageNet，对VFMs大量可训练参数进行微调会导致泛化能力受限，且现有的参数高效微调策略大多不适用于DGSS。-</li><li><strong>研究动机</strong>：基于利用更强预训练模型和更少可训练参数实现更优泛化能力的动机，作者评估并利用VFMs进行DGSS研究，提出“Rein”微调方法，以高效利用VFMs解决DGSS问题。</li></ul><p><strong>研究现状：</strong></p><ul><li><strong>领域广义语义分割（DGSS）</strong>：传统方法聚焦提升模型跨多未见领域的预测准确性，采用复杂数据增强和领域不变特征提取策略，多使用VGGNet、MobileNetV2等旧骨干网络。</li><li><strong>视觉基础模型（VFMs）</strong>：如CLIP、MAE、SAM等在计算机视觉挑战中表现出色，具有显著的跨场景泛化能力，但在DGSS任务中的表现缺乏专门研究。</li><li><strong>参数高效微调（PEFT）</strong>：在自然语言处理领域取得成功，部分方法开始应用于计算机视觉，但大多不是为DGSS设计，难以对单张图像中不同实例的特征进行细化。</li></ul><p><strong>研究思路：</strong></p><p>本文聚焦于在领域泛化语义分割（DGSS）中利用视觉基础模型（VFMs），研究思路清晰，具体如下：</p><ol><li><strong>提出问题</strong>：先前DGSS方法多采用传统骨干网络，而大规模VFMs虽在计算机视觉挑战中表现出色，但在DGSS中的性能及利用方式尚不明确。因此，作者提出评估VFMs在DGSS中的性能以及如何有效利用VFMs的问题。</li><li><strong>构建框架</strong>：以利用更强预训练模型和更少可训练参数实现更优泛化能力为动机，作者引入<strong>Rein</strong>微调方法，在骨干网络层间嵌入该机制，以有效利用VFMs的强大能力。</li><li><strong>选择方法</strong>：选择CLIP、MAE、SAM、EVA02和DINOv2等五种不同训练策略和数据集的VFMs进行评估。设置“Full”和“Freeze”两个基本基线，并提出“Rein”方法。采用AdamW优化器，设置特定学习率、迭代次数、批量大小等进行训练。</li><li><strong>分析数据</strong>：在多个数据集和三种泛化设置下进行实验，对比Rein与现有DGSS和参数高效微调（PEFT）方法的性能。通过消融实验分析Rein各组件的有效性、令牌长度和秩对模型性能的影响，以及训练速度、GPU内存使用和模型存储要求。</li><li><strong>得出结论</strong>：实验表明，冻结的VFMs性能优于先前DGSS方法，Rein以更少可训练参数显著增强VFMs的泛化能力，大幅超越现有方法。证明了VFMs在DGSS领域的巨大潜力以及Rein方法的有效性。</li></ol><p><strong>本文的创新点：</strong></p><ol><li><strong>评估并利用视觉基础模型（VFMs）</strong>：首次在**领域泛化语义分割（DGSS）**中评估多种VFMs，证实其强大泛化能力，为该领域建立重要基准。</li><li><strong>提出“Rein”微调方法</strong>：通过可学习令牌对特征图进行实例级细化，以较少可训练参数有效利用VFMs，显著提升泛化性，超越现有方法。</li><li><strong>设计优化策略</strong>：采用层共享MLP权重和低秩token序列，减少参数冗余，提高训练效率。</li></ol><blockquote><p>写作启发：<strong>领域泛化语义分割（DGSS）</strong>、<strong>视觉基础模型（VFMs）</strong>、<strong>参数高效微调（PEFT</strong>）</p></blockquote></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/DGSS.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-03-04T05:41:38.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/Segment%20Anything.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>Segment Anything</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/SED.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>基于分层编码器的开放词汇语义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"DiIpOnwg\",\"column_paper_dgss.md\":\"CYN0IrfE\",\"column_paper_high_quality_segmentation.md\":\"BHs2P4f_\",\"column_paper_night-time_semantic_segmentation.md\":\"BC4WDpEO\",\"column_paper_pat.md\":\"CEiDtdGY\",\"column_paper_sed.md\":\"BSCsLnGi\",\"column_paper_segment anything.md\":\"CNa276y5\",\"column_paper_visual studio code latex.md\":\"Ho1VqVCQ\",\"column_paper_index.md\":\"DNKtcvB5\",\"column_pytorch_index.md\":\"CtlPqgRe\",\"column_deeplearning_index.md\":\"BJkl6ZAN\",\"column_image_segmentation_20250224-语义分割概述.md\":\"BSrftiMJ\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"B3qCTyJM\",\"column_image_segmentation_fcn模型讲解.md\":\"j6mJ5HxQ\",\"column_image_segmentation_index.md\":\"aH6LqZIk\",\"column_image_segmentation_图像分割基础.md\":\"C-tO07ZV\",\"column_image_segmentation_语义分割基础模型.md\":\"BP97e6Yn\",\"index.md\":\"DBQBOojr\",\"markdown-examples.md\":\"CuHijpFw\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"Pytorch笔记\",\"link\":\"/column/Pytorch/\"},{\"text\":\"深度学习笔记\",\"link\":\"/column/deepLearning/\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>