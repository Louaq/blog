<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>DSMF-Net: Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/blog/assets/style.aP5EKgV3.css" as="style">
    <link rel="preload stylesheet" href="/blog/vp-icons.css" as="style">
    
    <script type="module" src="/blog/assets/app.BwL1vjh2.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.C5e9yPYk.js">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.Bw1ez5nK.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation.md.Kehsl8wn.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9bcec609><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b775d67b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b775d67b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-9bcec609 data-v-389f03b4><div class="VPNavBar" data-v-389f03b4 data-v-f2c85656><div class="wrapper" data-v-f2c85656><div class="container" data-v-f2c85656><div class="title" data-v-f2c85656><div class="VPNavBarTitle has-sidebar" data-v-f2c85656 data-v-02609f48><a class="title" href="/blog/" data-v-02609f48><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-52246203><!--]--><span data-v-02609f48>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-f2c85656><div class="content-body" data-v-f2c85656><!--[--><!--]--><div class="VPNavBarSearch search" data-v-f2c85656><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-f2c85656 data-v-33ef0f58><span id="main-nav-aria-label" class="visually-hidden" data-v-33ef0f58> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-f2c85656 data-v-b431cf34><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b431cf34 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-f2c85656 data-v-661149a6 data-v-965a88c0><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-965a88c0><span class="vpi-more-horizontal icon" data-v-965a88c0></span></button><div class="menu" data-v-965a88c0><div class="VPMenu" data-v-965a88c0 data-v-979bc427><!----><!--[--><!--[--><!----><div class="group" data-v-661149a6><div class="item appearance" data-v-661149a6><p class="label" data-v-661149a6>深浅模式</p><div class="appearance-action" data-v-661149a6><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-661149a6 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-f2c85656 data-v-057c9ada><span class="container" data-v-057c9ada><span class="top" data-v-057c9ada></span><span class="middle" data-v-057c9ada></span><span class="bottom" data-v-057c9ada></span></span></button></div></div></div></div><div class="divider" data-v-f2c85656><div class="divider-line" data-v-f2c85656></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-9bcec609 data-v-bb57d500><div class="container" data-v-bb57d500><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-bb57d500><span class="vpi-align-left menu-icon" data-v-bb57d500></span><span class="menu-text" data-v-bb57d500>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-bb57d500 data-v-600f5a1a><button data-v-600f5a1a>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-9bcec609 data-v-ff6a08f7><div class="curtain" data-v-ff6a08f7></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-ff6a08f7><span class="visually-hidden" id="sidebar-aria-label" data-v-ff6a08f7> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible collapsed" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible has-active" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于原型的像素级再分类提高弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>C-CAM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Rolling-Unet%20Revitalizing%20MLP%20Ability%20to%20Efficiently%20Extract%20Long-Distance%20Dependencies%20for%20Medical%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：Rolling-Net</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-9bcec609 data-v-b233c28d><div class="VPDoc has-sidebar has-aside" data-v-b233c28d data-v-4e3ca6fa><!--[--><!--]--><div class="container" data-v-4e3ca6fa><div class="aside" data-v-4e3ca6fa><div class="aside-curtain" data-v-4e3ca6fa></div><div class="aside-container" data-v-4e3ca6fa><div class="aside-content" data-v-4e3ca6fa><div class="VPDocAside" data-v-4e3ca6fa data-v-99a4d02b><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-99a4d02b data-v-6eb44d9d><div class="content" data-v-6eb44d9d><div class="outline-marker" data-v-6eb44d9d></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-6eb44d9d>当前大纲</div><ul class="VPDocOutlineItem root" data-v-6eb44d9d data-v-da2b5471><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-99a4d02b></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-4e3ca6fa><div class="content-container" data-v-4e3ca6fa><!--[--><!--]--><main class="main" data-v-4e3ca6fa><div style="position:relative;" class="vp-doc _blog_column_Paper_DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation" data-v-4e3ca6fa><div><h1 id="dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation" tabindex="-1">DSMF-Net: Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation <a class="header-anchor" href="#dsmf-net-dual-semantic-metric-learning-fusion-network-for-few-shot-aerial-image-semantic-segmentation" aria-label="Permalink to &quot;DSMF-Net: Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/30/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><blockquote><p><strong>Chinese Academy of Sciences、University of Chinese Academy of Sciences</strong></p></blockquote><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p><strong>Semantic segmentation</strong> of aerial images is crucial yet resource-intensive. Inspired by human ability to learn rapidly, few-shot semantic segmentation offers a promising solution by utilizing limited labeled data for efficient model training and generalization. However, the intrinsic complexities of aerial images, compounded by scarce samples, often result in inadequate feature representation and semantic ambiguity, detracting from themodel’s performance. In this article, we propose to tackle these challenging problems via dual semantic metric learning and multisemantic features fusion and introduce a novel few-shot segmentation Network (DSMF-Net). On the one hand, we consider the inherent semantic gap between the feature of graph and grid structures and metric learning of few-shot segmentation. To exploit multiscale global semantic context, we construct scale-aware graph prototypes from different stages of the feature layers based on graph convolutional networks (GCNs), while also incorporating prior-guided metric learning to further enhance context at the high-level convolution features. On the other hand, we design a pyramid-based fusion and condensa- tion mechanism to adaptively merge and couple the multisemantic information from support and query images. The indication and fusion of different semantic features can effectively emphasize the representation and coupling abilities of the network. We have conducted extensive experiments over the challenging iSAID-5i andDLRSD benchmarks. The experiments have demonstrated our network’s effectiveness and efficiency, yielding on-par performance with the state-of-the-art methods.</p><h2 id="翻译" tabindex="-1">翻译 <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;翻译&quot;">​</a></h2><p><strong>航空图像的语义分割</strong>是一个非常重要的问题。受人类快速学习能力的启发，少射语义分割通过利用有限的标记数据进行有效的模型训练和泛化，提供了一种很有前途的解决方案。然而，航空图像固有的复杂性，加上稀缺的样本，往往导致特征表示不足和语义模糊，从而降低了模型的性能。在本文中，我们提出通过双语义度量学习和多语义特征融合来解决这些具有挑战性的问题，并引入了一种新的少量样本学习分割网络(DSMFNet)。一方面，我们考虑了图和网格结构特征之间固有的语义差距和少量样本学习分割的度量学习。为了利用多尺度全局语义上下文，我们基于图卷积网络(GCNs)从特征层的不同阶段构建了尺度感知的图原型，同时还结合了先验引导的度量学习来进一步增强高级卷积特征的上下文。另一方面，我们设计了一种基于金字塔的融合与凝聚机制来自适应地融合和耦合来自支持和查询图像的多语义信息。不同语义特征的表示和融合可以有效地强调网络的表示和耦合能力。我们对具有挑战性的iSAID-5i和dlrsd基准进行了广泛的实验。实验证明了我们的网络的有效性和效率，产生了与最先进的方法相当的性能。</p><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><p>本文聚焦于航空影像少样本语义分割问题，研究背景如下：</p><ul><li><strong>语义分割需求与挑战</strong>：语义分割是计算机视觉基础技术，在城市管理、环境监测等领域应用广泛。但传统航空影像语义分割模型训练需大量标注数据，获取耗时耗力。</li><li><strong>少样本学习的潜力</strong>：少样本学习受人类学习能力启发，利用少量标注数据进行模型训练和泛化，为解决数据获取难题提供了思路。少样本语义分割作为其延伸，通过利用相关任务或领域的先验知识进行分割任务。</li><li><strong>航空影像少样本分割的困难</strong>：航空影像由机载或卫星传感器捕获，具有空间分辨率变化大、覆盖范围广的特点。不同语义对象外观差异大，同一类别对象在尺度和结构上也存在显著差异，导致特征表示不足和语义模糊，增加了少样本语义分割的难度。</li><li><strong>现有方法的局限性</strong>：现有方法虽有一定进展，但传统卷积神经网络在捕捉航空影像的全局和可变结构关系方面效率较低，特征提取存在特征耦合和细节保留不足的问题，导致特征歧义。 基于以上背景，作者提出DSMF - Net网络，以解决航空影像少样本语义分割中的特征建模不佳和语义模糊问题。</li></ul><h2 id="研究现状" tabindex="-1">研究现状 <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;研究现状&quot;">​</a></h2><ul><li><strong>语义分割</strong>：FCN、U - Net等方法推动了语义分割发展，后续如DeepLabv3 +、PSPNet等通过引入新机制提升性能，但监督分割方法标注要求高。</li><li><strong>少样本语义分割</strong>：出现半监督、弱监督、无监督学习等方法，近期少样本学习受关注，如OSLSM、PL、PANet等方法不断涌现，部分还探索了知识迁移问题。</li><li><strong>航空影像少样本语义分割</strong>：不同方法被提出，如Wang等人的原型队列学习法、Yao等人的多原型框架、DMML - Net的深度特征金字塔比较网络等。</li></ul><h2 id="提出的模型" tabindex="-1">提出的模型 <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;提出的模型&quot;">​</a></h2><p>文章提出了一种名为DSMF - Net（Dual Semantic Metric Learning Fusion Network）的新型少样本分割网络，用于解决航空影像少样本语义分割中特征建模不佳和语义模糊的问题。</p><ol><li><strong>图原型度量学习（Graph Prototypes Metric Learning）</strong><ul><li><strong>图卷积（Graph Convolution）</strong>：图卷积可有效表示图像中不同像素或区域之间的关系，捕捉长距离依赖和全局上下文信息，有助于减少特征耦合，使模型学习和区分图像内不同的结构和语义关系。</li><li><strong>尺度感知图原型（Scale - Aware Graph Prototypes）</strong>：静态网格卷积特征难以捕捉航空影像中的复杂关系，因此引入尺度感知图原型，以图投影的方式利用多尺度全局语义上下文。通过在预训练的ResNet - 50的三个中间阶段输出特征，利用GloRe单元进行全局推理，结合支持掩码加权和全局平均池化生成原型。</li><li><strong>原型度量学习（Prototype Metric Learning）</strong>：利用查询特征与原型之间的余弦距离进行度量学习，对查询特征应用相同的图投影操作，通过最小 - 最大归一化得到图原型概率图。</li></ul></li><li><strong>先验引导度量学习（Prior Guided Metric Learning）</strong>：图卷积特征金字塔为图结构数据的特征嵌入和全局信息捕捉提供了基础，但高级卷积特征中的语义信息也不能忽视。因此引入先验引导度量学习，生成高级查询和支持卷积特征之间的相似性度量，通过添加二进制支持掩码减轻背景影响，计算余弦距离并进行最小 - 最大归一化得到先验引导概率图。</li><li><strong>语义特征融合模块（SFF）</strong>：构建SFF模块解决特征耦合问题，增强模型对变化的鲁棒性。采用金字塔结构对不同尺度的特征进行上采样和下采样，通过1×1卷积合并特征生成中间尺度特征，最后插值和拼接生成新的融合特征，促进不同尺度特征的有效交互和集成。</li><li><strong>损失函数（Loss Function）</strong>：采用交叉熵损失作为主要损失函数<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="5.491ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 2427.1 840.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，并引入中间监督<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="4.405ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1947 840.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(529,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1101,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>u</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，总损失L是<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="5.491ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 2427.1 840.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="4.405ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1947 840.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(714,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(529,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1101,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>u</mi><mi>x</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>的加权和，其中λ设为1.0。</li></ol><h2 id="实验-compared-with-sota" tabindex="-1">实验（Compared with SOTA） <a class="header-anchor" href="#实验-compared-with-sota" aria-label="Permalink to &quot;实验（Compared with SOTA）&quot;">​</a></h2><p>数据集：iSAID-5i、DLRSD</p><ul><li><strong>SAID - 5i数据集</strong>：将提出的模型与其他流行方法进行比较，结果表明该模型明显优于当代少样本分割模型，随着骨干网络的增强，性能提升。在不同设置下，模型在各折数据上均有显著的mIoU提升，且通过配对t检验验证了模型性能提升的显著性。</li><li><strong>DLRSD数据集</strong>：在更具挑战性的DLRSD数据集上进行实验，结果显示该模型的mIoU得分高于其他方法，尤其在处理复杂场景和具有细微视觉差异的对象时表现出色。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-15.png" alt="Snipaste_2025-04-02_09-35-15" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-35-38.png" alt="Snipaste_2025-04-02_09-35-38" loading="lazy"></p><h2 id="实验-ablation-experiments" tabindex="-1">实验（Ablation Experiments）🥇 <a class="header-anchor" href="#实验-ablation-experiments" aria-label="Permalink to &quot;实验（Ablation Experiments）:1st_place_medal:&quot;">​</a></h2><ul><li>消融实验：使用ResNet - 50骨干网络在1 - shot设置下进行广泛的消融实验，分析提出模块的有效性和不同设置的影响。 <ul><li><strong>GPML模块</strong>：通过实验表明，基于图结构的GPML模块相比基于卷积结构的原型学习，能进一步提高模型性能，更好地捕捉航空图像中对象之间的复杂关系。</li><li><strong>SFF模块</strong>：实验验证了SFF模块在不同尺度下的特征融合策略的有效性，特别是引入图结构和合并操作后，模型的平均性能显著提升。</li><li><strong>效率评估</strong>：通过比较不同网络的参数数量和每秒帧数（FPS），证明了提出的模型在保持较低参数数量的同时，实现了较高的FPS，在少样本航空图像分割中具有高效性。</li></ul></li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-29.png" alt="Snipaste_2025-04-02_09-36-29" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-02_09-36-39.png" alt="Snipaste_2025-04-02_09-36-39" loading="lazy"></p><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>作者聚焦于<strong>航拍影像少样本语义分割</strong>中特征建模不佳和语义模糊的问题，提出了名为<strong>DSMF-Net</strong>的少样本分割网络。该网络采用<strong>双度量学习和多语义信息融合</strong>，增强了模型的解析和表达能力。具体而言，通过尺度感知图原型以图投影方式挖掘多尺度全局语义上下文，集成先验引导度量学习增强高层语义上下文，设计基于金字塔的融合模块更好地提取和浓缩语义特征。在<strong>iSAID - 5i和DLRSD</strong>两个具有挑战性的基准数据集上的实验表明，该方法性能优越，能有效处理有限样本下的密集预测任务。不过，作者也指出，未来需在更大、更多样化的数据集上进一步评估，以全面了解其能力和局限性，后续研究将关注模型对不同航空影像类型的适应性、在大规模数据集上的泛化能力和计算复杂度。</p></div></div></main><footer class="VPDocFooter" data-v-4e3ca6fa data-v-92f5315a><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-92f5315a><div class="edit-link" data-v-92f5315a><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation.md" target="_blank" rel="noreferrer" data-v-92f5315a><!--[--><span class="vpi-square-pen edit-link-icon" data-v-92f5315a></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-92f5315a><p class="VPLastUpdated" data-v-92f5315a data-v-8bddb0e8>最后更新于: <time datetime="2025-04-30T14:43:39.000Z" data-v-8bddb0e8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-92f5315a><span class="visually-hidden" id="doc-footer-aria-label" data-v-92f5315a>Pager</span><div class="pager" data-v-92f5315a><a class="VPLink link pager-link prev" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>上一页</span><span class="title" data-v-92f5315a>基于全局和局部注意力蒸馏的高效语义分割</span><!--]--></a></div><div class="pager" data-v-92f5315a><a class="VPLink link pager-link next" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>下一页</span><span class="title" data-v-92f5315a>基于剪枝的领域泛化语义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-9bcec609 data-v-93274d57><div class="container" data-v-93274d57><p class="message" data-v-93274d57>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-93274d57>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"CM58dPIy\",\"column_image_segmentation_20250224-语义分割概述.md\":\"DJ9aYSe4\",\"column_image_segmentation_20250312-pytorch教程.md\":\"D5CxUCg-\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"D5oc3w1y\",\"column_image_segmentation_fcn模型讲解.md\":\"BZLPVhhI\",\"column_image_segmentation_index.md\":\"Cnnp9LXF\",\"column_image_segmentation_segment algrothm.md\":\"DaRLgJ0n\",\"column_image_segmentation_图像分割基础.md\":\"BmgyYd22\",\"column_image_segmentation_语义分割基础模型.md\":\"DUH2PxjJ\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"DcrK4Xvz\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"DvX7x9-E\",\"column_paper_c-cam causal cam for weakly supervised semantic segmentation on medical image.md\":\"D1Kx1tOx\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"B893K9ex\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"IrpndHQH\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"ZdGVBa6V\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"a-y9d-aH\",\"column_paper_dgss.md\":\"DYlWMCsg\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"Kehsl8wn\",\"column_paper_high_quality_segmentation.md\":\"BczNX3CC\",\"column_paper_index.md\":\"CePKwe5R\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"DtChIPkY\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"FSl71yox\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"KVKYo0G7\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"CmDI8avq\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"B4Vd7yMW\",\"column_paper_night-time_semantic_segmentation.md\":\"B2lCfwcj\",\"column_paper_pat.md\":\"pcvVhIb3\",\"column_paper_pixel-wise reclassification with prototypes for enhancing weakly supervised semantic segmentation.md\":\"CczNRG9p\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"69bdYjZh\",\"column_paper_prompting_multi-moda_segmetation.md\":\"B7LyZ28Z\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"bwcHQVPD\",\"column_paper_rolling-unet revitalizing mlp ability to efficiently extract long-distance dependencies for medical image segmentation.md\":\"C0yqKavn\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"DjJdi5nt\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"CetPxX_K\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"SEmaCmIS\",\"column_paper_sed.md\":\"AGJR-cvQ\",\"column_paper_segment anything.md\":\"JUOxZkd-\",\"column_paper_self-supervised_vit.md\":\"CuqUnHnx\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"0ZmwNJcO\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"itszMl-0\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"BrQcgEAg\",\"column_paper_visual studio code latex.md\":\"Crcgltrd\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"DXzQMqhZ\",\"column_puruse_index.md\":\"CgK5aykD\",\"column_puruse_template.md\":\"BDhSDpKE\",\"index.md\":\"Cv44Amku\",\"markdown-examples.md\":\"CfpMxZtF\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于原型的像素级再分类提高弱监督语义分割\",\"link\":\"/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation\"},{\"text\":\"C-CAM\",\"link\":\"/column/Paper/C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image\"},{\"text\":\"医学图像分割：Rolling-Net\",\"link\":\"/column/Paper/Rolling-Unet Revitalizing MLP Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>