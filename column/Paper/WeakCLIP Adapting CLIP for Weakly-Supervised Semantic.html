<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>WeakCLIP: Adapting CLIP for Weakly-Supervised Semantic Segmentation | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/blog/assets/style.aP5EKgV3.css" as="style">
    <link rel="preload stylesheet" href="/blog/vp-icons.css" as="style">
    
    <script type="module" src="/blog/assets/app.BwL1vjh2.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.C5e9yPYk.js">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.Bw1ez5nK.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_WeakCLIP Adapting CLIP for Weakly-Supervised Semantic.md.DXzQMqhZ.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9bcec609><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b775d67b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b775d67b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-9bcec609 data-v-389f03b4><div class="VPNavBar" data-v-389f03b4 data-v-f2c85656><div class="wrapper" data-v-f2c85656><div class="container" data-v-f2c85656><div class="title" data-v-f2c85656><div class="VPNavBarTitle has-sidebar" data-v-f2c85656 data-v-02609f48><a class="title" href="/blog/" data-v-02609f48><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-52246203><!--]--><span data-v-02609f48>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-f2c85656><div class="content-body" data-v-f2c85656><!--[--><!--]--><div class="VPNavBarSearch search" data-v-f2c85656><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-f2c85656 data-v-33ef0f58><span id="main-nav-aria-label" class="visually-hidden" data-v-33ef0f58> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-f2c85656 data-v-b431cf34><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b431cf34 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-f2c85656 data-v-661149a6 data-v-965a88c0><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-965a88c0><span class="vpi-more-horizontal icon" data-v-965a88c0></span></button><div class="menu" data-v-965a88c0><div class="VPMenu" data-v-965a88c0 data-v-979bc427><!----><!--[--><!--[--><!----><div class="group" data-v-661149a6><div class="item appearance" data-v-661149a6><p class="label" data-v-661149a6>深浅模式</p><div class="appearance-action" data-v-661149a6><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-661149a6 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-f2c85656 data-v-057c9ada><span class="container" data-v-057c9ada><span class="top" data-v-057c9ada></span><span class="middle" data-v-057c9ada></span><span class="bottom" data-v-057c9ada></span></span></button></div></div></div></div><div class="divider" data-v-f2c85656><div class="divider-line" data-v-f2c85656></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-9bcec609 data-v-bb57d500><div class="container" data-v-bb57d500><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-bb57d500><span class="vpi-align-left menu-icon" data-v-bb57d500></span><span class="menu-text" data-v-bb57d500>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-bb57d500 data-v-600f5a1a><button data-v-600f5a1a>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-9bcec609 data-v-ff6a08f7><div class="curtain" data-v-ff6a08f7></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-ff6a08f7><span class="visually-hidden" id="sidebar-aria-label" data-v-ff6a08f7> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible collapsed" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible has-active" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于原型的像素级再分类提高弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>C-CAM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Rolling-Unet%20Revitalizing%20MLP%20Ability%20to%20Efficiently%20Extract%20Long-Distance%20Dependencies%20for%20Medical%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：Rolling-Net</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-9bcec609 data-v-b233c28d><div class="VPDoc has-sidebar has-aside" data-v-b233c28d data-v-4e3ca6fa><!--[--><!--]--><div class="container" data-v-4e3ca6fa><div class="aside" data-v-4e3ca6fa><div class="aside-curtain" data-v-4e3ca6fa></div><div class="aside-container" data-v-4e3ca6fa><div class="aside-content" data-v-4e3ca6fa><div class="VPDocAside" data-v-4e3ca6fa data-v-99a4d02b><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-99a4d02b data-v-6eb44d9d><div class="content" data-v-6eb44d9d><div class="outline-marker" data-v-6eb44d9d></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-6eb44d9d>当前大纲</div><ul class="VPDocOutlineItem root" data-v-6eb44d9d data-v-da2b5471><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-99a4d02b></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-4e3ca6fa><div class="content-container" data-v-4e3ca6fa><!--[--><!--]--><main class="main" data-v-4e3ca6fa><div style="position:relative;" class="vp-doc _blog_column_Paper_WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic" data-v-4e3ca6fa><div><h1 id="weakclip-adapting-clip-for-weakly-supervised-semantic-segmentation" tabindex="-1">WeakCLIP: Adapting CLIP for Weakly-Supervised Semantic Segmentation <a class="header-anchor" href="#weakclip-adapting-clip-for-weakly-supervised-semantic-segmentation" aria-label="Permalink to &quot;WeakCLIP: Adapting CLIP for Weakly-Supervised Semantic Segmentation&quot;">​</a></h1><p>华中科技大学、西北工业大学</p><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/30/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><div class="tip custom-block"><p class="custom-block-title">TIP</p><p>CLIP、弱监督语义分割</p></div><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic segmentation (WSSS)aims to refine the class activationmap(CAM)as pseudo masks, but heavily relies on inductive biases like hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a novel text-to-pixel matching paradigm forWSSS.However, directly applying CLIP toWSSS is challenging due to three critical problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the 1/16 down-sampling resolution ofViT. Thus, we proposeWeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP toWSSS. Specifically, we first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically.WeakCLIP provides an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released at <a href="https://github.com/hustvl/WeakCLIP" target="_blank" rel="noreferrer">https://github.com/hustvl/WeakCLIP</a>.</p><h2 id="翻译" tabindex="-1">翻译 <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;翻译&quot;">​</a></h2><p>对比语言和图像预训练(CLIP)在各种计算机视觉任务中取得了巨大的成功，并且利用其大规模的预训练知识为增强弱监督图像理解提供了一个很好的途径。弱监督语义分割(WSSS)是一种减少对像素级人工标注标签依赖的有效方法，其目的是细化类激活图(CAM)并生成高质量的伪掩膜。弱监督语义分割(WSSS)旨在将类激活图(CAM)细化为伪掩膜，但严重依赖于手工制作先验和数字图像处理方法等归纳偏差。对于视觉语言预训练模型，即CLIP，我们提出了一种新的文本到像素的wsss匹配范式。然而，直接应用CLIP toWSSS是具有挑战性的，因为存在三个关键问题:(1)对比预训练与WSSSCAM细化之间的任务差距;(2)缺乏文本到像素的建模以充分利用预训练的知识;(3)由于vit的下采样分辨率为1 16，细节不足。因此，我们提出了weakclip来解决问题，并利用CLIP toWSSS的预训练知识。具体来说，我们首先通过提出金字塔适配器和可学习的提示词符来提取特定于wss的表示来解决任务差距。然后，我们设计了一个共同关注匹配模块来模拟文本到像素的关系。最后，引入金字塔适配器和文本引导解码器，实现多级信息采集，并与文本引导分层集成。WeakCLIP提供了一种有效的、参数高效的方法来传递CLIP知识以改进CAM。大量的实验表明，WeakCLIP在标准基准测试上达到了最先进的WSSS性能，即在PASCAL VOC 2012的val集上达到了74.0%的mIoU，在COCO 2014的val集上达到了46.1%的mIoU。源代码和模型检查点在<a href="https://github.com/hustvl/WeakCLIP%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82" target="_blank" rel="noreferrer">https://github.com/hustvl/WeakCLIP上发布。</a></p><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><p>本文聚焦于弱监督语义分割（WSSS）任务，旨在解决当前方法在处理类激活图（CAM）种子时面临的问题，具体研究背景如下：</p><ul><li><strong>WSSS的重要性与挑战</strong>：语义分割中像素级标注耗时费力，限制了实际应用。WSSS利用弱监督信息生成伪像素级分割，可减轻标注负担，但仅使用图像级标签的WSSS是该领域最具挑战性的方向。</li><li><strong>现有CAM细化方法的局限性</strong>：现有方法多依赖手工先验和改进的数字图像处理算法来细化CAM，这些方法存在归纳偏差，限制了性能和鲁棒性。</li><li><strong>CLIP的潜力与应用挑战</strong>：CLIP在计算机视觉任务中取得了巨大成功，为WSSS带来了新的机遇。然而，直接将CLIP应用于WSSS存在任务差距、缺乏文本到像素建模以及细节不足等问题。 基于以上背景，作者提出了WeakCLIP方法，旨在利用CLIP的预训练知识，通过文本到像素匹配范式解决WSSS中的关键问题，提高伪掩码的质量，从而推动WSSS的发展。</li></ul><h2 id="研究现状" tabindex="-1">研究现状 <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;研究现状&quot;">​</a></h2><ul><li><strong>弱监督语义分割（WSSS）</strong>：为减轻像素级标注负担，出现多种基于不同弱监督信息（如<strong>边界框、涂鸦、点、图像级标签</strong>）的算法。其中，基于<strong>图像级标签</strong>的WSSS最具挑战性，常使用类激活图（CAM）定位目标，但原始CAM噪声大、易出错，已有多种方法对其进行优化。</li><li><strong>大规模预训练模型</strong>：大规模预训练模型在各领域广泛应用，如CLIP通过对比学习在大量图像-文本对上预训练，展现出强大的知识迁移能力。已有研究尝试将CLIP应用于WSSS，如CLIMS引入辅助损失，CLIP - ES利用文本提示和GradCAM提升CAM质量。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-44-06.png" alt="Snipaste_2025-04-09_08-44-06" loading="lazy"></p><h2 id="提出的模型" tabindex="-1">提出的模型 <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;提出的模型&quot;">​</a></h2><p>本文提出了一种名为<strong>WeakCLIP的弱监督语义分割（WSSS）方法</strong>，旨在利用预训练的CLIP模型知识来改进WSSS网络的类激活图（CAM）细化过程。以下是WeakCLIP模型的详细介绍：</p><ol><li><p><strong>文本到像素匹配范式</strong>：与以往基于CLIP的WSSS方法不同，WeakCLIP提出了<strong>文本到像素匹配</strong>范式，以在像素级别查询相似度。具体而言，输入图像通过CLIP预训练的ViT - B网络提取多层特征图，经过投影层后，定义文本到像素匹配操作，得到文本到像素匹配的嵌入。</p></li><li><p><strong>WeakCLIP框架</strong></p><ul><li><strong>可学习提示（Learnable Prompt）</strong>：受CoOp和CLIP - Adapter启发，提出可学习嵌入作为自适应提示。将类文本标记并嵌入为类文本嵌入，与随机初始化的可学习嵌入拼接，作为文本编码器的输入，最终投影得到文本嵌入。</li><li><strong>金字塔适配器（Pyramid Adapter）</strong>：为解决CLIP视觉编码器专注于整体图像内容以及低分辨率问题，提出金字塔适配器。它独立于CLIP图像编码器，对不同分辨率的特征图进行处理，通过上采样和下采样操作，生成不同分辨率的特征，有效融合低级细节和高级表示。</li><li><strong>协同注意力匹配模块（Co - attention Matching）</strong>：为充分利用CLIP预训练知识，提出协同注意力匹配模块，用于建模双向文本到像素匹配。该模块使用两个交叉注意力模块分别建模文本到像素和像素到文本的关系，并通过残差连接更新文本和图像嵌入，最后进行文本到图像匹配得到协同注意力匹配的嵌入。</li><li><strong>文本引导解码器（Text - Guided Decoder）</strong>：为解决CLIP ViT - B的分辨率限制问题，引入文本引导解码器。将协同注意力匹配的嵌入插值到与适配器输出特征对应的大小，与适配器输出特征拼接后进行解码，得到分割预测。</li><li><strong>WSSS损失（WSSS Losses）</strong>：采用DSRG中使用的WSSS损失，包括平衡种子损失和边界损失。平衡种子损失计算分割预测与CAM种子之间的加权交叉熵损失；边界损失先使用条件随机场（CRF）处理分割预测以细化对象边界，然后计算CRF细化结果与分割预测之间的Kullback - Leibler散度损失。</li></ul></li><li><p><strong>伪掩码生成和再训练</strong>：使用训练好的WeakCLIP网络生成高质量的伪掩码。当推理结果中的类别不在图像级标签中时，将其标记为未知标签。最后，使用生成的伪掩码进行全监督分割，采用DeepLabv1网络架构，并尝试使用更先进的基于ViT的分割方法进行再训练。 实验结果表明，WeakCLIP在PASCAL VOC 2012和COCO 2014数据集上取得了优于以往WSSS方法的结果，证明了该方法的有效性和高效性。</p></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-09_08-49-54.png" alt="Snipaste_2025-04-09_08-49-54" loading="lazy"></p><h2 id="实验-compared-with-sota" tabindex="-1">实验（Compared with SOTA） <a class="header-anchor" href="#实验-compared-with-sota" aria-label="Permalink to &quot;实验（Compared with SOTA）&quot;">​</a></h2><p>数据集：<strong>PASCAL VOC 2012、COCO 2014</strong></p><ul><li><strong>ASCAL VOC 2012</strong>：在CAM监督方面，WeakCLIP与MCTformer相同，但低于ViT - PCM；在伪掩码质量上，比基线MCTformer提高了8.1%，比AMN提高了5.0%。使用精炼后的伪掩码训练DeepLabV1网络，WeakCLIP在验证集和测试集上的mIoU分别达到74.0%和73.8%，优于其他仅使用图像级监督的方法，以及部分使用额外显著图监督或边界框监督的方法。使用基于ViT的再训练基准（Segmenter和SegFormer）可进一步提升分割结果，混合ViT再训练的WeakCLIP表现最佳。</li><li><strong>COCO 2014</strong>：WeakCLIP在验证集上的mIoU达到46.1%，比基线MCTformer提高了4.1%，优于其他仅使用图像级监督的方法。使用SegFormer和MiT - B2骨干进行再训练，WeakCLIP在COCO 2014验证集上取得最佳性能。</li></ul><h2 id="实验-ablation-experiments" tabindex="-1">实验（Ablation Experiments）🥇 <a class="header-anchor" href="#实验-ablation-experiments" aria-label="Permalink to &quot;实验（Ablation Experiments）:1st_place_medal:&quot;">​</a></h2><ol><li><ul><li><strong>组件改进</strong>：协同注意力匹配模块将验证集mIoU提高到67.4%；可学习提示将其提高到68.9%；金字塔适配器将性能提升到70.3%；文本引导解码器将验证集mIoU进一步提升到72.6%。</li><li><strong>可学习嵌入数量</strong>：可学习嵌入数量为8时性能最佳。</li><li><strong>可学习温度初始值</strong>：协同注意力匹配中可学习温度初始值为1e - 1时性能最佳。</li></ul></li></ol><h2 id="其他实验-other-experiments" tabindex="-1">其他实验（Other Experiments）🥇 <a class="header-anchor" href="#其他实验-other-experiments" aria-label="Permalink to &quot;其他实验（Other Experiments）:1st_place_medal:&quot;">​</a></h2><ol><li><strong>逐类语义分割结果</strong>：在PASCAL VOC 2012的验证集和测试集以及COCO 2014的验证集上，将WeakCLIP与基线MCTformer进行逐类分割结果比较，WeakCLIP在大多数类别中表现更优。</li><li><strong>可视化分析</strong><ul><li>比较MCTformer和WeakCLIP生成的伪掩码，WeakCLIP生成的语义信息更准确、精确，能识别出MCTformer遗漏或识别不准确的对象位置。</li><li>在PASCAL VOC 2012验证集上再训练后的分割结果可视化显示，WeakCLIP对室内和室外场景都能实现准确分割。</li></ul></li><li><strong>参数效率分析</strong>：与MCTformer相比，WeakCLIP仅训练12.4%的参数，训练帧率（FPS）快4.3倍，节省68.4%的GPU内存。</li><li><strong>不同CLIP骨干实验</strong>：使用不同CLIP骨干进行实验，结果表明WeakCLIP - ResNet101性能优于WeakCLIP - ResNet50，WeakCLIP - ViT - B表现最佳。</li></ol><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>作者提出了名为<strong>WeakCLIP</strong>的新方案，旨在利用预训练CLIP模型的知识来增强弱监督语义分割（WSSS）网络的**类激活图（CAM）**细化过程。该框架采用了新的文本到像素匹配范式，有效解决了将CLIP集成到WSSS中存在的三个关键问题。在广泛使用的PASCAL VOC 2012和COCO 2014数据集上的实验结果表明，与以往的WSSS方法相比，WeakCLIP取得了显著改进。引入利用大规模视觉语言预训练的WeakCLIP范式，有望推动WSSS问题的解决。未来，作者计划探索更先进的大规模CLIP，以提升WSSS的像素级理解能力。</p></div></div></main><footer class="VPDocFooter" data-v-4e3ca6fa data-v-92f5315a><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-92f5315a><div class="edit-link" data-v-92f5315a><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic.md" target="_blank" rel="noreferrer" data-v-92f5315a><!--[--><span class="vpi-square-pen edit-link-icon" data-v-92f5315a></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-92f5315a><p class="VPLastUpdated" data-v-92f5315a data-v-8bddb0e8>最后更新于: <time datetime="2025-04-30T14:43:39.000Z" data-v-8bddb0e8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-92f5315a><span class="visually-hidden" id="doc-footer-aria-label" data-v-92f5315a>Pager</span><div class="pager" data-v-92f5315a><a class="VPLink link pager-link prev" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>上一页</span><span class="title" data-v-92f5315a>基于全对一致性学习的弱监督语义分割</span><!--]--></a></div><div class="pager" data-v-92f5315a><a class="VPLink link pager-link next" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>下一页</span><span class="title" data-v-92f5315a>弱监督语义分割中的共享权重校准</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-9bcec609 data-v-93274d57><div class="container" data-v-93274d57><p class="message" data-v-93274d57>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-93274d57>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"CM58dPIy\",\"column_image_segmentation_20250224-语义分割概述.md\":\"DJ9aYSe4\",\"column_image_segmentation_20250312-pytorch教程.md\":\"D5CxUCg-\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"D5oc3w1y\",\"column_image_segmentation_fcn模型讲解.md\":\"BZLPVhhI\",\"column_image_segmentation_index.md\":\"Cnnp9LXF\",\"column_image_segmentation_segment algrothm.md\":\"DaRLgJ0n\",\"column_image_segmentation_图像分割基础.md\":\"BmgyYd22\",\"column_image_segmentation_语义分割基础模型.md\":\"DUH2PxjJ\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"DcrK4Xvz\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"DvX7x9-E\",\"column_paper_c-cam causal cam for weakly supervised semantic segmentation on medical image.md\":\"D1Kx1tOx\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"B893K9ex\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"IrpndHQH\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"ZdGVBa6V\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"a-y9d-aH\",\"column_paper_dgss.md\":\"DYlWMCsg\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"Kehsl8wn\",\"column_paper_high_quality_segmentation.md\":\"BczNX3CC\",\"column_paper_index.md\":\"CePKwe5R\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"DtChIPkY\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"FSl71yox\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"KVKYo0G7\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"CmDI8avq\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"B4Vd7yMW\",\"column_paper_night-time_semantic_segmentation.md\":\"B2lCfwcj\",\"column_paper_pat.md\":\"pcvVhIb3\",\"column_paper_pixel-wise reclassification with prototypes for enhancing weakly supervised semantic segmentation.md\":\"CczNRG9p\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"69bdYjZh\",\"column_paper_prompting_multi-moda_segmetation.md\":\"B7LyZ28Z\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"bwcHQVPD\",\"column_paper_rolling-unet revitalizing mlp ability to efficiently extract long-distance dependencies for medical image segmentation.md\":\"C0yqKavn\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"DjJdi5nt\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"CetPxX_K\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"SEmaCmIS\",\"column_paper_sed.md\":\"AGJR-cvQ\",\"column_paper_segment anything.md\":\"JUOxZkd-\",\"column_paper_self-supervised_vit.md\":\"CuqUnHnx\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"0ZmwNJcO\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"itszMl-0\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"BrQcgEAg\",\"column_paper_visual studio code latex.md\":\"Crcgltrd\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"DXzQMqhZ\",\"column_puruse_index.md\":\"CgK5aykD\",\"column_puruse_template.md\":\"BDhSDpKE\",\"index.md\":\"Cv44Amku\",\"markdown-examples.md\":\"CfpMxZtF\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于原型的像素级再分类提高弱监督语义分割\",\"link\":\"/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation\"},{\"text\":\"C-CAM\",\"link\":\"/column/Paper/C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image\"},{\"text\":\"医学图像分割：Rolling-Net\",\"link\":\"/column/Paper/Rolling-Unet Revitalizing MLP Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>