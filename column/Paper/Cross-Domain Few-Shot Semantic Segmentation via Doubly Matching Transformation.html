<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇 | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/blog/assets/style.aP5EKgV3.css" as="style">
    <link rel="preload stylesheet" href="/blog/vp-icons.css" as="style">
    
    <script type="module" src="/blog/assets/app.g4FfhqZ8.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.dzp_CulL.js">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.Bw1ez5nK.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.md.CgRAYt2-.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-9bcec609><!--[--><!--]--><!--[--><span tabindex="-1" data-v-b775d67b></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-b775d67b>Skip to content</a><!--]--><!----><header class="VPNav" data-v-9bcec609 data-v-389f03b4><div class="VPNavBar" data-v-389f03b4 data-v-f2c85656><div class="wrapper" data-v-f2c85656><div class="container" data-v-f2c85656><div class="title" data-v-f2c85656><div class="VPNavBarTitle has-sidebar" data-v-f2c85656 data-v-02609f48><a class="title" href="/blog/" data-v-02609f48><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b1.png" alt data-v-52246203><!--]--><span data-v-02609f48>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-f2c85656><div class="content-body" data-v-f2c85656><!--[--><!--]--><div class="VPNavBarSearch search" data-v-f2c85656><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-f2c85656 data-v-33ef0f58><span id="main-nav-aria-label" class="visually-hidden" data-v-33ef0f58> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-33ef0f58 data-v-297ea667><!--[--><span data-v-297ea667>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-f2c85656 data-v-b431cf34><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b431cf34 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-f2c85656 data-v-661149a6 data-v-965a88c0><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-965a88c0><span class="vpi-more-horizontal icon" data-v-965a88c0></span></button><div class="menu" data-v-965a88c0><div class="VPMenu" data-v-965a88c0 data-v-979bc427><!----><!--[--><!--[--><!----><div class="group" data-v-661149a6><div class="item appearance" data-v-661149a6><p class="label" data-v-661149a6>深浅模式</p><div class="appearance-action" data-v-661149a6><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-661149a6 data-v-614ada65 data-v-46c8899f><span class="check" data-v-46c8899f><span class="icon" data-v-46c8899f><!--[--><span class="vpi-sun sun" data-v-614ada65></span><span class="vpi-moon moon" data-v-614ada65></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-f2c85656 data-v-057c9ada><span class="container" data-v-057c9ada><span class="top" data-v-057c9ada></span><span class="middle" data-v-057c9ada></span><span class="bottom" data-v-057c9ada></span></span></button></div></div></div></div><div class="divider" data-v-f2c85656><div class="divider-line" data-v-f2c85656></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-9bcec609 data-v-bb57d500><div class="container" data-v-bb57d500><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-bb57d500><span class="vpi-align-left menu-icon" data-v-bb57d500></span><span class="menu-text" data-v-bb57d500>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-bb57d500 data-v-600f5a1a><button data-v-600f5a1a>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-9bcec609 data-v-ff6a08f7><div class="curtain" data-v-ff6a08f7></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-ff6a08f7><span class="visually-hidden" id="sidebar-aria-label" data-v-ff6a08f7> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible collapsed has-active" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-941144ff><section class="VPSidebarItem level-0 collapsible" data-v-941144ff data-v-b9329948><div class="item" role="button" tabindex="0" data-v-b9329948><div class="indicator" data-v-b9329948></div><h2 class="text" data-v-b9329948>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b9329948><span class="vpi-chevron-right caret-icon" data-v-b9329948></span></div></div><div class="items" data-v-b9329948><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>基于原型的像素级再分类提高弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>C-CAM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b9329948 data-v-b9329948><div class="item" data-v-b9329948><div class="indicator" data-v-b9329948></div><a class="VPLink link link" href="/blog/column/Paper/Rolling-Unet%20Revitalizing%20MLP%20Ability%20to%20Efficiently%20Extract%20Long-Distance%20Dependencies%20for%20Medical%20Image%20Segmentation.html" data-v-b9329948><!--[--><p class="text" data-v-b9329948>医学图像分割：Rolling-Net</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-9bcec609 data-v-b233c28d><div class="VPDoc has-sidebar has-aside" data-v-b233c28d data-v-4e3ca6fa><!--[--><!--]--><div class="container" data-v-4e3ca6fa><div class="aside" data-v-4e3ca6fa><div class="aside-curtain" data-v-4e3ca6fa></div><div class="aside-container" data-v-4e3ca6fa><div class="aside-content" data-v-4e3ca6fa><div class="VPDocAside" data-v-4e3ca6fa data-v-99a4d02b><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-99a4d02b data-v-6eb44d9d><div class="content" data-v-6eb44d9d><div class="outline-marker" data-v-6eb44d9d></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-6eb44d9d>当前大纲</div><ul class="VPDocOutlineItem root" data-v-6eb44d9d data-v-da2b5471><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-99a4d02b></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-4e3ca6fa><div class="content-container" data-v-4e3ca6fa><!--[--><!--]--><main class="main" data-v-4e3ca6fa><div style="position:relative;" class="vp-doc _blog_column_Paper_Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation" data-v-4e3ca6fa><div><h1 id="cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation" tabindex="-1">Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation 🥇 <a class="header-anchor" href="#cross-domain-few-shot-semantic-segmentation-via-doubly-matching-transformation" aria-label="Permalink to &quot;Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation :1st_place_medal:&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/30/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><p>Nanjing University of Aeronautics and Astronautics 、State Key Laboratory of Integrated Services Networks, Xidian University</p><h2 id="摘要" tabindex="-1"><strong>摘要</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要**&quot;">​</a></h2><blockquote><p>Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in ad- dressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper, we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matri- ces based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query im- age with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at <a href="https://github.com/ChenJiayi68/DMTNet" target="_blank" rel="noreferrer">https://github.com/ChenJiayi68/DMTNet</a>.</p></blockquote><h2 id="翻译" tabindex="-1"><strong>翻译</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译**&quot;">​</a></h2><blockquote><p>**跨域少样本语义分割（CD-FSS）**旨在训练能够以少量标注图像对不同领域进行分割的通用模型。以往的研究已经证明了特征转换在解决CD-FSS问题中的有效性。然而，这些方法完全依赖于支持图像进行特征转换，而重复利用少量支持图像来处理每个类别，容易导致过拟合，并忽视类内外观的差异。为了解决上述问题，本文提出了一种基于双重匹配转换的网络（DMTNet）。我们并不完全依赖支持图像，而是提出了自匹配转换（SMT），通过查询图像自身构建特定的转换矩阵，将领域特定的查询特征转换为领域无关的特征。计算查询特定的转换矩阵有助于防止过拟合，特别是在元测试阶段，此时仅使用一张或几张图像作为支持图像来对数百或数千张图像进行分割。在获得领域无关特征后，我们利用双重超相关构建（DHC）模块，探索查询图像与支持图像前景和背景之间的超相关性，基于此生成前景和背景的预测图并进行监督，从而增强分割结果。此外，我们还提出了一种测试时自我微调（TSF）策略，以更准确地在未知领域自我调整查询预测。在四个流行数据集上的大量实验表明，DMTNet在性能上优于现有的最先进方法。相关代码可在 <a href="https://github.com/ChenJiayi68/DMTNet" target="_blank" rel="noreferrer">https://github.com/ChenJiayi68/DMTNet</a> 获取。</p></blockquote><h2 id="研究背景" tabindex="-1"><strong>研究背景</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景**&quot;">​</a></h2><blockquote><p><strong>语义分割</strong>近年来依赖大规模标注数据集取得快速发展，但实际场景中收集大量训练数据耗时且成本高。少样本语义分割（FSS）应运而生，旨在用少量标注支持图像实现查询图像的准确分割，常采用元学习，但在实际应用中，源数据集和目标数据集存在较大领域差距，导致FSS模型对未见领域的泛化能力较差。 为解决FSS模型在跨领域场景下性能显著下降的问题，跨领域少样本语义分割（CD - FSS）被提出。现有主要的CD - FSS方法PATNet通过将特定领域特征转换为领域无关特征来消除领域差距，但在元测试阶段仅基于少量支持图像的转换矩阵为大量查询图像生成领域无关特征，易导致过拟合。此外，多数现有CD - FSS方法在分割过程中只关注前景目标区域，忽略背景区域。 基于上述问题，本文提出一种基于<strong>双重匹配变换的网络（DMTNet）</strong>，以解决特征变换过度依赖支持图像、类内外观差异以及信息利用不充分等关键问题。</p></blockquote><h2 id="研究现状" tabindex="-1"><strong>研究现状</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状**&quot;">​</a></h2><ul><li><strong>少样本语义分割（FSS）</strong>：现有方法分为基于度量和基于关系两类。前者将支持图像表示为类原型，通过非参数测量工具分割查询图像；后者构建支持 - 查询对的密集对应关系。但在源域和目标域差距大时，性能会下降。</li><li><strong>跨域语义分割</strong>：分为域<strong>自适应语义分割（DASS）<strong>和</strong>域泛化语义分割（DGSS）</strong>。DASS通过联合使用源域和目标域数据训练模型；DGSS通过归一化和白化（NW）、域随机化（DR）等方法缩小域差距。</li><li><strong>跨域少样本语义分割（CD - FSS）</strong>：近期提出了一些方法，如PixDA、RTD、PATNet等，旨在解决少样本和域差距问题。</li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型**&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-05-44.png" alt="Snipaste_2025-03-10_10-05-44" loading="lazy"></p><p><strong>本文提出了一种基于双重匹配变换的网络（Doubly Matching Transformation-based Network，DMTNet）用于跨域少样本语义分割（Cross-Domain Few-shot Semantic Segmentation，CD-FSS）</strong></p><ol><li><p><strong>自匹配变换模块（Self-Matching Transformation，SMT）</strong></p><ul><li><p><strong>相似性自匹配</strong>：通过查询特征与支持图像的前景和背景原型之间的基于相似性的自匹配，为查询图像生成粗略的分割掩码。将支持特征划分为多个局部特征，通过测量支持局部原型与查询全局特征之间的相似性，生成更细粒度的预测查询掩码，并使用二元交叉熵（BCE）损失函数进行监督。</p></li><li><p><strong>自适应特征变换</strong>：为支持和查询特征分别构建专门的变换矩阵，确保在自适应变换过程中前景对象的不变性。通过求解线性方程得到变换矩阵，同时提出整合支持图像的广义逆来优化查询图像的广义逆。</p></li></ul></li><li><p><strong>双超相关构建模块（Dual Hypercorrelation Construction，DHC）</strong>：探索查询特征与支持图像的前景和背景特征在无域特征空间中的密集相关性。分别基于支持前景特征和背景特征与查询特征构建4D相关张量，将得到的密集相关图输入到4D卷积金字塔编码器和2D卷积金字塔解码器中，生成预测的查询前景掩码和背景掩码，并使用BCE损失函数进行训练监督。</p></li><li><p><strong>测试时自微调策略（Test-time Self-Finetuning，TSF）</strong>：在元测试阶段，通过尝试预测支持图像的真实掩码来微调网络，使模型学习目标域的风格信息，从而为查询图像生成更准确的掩码。只微调编码器的少数参数，避免对支持图像过拟合。</p></li></ol><h2 id="实验-compared-with-sota" tabindex="-1"><strong>实验（Compared with SOTA）</strong> <a class="header-anchor" href="#实验-compared-with-sota" aria-label="Permalink to &quot;**实验（Compared with SOTA）**&quot;">​</a></h2><blockquote><p>数据集：PASCAL VOC 2012、ISIC2018、Chest X-ray、 Deepglobe、 and FSS-1000</p></blockquote><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-10-33.png" alt="Snipaste_2025-03-10_10-10-33" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-14-22.png" alt="Snipaste_2025-03-10_10-14-22" loading="lazy"></p><p>与迁移学习、少样本语义分割和跨域少样本语义分割的几种先进方法进行比较，<strong>DMTNet在四个数据集的平均结果上表现优异</strong>，在1 - shot设置下达到**59.74%<strong>的平均IoU，在5 - shot设置下达到</strong>66.01%**的平均IoU。与最先进的PATNet相比，在1 - shot和5 - shot设置下分别提高了3.68%和4.02%。</p><h2 id="实验-ablation-study" tabindex="-1"><strong>实验（Ablation Study）</strong> <a class="header-anchor" href="#实验-ablation-study" aria-label="Permalink to &quot;**实验（Ablation Study）**&quot;">​</a></h2><p>验证了SMT、DHC和TSF三个关键模块的有效性，使用所有三个模块时模型性能最佳，移除任何一个模块都会导致平均性能下降。同时，通过实验确定了TSF策略中微调编码器层能取得最佳性能。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-18.png" alt="Snipaste_2025-03-10_10-12-18" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-10_10-12-35.png" alt="Snipaste_2025-03-10_10-12-35" loading="lazy"></p><h2 id="结论" tabindex="-1"><strong>结论</strong> <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;**结论**&quot;">​</a></h2><p><strong>作者提出用于跨域少样本语义分割的DMTNet，并得出以下结论：</strong></p><ol><li><strong>DMTNet利用SMT模块基于自身原型为支持和查询图像计算变换矩阵，将特定领域特征自适应转换为通用特征，避免过度依赖支持图像导致过拟合。</strong></li><li><strong>DHC模块在通用特征空间中探索查询图像与支持图像前景和背景的双重超相关性，生成并监督前景和背景预测掩码，提升分割效果。</strong></li><li><strong>在元测试阶段，TSF策略微调少量参数，使模型学习目标域风格信息，进一步提高分割性能。</strong></li><li><strong>大量实验表明，DMTNet在四个具有不同领域差距的数据集上有效，达到了当前最优性能。</strong></li></ol></div></div></main><footer class="VPDocFooter" data-v-4e3ca6fa data-v-92f5315a><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-92f5315a><div class="edit-link" data-v-92f5315a><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation.md" target="_blank" rel="noreferrer" data-v-92f5315a><!--[--><span class="vpi-square-pen edit-link-icon" data-v-92f5315a></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-92f5315a><p class="VPLastUpdated" data-v-92f5315a data-v-8bddb0e8>最后更新于: <time datetime="2025-04-30T15:17:56.000Z" data-v-8bddb0e8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-92f5315a><span class="visually-hidden" id="doc-footer-aria-label" data-v-92f5315a>Pager</span><div class="pager" data-v-92f5315a><a class="VPLink link pager-link prev" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>上一页</span><span class="title" data-v-92f5315a>基于Transformer的自适应原型匹配网络</span><!--]--></a></div><div class="pager" data-v-92f5315a><a class="VPLink link pager-link next" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-92f5315a><!--[--><span class="desc" data-v-92f5315a>下一页</span><span class="title" data-v-92f5315a>相关内在特征增强的少样本与意义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-9bcec609 data-v-93274d57><div class="container" data-v-93274d57><p class="message" data-v-93274d57>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-93274d57>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"z19mZNku\",\"column_image_segmentation_20250224-语义分割概述.md\":\"iuS4dJhu\",\"column_image_segmentation_20250312-pytorch教程.md\":\"BaWbulcg\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"CVQDP8QM\",\"column_image_segmentation_fcn模型讲解.md\":\"CE8bEZdZ\",\"column_image_segmentation_index.md\":\"lnBprgqY\",\"column_image_segmentation_segment algrothm.md\":\"Dr-YMOmv\",\"column_image_segmentation_图像分割基础.md\":\"BmGD68I9\",\"column_image_segmentation_语义分割基础模型.md\":\"gixyT3Vv\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"54AzNJk9\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"CSSeQOKg\",\"column_paper_c-cam causal cam for weakly supervised semantic segmentation on medical image.md\":\"DEOKIhFN\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"DEsrsnA7\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"Bo14D9-A\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"DSfv6MJN\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"CgRAYt2-\",\"column_paper_dgss.md\":\"DSYavnQF\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"CZoquaSX\",\"column_paper_high_quality_segmentation.md\":\"CC8DfBn1\",\"column_paper_index.md\":\"Synlh4NI\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"BeUtBo0E\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"DVJSSNdD\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"BJ_F3svD\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"DTxxhFdx\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"3J1B893t\",\"column_paper_night-time_semantic_segmentation.md\":\"BRrk_XVi\",\"column_paper_pat.md\":\"BptPSP_H\",\"column_paper_pixel-wise reclassification with prototypes for enhancing weakly supervised semantic segmentation.md\":\"DdsmXG9W\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"B5sgGZt8\",\"column_paper_prompting_multi-moda_segmetation.md\":\"DUQOziMy\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"CXgHjlnn\",\"column_paper_rolling-unet revitalizing mlp ability to efficiently extract long-distance dependencies for medical image segmentation.md\":\"CJUaFy27\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"BX_WuxG-\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"Dj1NeIuT\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"DZ8y7O_I\",\"column_paper_sed.md\":\"m07QoKIc\",\"column_paper_segment anything.md\":\"B92nF2Yh\",\"column_paper_self-supervised_vit.md\":\"CH5QKIeK\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"BeaNaAZm\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"T6SYAXM0\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"CCyCHAAw\",\"column_paper_visual studio code latex.md\":\"BCPb2vsp\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"DOyuAc6K\",\"column_puruse_index.md\":\"CFTXr3a1\",\"column_puruse_template.md\":\"BXMZIw1p\",\"index.md\":\"BfZhzBoj\",\"markdown-examples.md\":\"Crq_jGJN\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b1.png\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于原型的像素级再分类提高弱监督语义分割\",\"link\":\"/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation\"},{\"text\":\"C-CAM\",\"link\":\"/column/Paper/C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image\"},{\"text\":\"医学图像分割：Rolling-Net\",\"link\":\"/column/Paper/Rolling-Unet Revitalizing MLP Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>