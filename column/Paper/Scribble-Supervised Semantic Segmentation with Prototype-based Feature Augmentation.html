<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.DjOpM9sR.css" as="style">
    
    <script type="module" src="/blog/assets/app.CNJiusZ9.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.CLo04awk.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.BI0VntTk.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.md.D4XCsM9u.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation" data-v-35102dec><div><h1 id="scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation" tabindex="-1">Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation <a class="header-anchor" href="#scribble-supervised-semantic-segmentation-with-prototype-based-feature-augmentation" aria-label="Permalink to &quot;Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/11/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><p><strong>Hohai University, Nanjing, China</strong></p><p><strong>RMIT University, Melbourne, Australia</strong></p><h2 id="摘要" tabindex="-1"><strong>摘要</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要**&quot;">​</a></h2><p><strong>Scribble-supervised semantic segmentation</strong> presents a cost-effective training method that utilizes annotations generated through scribbling. It is valued in attaining high performance while minimizing annotation costs, which has made it highly regarded among researchers. Scribble supervision propagates information from labeled pixels to the surrounding unlabeled pixels, enabling semantic segmentation for the entire image. However, existing methods often ignore the features of classified pixels during feature propagation. To address these limitations, this paper proposes a prototype-based feature augmentation method that leverages feature prototypes to augment scribble supervision. Experimental results demonstrate that our approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset in scribble-supervised semantic segmentation tasks. The code is available at <a href="https://github.com/TranquilChan/PFA" target="_blank" rel="noreferrer">https://github.com/TranquilChan/PFA</a>.</p><h2 id="翻译" tabindex="-1"><strong>翻译</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译**&quot;">​</a></h2><p>**涂鸦监督语义分割（Scribble-supervised semantic segmentation）**提出了一种经济高效的训练方法，通过使用涂鸦生成的标注进行模型训练。该方法因能在显著降低标注成本的同时实现高性能表现，因此备受研究人员推崇。其核心原理是通过将已标注像素的信息传递至相邻未标注区域，从而完成整幅图像的语义分割。然而，我们发现现有方法在特征传递过程中普遍忽视已分类像素的特征特性。针对这一局限性，本文提出基于原型（prototype）的特征增强方法，通过挖掘特征原型（feature prototypes）的统计特性来强化涂鸦监督效果。实验表明，我们的方法在 PASCAL VOC 2012 数据集的涂鸦监督语义分割任务中达到了当前最佳水平，相关代码已开源：<a href="https://github.com/TranbilChan/PFA%E3%80%82" target="_blank" rel="noreferrer">https://github.com/TranbilChan/PFA。</a></p><h2 id="研究背景" tabindex="-1"><strong>研究背景</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景**&quot;">​</a></h2><p><strong>标注成本问题</strong>：深度学习技术推动了深度神经网络在图像分割的发展，但是，对于标注像素级别的样本需要大量的人力和财力，并且其标注过程也非常繁琐。因此，研究者越来越关注利用涂鸦标签进行监督学习的方法。<strong>涂鸦标签属于弱监督学习</strong>，相比像素级标注，能显著减少标注工作量、提高效率，且比点、边界框和图像级标签提供更多关键语义信息。</p><p><strong>现有方法的局限性</strong>：现有涂鸦监督语义分割方法主要依赖<strong>正则化损失、一致性损失、伪建议、辅助任务和标签扩散</strong>等，但这些方法存在一定缺陷。例如，正则化方法常忽略利用高层语义信息，一致性损失未在类别层面提供直接监督，伪标签方法耗时，辅助任务会引入额外数据和预测误差，标签扩散主要依赖局部信息，且许多方法忽略了正确分类像素特征在指导边界区域像素分类中的作用。 基于以上背景，作者提出基于原型的特征增强方法，以解决现有方法的不足，提高涂鸦监督语义分割的性能。</p><h2 id="研究现状" tabindex="-1"><strong>研究现状</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状**&quot;">​</a></h2><ul><li><p><strong>标注方式</strong>：图像语义分割任务训练通常需大量高质量标注样本，像素级标注耗时耗力，因此弱监督学习方法受关注，如使用涂鸦、点、边界框和图像级标签等。其中，涂鸦监督能提供更多关键语义信息，表现更优。</p></li><li><p><strong>现有方法</strong>：现有涂鸦监督语义分割方法主要依赖正则化损失、一致性损失、伪建议、辅助任务和标签扩散等，但这些方法存在一定局限性。</p></li><li><p><strong>原型方法</strong>：特征原型在计算机视觉任务中用于增强模型识别能力，部分方法在弱监督语义分割中探索了原型的使用，但未充分发挥其特征增强和引导作用。</p></li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型**&quot;">​</a></h2><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-29-04.png" alt="Snipaste_2025-03-11_19-29-04" loading="lazy"></p><ul><li><strong>特征提取</strong>：使用基于Mix Transformer的编码器（Segformer中的MiT-B1）提取初始特征图。</li><li><strong>初始预测</strong>：将特征图输入解码器生成语义分割预测图，通过部分交叉熵损失（partial cross-entropy loss），利用涂鸦标签进行监督，细化预测结果。</li><li><strong>原型提取与更新</strong>：从初始预测图的高置信区域中提取对应特征向量，通过加权平均形成局部原型。在训练迭代中，局部原型动态更新全局原型。</li><li><strong>特征增强</strong>：使用局部和全局原型通过原型特征增强器对初始特征进行增强。</li><li><strong>一致性监督</strong>：将增强后的特征图再次通过解码器生成增强预测图，使用一致性损失（consistency loss）对初始预测图和增强预测图进行约束。</li></ul><h2 id="实验过程-compared-with-sota" tabindex="-1"><strong>实验过程（Compared with SOTA）</strong> <a class="header-anchor" href="#实验过程-compared-with-sota" aria-label="Permalink to &quot;**实验过程（Compared with SOTA）**&quot;">​</a></h2><p>数据集：<strong>PASCAL-Scribble</strong></p><ul><li>选择<strong>MiT-B1</strong>作为骨干网络，与现有方法在<strong>PASCAL VOC 2012</strong>验证集上进行比较。</li><li>与当前最先进的方法TEL相比，尽管MiT-B1骨干网络在全监督数据集上的性能稍弱，但该方法的mIoU仍提高了0.6%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-11_19-33-24.png" alt="Snipaste_2025-03-11_19-33-24" loading="lazy"></p><h2 id="实验过程-ablation-experiments" tabindex="-1"><strong>实验过程（Ablation Experiments）</strong> <a class="header-anchor" href="#实验过程-ablation-experiments" aria-label="Permalink to &quot;**实验过程（Ablation Experiments）**&quot;">​</a></h2><ul><li><strong>各组件有效性</strong>：以仅使用部分交叉熵损失作为基线，对<strong>局部原型增强</strong>和<strong>全局原型增强</strong>方法进行消融实验。结果表明，同时使用两种原型增强时性能最佳，mIoU比基线提高了10.4%。</li><li><strong>原型设置</strong>：实验发现，当每个类别的全局原型数量增加到约5时，mIoU的增加趋于饱和；在原型提取时，k百分比为8%时方法性能较好。</li><li><strong>骨干网络影响</strong>：研究了不同骨干网络对方法的影响，发现基于Transformer的骨干网络在效率和性能上限方面表现更优。使用MiT - B5时，mIoU达到81.5%，显著超过现有方法。</li></ul><h2 id="结论" tabindex="-1"><strong>结论</strong> <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;**结论**&quot;">​</a></h2><p>作者提出了一种基于原型的特征增强方法用于涂鸦监督语义分割，得出以下结论：</p><ul><li><p>从<strong>涂鸦监督</strong>初始结果的置信部分提取原型，利用这些原型增强初始特征，并根据涂鸦监督的具体情况采用不同原型策略，能以正确分类像素的原型引导错误分类像素的分类，提升预测性能。</p></li><li><p>实验结果表明，该方法在PASCAL VOC 2012数据集上达到了最先进的性能，相比当前最优方法TEL，使用稍弱的骨干网络MiT-B1仍使mIoU提高了0.6%。</p></li><li><p><strong>未来计划将此方法应用于其他任务，以挖掘其巨大潜力和应用价值 （下一个创新点）</strong></p></li></ul></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-04-11T09:37:48.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>相关内在特征增强的少样本与意义分割</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>面向弱监督语义分割的渐进式特征自增强</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"D5aKduv8\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"B--tmkRd\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"DOzoNQEK\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"BJdHqWXZ\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"MQ5IotcS\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"C-Ay97Y-\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"DP0-0bBL\",\"column_paper_dgss.md\":\"BX1YZhn-\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"BwE2wND-\",\"column_paper_high_quality_segmentation.md\":\"BF6oqMqy\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"BcpagYs7\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"DdF76l77\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"BLYPPqDR\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"CmrhycHk\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"BUGoO665\",\"column_paper_night-time_semantic_segmentation.md\":\"FfG0Jyqa\",\"column_paper_pat.md\":\"CvbTl1SQ\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"BqorakmV\",\"column_paper_prompting_multi-moda_segmetation.md\":\"Ben9Nl-6\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"DxYEt1BA\",\"column_paper_sed.md\":\"vCH0Lv17\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"B6H0YLzb\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"Cw3rfmJ8\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"Bh5WiKlk\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"D4XCsM9u\",\"column_paper_segment anything.md\":\"Dty7-lcy\",\"column_paper_self-supervised_vit.md\":\"xMu1aKEJ\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"BPDcB6-k\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"B2PfEQxd\",\"column_paper_visual studio code latex.md\":\"CzNUGyqb\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"o-dpcudj\",\"column_paper_index.md\":\"B30j3khH\",\"column_puruse_index.md\":\"BP9-gop-\",\"column_puruse_template.md\":\"B0U97JJB\",\"column_image_segmentation_20250224-语义分割概述.md\":\"mQ6rMlYH\",\"column_image_segmentation_20250312-pytorch教程.md\":\"DdEW4ONw\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"BTXAuYMi\",\"column_image_segmentation_fcn模型讲解.md\":\"CDxGKwtj\",\"column_image_segmentation_index.md\":\"DMQR_je7\",\"column_image_segmentation_segment algrothm.md\":\"e2j2ZgBI\",\"column_image_segmentation_图像分割基础.md\":\"uFajrVas\",\"column_image_segmentation_语义分割基础模型.md\":\"Cii2xXeN\",\"index.md\":\"RGMUHMKX\",\"markdown-examples.md\":\"DcnrEzY2\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>