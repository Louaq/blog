<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>High Quality Segmentation for Ultra High-resolution Images | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.CobjoJX1.css" as="style">
    
    <script type="module" src="/blog/assets/app.BYImmo1T.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.D6Cah3zj.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.MSp81Bon.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_High_Quality_Segmentation.md.CFEq4ahO.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Pytorch/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>Pytorch笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/deepLearning/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>深度学习笔记</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_High_Quality_Segmentation" data-v-35102dec><div><h1 id="high-quality-segmentation-for-ultra-high-resolution-images" tabindex="-1">High Quality Segmentation for Ultra High-resolution Images <a class="header-anchor" href="#high-quality-segmentation-for-ultra-high-resolution-images" aria-label="Permalink to &quot;High Quality Segmentation for Ultra High-resolution Images&quot;">​</a></h1><h1 id="香港中文大学-adobe-等" tabindex="-1">香港中文大学 Adobe 等 <a class="header-anchor" href="#香港中文大学-adobe-等" aria-label="Permalink to &quot;香港中文大学  Adobe 等&quot;">​</a></h1><p>**摘要：**To segment 4K or 6K ultra high-resolution images needs extra computation consideration in image segmentation. Common strategies, such as down-sampling, patch crop- ping, and cascade model, cannot address well the balance issue between accuracy and computation cost. Motivated by the fact that humans distinguish among objects continu- ously from coarse to precise levels, we propose the Contin- uous Refinement Model (CRM) for the ultra high-resolution segmentation refinement task. CRM continuously aligns the feature map with the refinement target and aggregates fea- tures to reconstruct these image details. Besides, our CRM shows its significant generalization ability to fill the resolu- tion gap between low-resolution training images and ultra high-resolution testing ones. We present quantitative per- formance evaluation and visualization to show that our pro- posed method is fast and effective on image segmentation refinement. Code is available at <a href="https://github.com/dvlab-research/Entity/tree/main/CRM" target="_blank" rel="noreferrer">https://github.com/dvlab-research/Entity/tree/main/CRM</a>.</p><p><strong>翻译:</strong> 对4K或6K超高分辨率图像进行分割时，需要额外的计算资源。常见的策略，如下采样、图像裁剪和级联模型，往往难以很好地平衡准确性和计算成本。基于人类从粗略到精细地逐步区分物体的方式，我们提出了“连续细化模型”（CRM）来进行超高分辨率图像的分割细化任务。CRM通过不断地对齐特征图和细化目标，逐步聚合特征，重建图像细节。更重要的是，CRM展现出了很强的泛化能力，能够有效弥补低分辨率训练图像与超高分辨率测试图像之间的分辨率差距。我们通过定量的性能评估和可视化结果，展示了该方法在图像分割细化上的高效性和快速性。相关代码可以在<a href="https://github.com/dvlab-research/Entity/tree/main/CRM%E6%89%BE%E5%88%B0%E3%80%82" target="_blank" rel="noreferrer">https://github.com/dvlab-research/Entity/tree/main/CRM找到。</a></p><p><strong>研究背景</strong></p><p>随着相机和显示设备的快速发展，图像分辨率越来越高，4K和6K分辨率变得常见，这在人像照片后期处理、工业缺陷检测、医学诊断等领域带来了新机遇。然而，超高分辨率图像也给经典图像分割方法带来了挑战：</p><ol><li><strong>计算成本高</strong>：大量的输入像素在计算上代价高昂，且对GPU内存需求大。</li><li><strong>细节重建难</strong>：大多数现有方法通过插值对最终预测进行4到8倍上采样，无法在输出掩码上构建细粒度细节。 以往的分割细化方法，如针对1K - 2K分辨率图像的方法，存在图形模型依赖低级别颜色边界、基于传播的方法面临计算和内存限制、大模型易过拟合而浅细化网络细化能力有限等问题。而处理超高分辨率细化的级联解码器方法，虽能取得较好性能，但在推理时需要下采样和裁剪补丁，增加了成本、丢失了细节并破坏了全局上下文。 因此，为解决超高分辨率图像分割中精度与计算成本的平衡问题，作者提出了连续细化模型（CRM），以实现高效、精确的图像分割细化。</li></ol><p><strong>研究现状</strong></p><ol><li><strong>语义分割</strong>：FCN 引入深度卷积网络，PSPNet、DeepLab 系列等方法不断发展，输出步长多设为 4×或 8×，但直接插值预测结果存在边缘锯齿和细节缺失问题。</li><li><strong>分割细化</strong>：针对 1K 分辨率图像的细化技术能提升分割质量，但存在图形模型依赖低层次颜色边界、传播方法有计算和内存限制、模型易过拟合或细化能力有限等问题。对于 4K - 6K 超高清图像，级联解码器方法能取得较好效果，但结构复杂。</li><li><strong>隐式函数表示</strong>：在神经网络中用于表示对象或场景，如 NeRF、PixelNerf 等，其多视图一致性和光滑性有利于分割。</li></ol><p><strong>研究方法</strong></p><ol><li>提出<strong>连续细化模型（CRM）</strong>，引入<strong>隐函数</strong>，利用连续位置信息和特征对齐，有效降低计算成本，重建更多细节。</li><li>CRM采用多<strong>分辨率推理策略</strong>，适用于<strong>低分辨率训练和超高分辨率测试</strong>，总推理时间不到CascadePSP的一半。</li><li>实验表明，CRM在超高分辨率图像上取得最佳分割结果，还能提升现有全景分割模型性能，无需微调。</li></ol><p><strong>实验步骤</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-02_14-24-58.png" alt="Snipaste_2025-03-02_14-24-58" loading="lazy"></p><ol><li><strong>数据集和对比方法选择</strong> - <strong>训练数据集</strong>：遵循CascadePSP的设置，将MSRA - 10K、DUT - OMRON、EC - SSD和FSS - 1000合并为训练数据集，包含36,572张具有超过1000个语义类别的图像。 - <strong>测试数据集</strong>：使用CascadePSP提出的高分辨率图像分割数据集BIG（图像分辨率范围2K - 6K）进行超高清图像评估；还在重新标注的PASCAL VOC 2012数据集上进行评估；为证明模型的通用性，将CRM作为全景分割和实体分割的扩展进行评估。 - <strong>对比方法</strong>：选择CascadePSP作为超高清图像的主要对比方法；选择MGMatting作为掩码引导抠图方法，Segfix作为高分辨率分割细化方法；选择PanopticFCN和Entity Segmentor作为全景和实体分割的基准方法。</li><li><strong>实现细节</strong> - <strong>模型实现</strong>：使用PyTorch实现模型，使用去掉conv5 x的ResNet - 50作为编码器Eθ。 - <strong>训练设置</strong>：使用Adam优化器，学习率为2.25×10⁻⁴，在22,500和37,500步时将学习率降至十分之一，总步数45,000步。训练输入是从原始图像及其对应的扰动掩码中裁剪的224×224的图像块，扰动掩码是在真实掩码上随机扰动得到，随机IoU阈值在0.8 - 1.0之间。 - <strong>评估设置</strong>：在实验中从连续范围中选择4个缩放比例进行细化，CRM的总推理时间仍不到CascadePSP的一半。</li><li>定量结果评估** - <strong>对比实验</strong>：在BIG数据集上对比CRM、CascadePSP、Segfix和MGMatting的性能，包括交并比（IoU）、平均边界准确率（mBA）、全景质量（PQ）和平均精度（AP）等指标。结果表明CRM性能更好，在高分辨率图像上运行速度更快，且FLOPs和参数更少。 - <strong>扩展实验</strong>：在全景分割和实体分割实验中，将CRM添加到PanopticFCN和EntitySeg后，它们的分割性能得到增强；在重新标注的Pascal VOC 2012数据集上，CRM比Segfix表现更好，与CascadePSP的IoU相当，但更注重细节</li><li><strong>定性结果展示</strong> - <strong>可视化对比</strong>：展示CascadePSP、Segfix和CRM的细化结果对比，CRM的细化结果包含更多细节，仅使用语义分割标注训练就能生成类似抠图的结果，并且能更好地重建粗掩码中缺失的部分。 - <strong>应用示例</strong>：展示将CRM应用于全景分割的可视化结果，掩码细节和整体分割效果都有显著改善。</li><li><strong>消融实验</strong> - <strong>CRM和推理分辨率</strong>：分析CRM和隐式函数对不同分辨率下性能的影响，CRM在低分辨率下能细化出较好的通用掩码，随着分辨率增加，能生成更多细节，mBA提高。 - <strong>CAM和隐式函数</strong>：验证CAM和隐式函数都是CRM不可或缺的部分，二者协同作用能提升性能。 - <strong>推理连续性的影响</strong>：性能随着采样的缩放比例数量增加而提升，更多的缩放比例意味着推理分辨率的连续性更好，有助于提高性能直至收敛。</li></ol><p><strong>结论：</strong></p><p>作者提出了用于超高清图像分割细化的连续细化模型（CRM），并得出以下结论：</p><ol><li><strong>性能优势</strong>：CRM 能连续对齐特征图与细化目标，有助于聚合特征以重建高分辨率掩码上的细节，在性能和速度方面表现出色，实验表明其在超高清图像上的分割效果最佳，还能提升现有全景分割模型的性能。</li><li><strong>泛化能力</strong>：CRM 具有显著的泛化潜力，可处理低分辨率训练和超高清测试之间的分辨率差距，即使从低分辨率细化到高分辨率，总推理时间也不到 CascadePSP 的一半。</li><li><strong>未来展望</strong>：目前采用“低分辨率训练和超高清测试”的配置，使用超高清图像进行训练和测试仍耗资源，解决该问题将是未来工作方向。</li></ol><p><strong>不足：</strong></p><ol><li><strong>训练资源限制</strong>：目前采用“低分辨率训练和超高分辨率测试”的配置，使用超高分辨率图像进行训练和测试仍面临资源消耗大的问题，这限制了模型在实际应用中对超高分辨率数据的处理能力。</li><li><strong>未来工作待明确</strong>：如使用预训练或低分辨率训练测试。</li></ol></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/High_Quality_Segmentation.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-03-06T02:07:25.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/SED.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>基于分层编码器的开放词汇语义分割</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>夜间场景语义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"Du2lL3IT\",\"column_paper_dgss.md\":\"CbyojzLm\",\"column_paper_high_quality_segmentation.md\":\"CFEq4ahO\",\"column_paper_night-time_semantic_segmentation.md\":\"D3__wCKw\",\"column_paper_pat.md\":\"CWCc_V88\",\"column_paper_sed.md\":\"uhEGJNof\",\"column_paper_segment anything.md\":\"CHzTsHvx\",\"column_paper_visual studio code latex.md\":\"Dlw4ykQG\",\"column_paper_index.md\":\"xFsQFYkx\",\"column_pytorch_index.md\":\"DJeFfLlo\",\"column_deeplearning_index.md\":\"ZDdJkEzX\",\"column_image_segmentation_20250224-语义分割概述.md\":\"CCaJEfuu\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"CsEduP9a\",\"column_image_segmentation_fcn模型讲解.md\":\"DaPpN4km\",\"column_image_segmentation_index.md\":\"DlE35Yyh\",\"column_image_segmentation_图像分割基础.md\":\"C-RWntn0\",\"column_image_segmentation_语义分割基础模型.md\":\"BL9q-hoI\",\"index.md\":\"fKJsbWXg\",\"markdown-examples.md\":\"BR1j5ldH\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"Pytorch笔记\",\"link\":\"/column/Pytorch/\"},{\"text\":\"深度学习笔记\",\"link\":\"/column/deepLearning/\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>