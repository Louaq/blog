<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.DjkTm10l.css" as="style">
    
    <script type="module" src="/blog/assets/app.BAcATsh1.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.CLo04awk.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.BJMxo-8a.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_A Transformer-basedAdaptivePrototypeMatchingNetwork.md.CGGHGbc7.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><canvas style="position:fixed;left:0;top:0;pointer-events:none;z-index:999999;" data-v-23bac76b></canvas><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Pytorch/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>Pytorch笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/deepLearning/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>深度学习笔记</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_A%20Transformer-basedAdaptivePrototypeMatchingNetwork" data-v-35102dec><div><h1 id="a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation" tabindex="-1">A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation <a class="header-anchor" href="#a-transformer-based-adaptive-prototype-matching-network-for-few-shot-semantic-segmentation" aria-label="Permalink to &quot;A Transformer-based Adaptive Prototype Matching Network for Few-Shot Semantic Segmentation&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 3/20/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><h2 id="南京信息工程大学、青海师范大学、澳门大学、中国科学院" tabindex="-1">南京信息工程大学、青海师范大学、澳门大学、中国科学院 💯 <a class="header-anchor" href="#南京信息工程大学、青海师范大学、澳门大学、中国科学院" aria-label="Permalink to &quot;南京信息工程大学、青海师范大学、澳门大学、中国科学院  :100:&quot;">​</a></h2><h2 id="摘要" tabindex="-1"><strong>摘要：</strong> <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;**摘要：**&quot;">​</a></h2><blockquote><p><strong>Few-shot semantic segmentation (FSS)</strong> aims to generate a model for segmenting novel classes using a limited number of annotated samples. Previous FSS methods have shown sensitivity to background noise due to inherent bias, attention bias, and spatial-aware bias. In this study, we propose a <strong>Transformer-Based Adaptive Prototype Matching Network</strong> to establish robust matching relationships by improving the semantic and spatial perception of query features. The model includes three modules: <strong>target enhancement module (TEM)</strong>, <strong>dual constraint aggregation module (DCAM)</strong>, and <strong>dual classification module (DCM)</strong>. In particular, TEM mitigates inherent bias by exploring the relevance of multi-scale local context to enhance foreground features. Then, DCAM addresses attention bias through the dual semantic-aware attention mechanism to strengthen constraints. Finally, the DCM module decouples the segmentation task into semantic alignment and spatial alignment to alleviate spatial-aware bias. Extensive experiments on <strong>PASCAL-5i</strong> and <strong>COCO-20i</strong> confirm the effectiveness of our approach.</p></blockquote><h2 id="翻译" tabindex="-1"><strong>翻译：</strong> <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;**翻译：**&quot;">​</a></h2><blockquote><p>Few-shot语义分割（FSS）旨在通过少量的标注样本为新的类别生成一个分割模型。以往的FSS方法由于固有偏差、注意力偏差和空间感知偏差，往往对背景噪声过于敏感。在本研究中，我们提出了一种基于Transformer的自适应原型匹配网络，通过增强查询特征的语义和空间感知能力，建立更为稳定的匹配关系。该模型包含三个模块：目标增强模块（TEM）、双重约束聚合模块（DCAM）和双重分类模块（DCM）。其中，TEM通过探索多尺度局部上下文的相关性，增强前景特征，从而减轻固有的偏差。接着，DCAM通过双重语义感知注意力机制解决了注意力偏差问题，强化了约束效果。最后，DCM模块将分割任务拆解为语义对齐和空间对齐，帮助缓解空间感知偏差。我们在PASCAL-5i和COCO-20i数据集上进行了大量实验，验证了该方法的有效性。</p></blockquote><h2 id="研究背景" tabindex="-1"><strong>研究背景：</strong> <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;**研究背景：**&quot;">​</a></h2><p>近年来，由于深度学习在计算机视觉领域的快速发展，所以传统的语义分割取得了飞速进步。在这种情况下，少样本分割(few-shot segmentation, FSS)被提出用于模拟有限数据和多类别的真实世界场景。</p><p>FSS遵循元学习框架，执行过程分特征提取、匹配和分类三个阶段。现有FSS模型虽有成果，但受背景干扰，存在三方面问题：一是特征提取阶段，预训练骨干网络有固有偏差，易优先提取无关特征；二是特征匹配阶段，注意力机制在目标类别内差异大时，会导致注意力偏差；三是分类阶段，现有方法多依赖语义相关性，忽略空间信息，产生空间感知偏差。</p><p>基于上述问题，作者提出一种基于Transformer的自适应原型匹配网络，通过在模型执行的三个阶段进行策略性和高效交互，减轻FSS中的背景干扰，利用查询特征的语义和空间感知，增强模型的鲁棒性，以解决现有FSS模型存在的问题。</p><h2 id="研究现状" tabindex="-1"><strong>研究现状：</strong> <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;**研究现状：**&quot;">​</a></h2><ul><li><strong>Few - Shot Semantic Segmentation（FSS）</strong>：FSS旨在用<strong>少量标注样本</strong>为新类别生成分割模型，基于度量学习的FSS主要分为基于原型和基于像素匹配两类方法。<strong>基于原型的方法</strong>用原型代表目标类信息进行匹配预测；<strong>基于像素匹配</strong>的方法建立支持像素和查询像素的密集关联。</li><li><strong>Transformer应用</strong>：Transformer因能捕捉长距离相关性，在FSS中得到应用，如动态调整分类器权重、过滤无关像素、聚合多级别支持掩码等。</li></ul><h2 id="提出的模型" tabindex="-1"><strong>提出的模型：</strong> <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;**提出的模型：**&quot;">​</a></h2><p>本文提出了一种基于Transformer的自适应原型匹配网络（Transformer - Based Adaptive Prototype Matching Network），用于<strong>少样本语义分割（Few - Shot Semantic Segmentation，FSS）<strong>任务，以解决现有FSS模型存在的</strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的对背景噪声敏感的问题。该模型主要包含以下三个模块：</p><ol><li><blockquote><p><strong>目标增强模块（Target Enhancement Module，TEM）</strong> <strong>设计目的</strong>：缓解骨干网络的<strong>固有偏差</strong>，增强前景特征。在特征提取阶段，以往工作依赖预训练骨干网络直接提取的特征，存在固有偏差，倾向于提取与当前任务无关的特征。 <strong>具体方法</strong>：引入基于卷积Transformer架构的多尺度局部感知调制Transformer进行多尺度特征提取，采用多尺度自适应局部注意力增强前景信息、减轻背景干扰；用可逆神经网络（INN）替代标准多层感知器（MLP），在前馈过程中保留更细粒度的特征。</p></blockquote></li><li><blockquote><p><strong>双约束聚合模块（Dual Constraint Aggregation Module，DCAM）</strong> <strong>设计目的</strong>：解决特征匹配阶段的<strong>注意力偏差</strong>问题。现有方法利用单层注意力机制建立支持集和查询集的关系，在目标类别存在显著类内差异时，这种关系不足以准确匹配，导致注意力偏差。 <strong>具体方法</strong>：由类内差异表示和双语义感知注意力机制两个关键部分组成。类内差异表示利用一组可学习向量建模支持集和查询集之间的差异；双语义感知注意力机制通过两层约束，先以支持原型为参考在查询特征中选择匹配置信度高的点，再以此为指导在整个查询特征图中寻找特征相似度高的点，生成鲁棒的支持类别原型。</p></blockquote></li><li><blockquote><p><strong>双分类模块（Dual Classification Module，DCM）</strong> <strong>设计目的</strong>：解决特征分类阶段的<strong>空间感知偏差</strong>问题。现有方法主要基于语义一致性进行预测，忽略了目标对象的空间一致性，导致难以准确定位目标类别。 <strong>具体方法</strong>：将分割任务解耦为语义对齐和空间对齐两个子任务。通过优化查询特征和类别原型生成基于语义相似度的掩码来识别目标类别；利用查询特征的内在引导，挖掘目标对象自身的空间一致性，得到基于空间分布概率的掩码用于精确的定位，最后将两个掩码相加得到最终的查询前景分割图。 实验结果表明，该模型在PASCAL - 5i和COCO - 20i两个基准数据集上取得了优于现有方法的性能，且参数数量较少。</p></blockquote></li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-36-07.png" alt="Snipaste_2025-03-08_10-36-07" loading="lazy"></p><h2 id="实验-compared-with-the-state-of-the-art-models-and-ablation-experiments" tabindex="-1"><strong>实验（compared with the state-of-the-art models and ablation experiments）</strong> <a class="header-anchor" href="#实验-compared-with-the-state-of-the-art-models-and-ablation-experiments" aria-label="Permalink to &quot;**实验（compared with the state-of-the-art models and ablation experiments）**&quot;">​</a></h2><ul><li><h3 id="comparison-with-the-state-of-the-arts" tabindex="-1"><strong>Comparison with the State-of-the-Arts</strong> <a class="header-anchor" href="#comparison-with-the-state-of-the-arts" aria-label="Permalink to &quot;**Comparison with the State-of-the-Arts**&quot;">​</a></h3></li></ul><p>数据集：PASCAL-5<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0;" xmlns="http://www.w3.org/2000/svg" width="0.74ex" height="1.879ex" role="img" focusable="false" viewBox="0 -830.4 327 830.4" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mi" transform="translate(33,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi>i</mi></msup></mrow></math></mjx-assistive-mml></mjx-container>，COCO-20<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0;" xmlns="http://www.w3.org/2000/svg" width="0.74ex" height="1.879ex" role="img" focusable="false" viewBox="0 -830.4 327 830.4" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mi" transform="translate(33,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi>i</mi></msup></mrow></math></mjx-assistive-mml></mjx-container></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-39-17.png" alt="Snipaste_2025-03-08_10-39-17" loading="lazy"></p><ul><li><h3 id="ablation-experiments" tabindex="-1"><strong>ablation experiments</strong> <a class="header-anchor" href="#ablation-experiments" aria-label="Permalink to &quot;**ablation experiments**&quot;">​</a></h3></li></ul><ol><li><strong>组件分析</strong>：该方法包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）三个主要模块。与基线相比，单独使用TEM增强查询前景特征可使性能提升0.9%，单独使用DCAM增强类别原型的判别能力可提升2.2%，TEM和DCAM协同作用可提升2.6%，使用DCM实现语义对齐和空间对齐可额外提升1.3%。模型整体比基线提升了3.9%，表明引入的模块有效解决了固有偏差、注意力偏差和空间感知偏差问题，减少了背景干扰，实现了精确分割。</li><li><strong>目标增强模块（TEM）</strong>：TEM旨在减轻骨干网络的固有偏差并增强查询前景区域。通过与其他方法在计算量和准确性方面进行对比实验，包括采用自对齐模块（SA）、卷积变压器架构（SAM）、多尺度自适应局部注意力（MSLA + MLP）以及用可逆神经网络（INN）代替多层感知器（MLP）作为前馈网络（MSLA + INN）。结果表明，该方法在降低计算复杂度的同时保持了较高的准确性，且前馈网络在略微增加计算成本的情况下保留了更多特征细节。</li><li><strong>双约束聚合模块（DCAM）</strong>：对DCAM中的关键组件进行了全面分析，通过修改模型采用不同的注意力机制，如原始的普通注意力（VA）、掩码注意力（MA）、双语义感知注意力（DSAA）和类内差异表示（IDR）。结果显示，使用掩码注意力减轻背景噪声干扰对性能提升影响不大，因为支持集和查询集之间的相似度掩码在类内差异较大时准确性存在挑战。而双语义感知注意力机制通过可学习的方式减轻背景干扰，能应对类内差异的敏感性，类内差异表示在三种不同的注意力机制中都有益。</li><li><strong>双分类模块（DCM）</strong>：通过消融实验评估不同的DCM组件。仅使用基于语义相似度的掩码可使模型性能提升1.5%，证明了优化类别原型和查询特征的必要性；仅使用基于空间分布概率的分割图时，性能下降2.3%，这是因为仅依赖查询图像本身的前景分布会使模型偏向已知类别的区域，导致对未知类别的分割失败。</li></ol><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-19.png" alt="Snipaste_2025-03-08_10-41-19" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-03-08_10-41-25.png" alt="Snipaste_2025-03-08_10-41-25" loading="lazy"></p><h2 id="结论" tabindex="-1"><strong>结论：</strong> <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;**结论：**&quot;">​</a></h2><blockquote><p>作者提出了一种<strong>基于Transformer的自适应原型匹配网络</strong>，以应对少样本语义分割（FSS）中<strong>固有偏差、注意力偏差和空间感知偏差</strong>导致的背景干扰问题。该网络包含目标增强模块（TEM）、双约束聚合模块（DCAM）和双分类模块（DCM）。TEM通过多尺度局部上下文相关性增强前景特征，解决固有偏差；DCAM利用双语义感知注意力机制加强约束，处理注意力偏差；DCM将分割任务解耦为语义对齐和空间对齐，缓解空间感知偏差。实验表明，该方法在PASCAL - 5i和COCO - 20i数据集上以最少的参数达到了最先进的性能，有效减少了背景干扰，实现了精确分割。</p></blockquote></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-03-20T08:41:39.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>多模态图像分割</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>跨领域少样本语义分割</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"DCz0nzcS\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"CGGHGbc7\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"BE1P3xgB\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"klBZUhR8\",\"column_paper_dgss.md\":\"DGlr4r4p\",\"column_paper_high_quality_segmentation.md\":\"CIDAIMLw\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"MzCl3yWx\",\"column_paper_night-time_semantic_segmentation.md\":\"BFhkiy-p\",\"column_paper_pat.md\":\"B9cJ_6yy\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"Byq3St39\",\"column_paper_prompting_multi-moda_segmetation.md\":\"CuopTGIf\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"C7v_A7dA\",\"column_paper_sed.md\":\"DfoHBQl1\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"N0RIoEXo\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"5ZJ-MrQl\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"C447nHVS\",\"column_paper_segment anything.md\":\"DJd_k7Ii\",\"column_paper_self-supervised_vit.md\":\"KNZnsQA_\",\"column_paper_visual studio code latex.md\":\"D5hSUgfz\",\"column_paper_index.md\":\"DW9SfzjO\",\"column_pytorch_index.md\":\"DplE3QeN\",\"column_deeplearning_index.md\":\"Cit7AgNl\",\"column_image_segmentation_20250224-语义分割概述.md\":\"B0nh4hMe\",\"column_image_segmentation_20250312-pytorch教程.md\":\"B9axxj1Q\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"DW6rBbS0\",\"column_image_segmentation_fcn模型讲解.md\":\"ChsDp3zP\",\"column_image_segmentation_index.md\":\"vwDPmO9L\",\"column_image_segmentation_图像分割基础.md\":\"DS8RzhX2\",\"column_image_segmentation_语义分割基础模型.md\":\"B8QH0VsE\",\"index.md\":\"_n_wNgUN\",\"markdown-examples.md\":\"BM745lml\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"Pytorch笔记\",\"link\":\"/column/Pytorch/\"},{\"text\":\"深度学习笔记\",\"link\":\"/column/deepLearning/\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"},{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>