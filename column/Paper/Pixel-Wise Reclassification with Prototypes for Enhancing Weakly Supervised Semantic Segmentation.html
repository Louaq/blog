<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation | lab blog</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.2.2">
    <link rel="preload stylesheet" href="/blog/assets/style.DjOpM9sR.css" as="style">
    
    <script type="module" src="/blog/assets/app.DhGckwQj.js"></script>
    <link rel="preload" href="/blog/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/blog/assets/chunks/framework.CLo04awk.js">
    <link rel="modulepreload" href="/blog/assets/chunks/theme.B-Vs77GU.js">
    <link rel="modulepreload" href="/blog/assets/column_Paper_Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation.md.wjFNBiTi.lean.js">
    <link rel="icon" href="/blog/icon.png">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <script id="register-sw">"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
    <script>typeof LA<"u"&&LA.init({id:"3LPXyA1ZitpV3O1s",ck:"3LPXyA1ZitpV3O1s",autoTrack:!0,hashMode:!0});</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-f10dfbfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-990bc0c7></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-990bc0c7> Skip to content </a><!--]--><!----><header class="VPNav" data-v-f10dfbfe data-v-a3d208eb><div class="VPNavBar has-sidebar top" data-v-a3d208eb data-v-b617171c><div class="wrapper" data-v-b617171c><div class="container" data-v-b617171c><div class="title" data-v-b617171c><div class="VPNavBarTitle has-sidebar" data-v-b617171c data-v-40e0514d><a class="title" href="/blog/" data-v-40e0514d><!--[--><!--]--><!--[--><img class="VPImage logo" src="/blog/b.jpg" alt data-v-72d3edbf><!--]--><span data-v-40e0514d>lab blog</span><!--[--><!--]--></a></div></div><div class="content" data-v-b617171c><div class="content-body" data-v-b617171c><!--[--><!--]--><div class="VPNavBarSearch search" data-v-b617171c><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-b617171c data-v-9189896b><span id="main-nav-aria-label" class="visually-hidden" data-v-9189896b>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Paper/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文阅读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/Puruse/" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>论文精读笔记</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/column/image_segmentation.html" tabindex="0" data-v-9189896b data-v-c2001aa6><!--[--><span data-v-c2001aa6>图像分割</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-b617171c data-v-cdb349b7><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-cdb349b7 data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-b617171c data-v-81c4a4eb data-v-3cb196e4><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3cb196e4><span class="vpi-more-horizontal icon" data-v-3cb196e4></span></button><div class="menu" data-v-3cb196e4><div class="VPMenu" data-v-3cb196e4 data-v-e035cd6c><!----><!--[--><!--[--><!----><div class="group" data-v-81c4a4eb><div class="item appearance" data-v-81c4a4eb><p class="label" data-v-81c4a4eb>深浅模式</p><div class="appearance-action" data-v-81c4a4eb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-81c4a4eb data-v-6a6948a0 data-v-33c43c25><span class="check" data-v-33c43c25><span class="icon" data-v-33c43c25><!--[--><span class="vpi-sun sun" data-v-6a6948a0></span><span class="vpi-moon moon" data-v-6a6948a0></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-b617171c data-v-9f10abfc><span class="container" data-v-9f10abfc><span class="top" data-v-9f10abfc></span><span class="middle" data-v-9f10abfc></span><span class="bottom" data-v-9f10abfc></span></span></button></div></div></div></div><div class="divider" data-v-b617171c><div class="divider-line" data-v-b617171c></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-f10dfbfe data-v-c91f5bc2><div class="container" data-v-c91f5bc2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-c91f5bc2><span class="vpi-align-left menu-icon" data-v-c91f5bc2></span><span class="menu-text" data-v-c91f5bc2>目录</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-c91f5bc2 data-v-32c38b9b><button data-v-32c38b9b>返回顶部</button><!----></div></div></div><aside class="VPSidebar" data-v-f10dfbfe data-v-0f776768><div class="curtain" data-v-0f776768></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-0f776768><span class="visually-hidden" id="sidebar-aria-label" data-v-0f776768> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible collapsed" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Visual%20Studio%20Code%20latex.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>latex环境配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Segment%20Anything.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>Segment Anything</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DGSS.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SED.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于分层编码器的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/High_Quality_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>超高分辨率分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Night-time_Semantic_Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>夜间场景语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/PAT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>提示词迁移的少样本分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Prompting_Multi-Moda_Segmetation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>多模态图像分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/A%20Transformer-basedAdaptivePrototypeMatchingNetwork.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于Transformer的自适应原型匹配网络</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Cross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Doubly%20Matching%20Transformation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>跨领域少样本语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Relevant%20Intrinsic%20Feature%20Enhancement%20Network%20for%20Few-Shot%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>相关内在特征增强的少样本与意义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribble-Supervised%20Semantic%20Segmentation%20with%20Prototype-based%20Feature%20Augmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的无监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Progressive%20Feature%20Self-Reinforcement%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向弱监督语义分割的渐进式特征自增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Self-supervised_ViT.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于自监督Vit的语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>医学图像分割：基于解耦特征查询</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于语句嵌入的多领域语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with%20Its%20Class%20Label.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于涂鸦的弱监督语义分割</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="group" data-v-0f776768><section class="VPSidebarItem level-0 collapsible has-active" data-v-0f776768 data-v-7f773a2d><div class="item" role="button" tabindex="0" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><h2 class="text" data-v-7f773a2d>语义分割论文阅读</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-7f773a2d><span class="vpi-chevron-right caret-icon" data-v-7f773a2d></span></div></div><div class="items" data-v-7f773a2d><!--[--><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/CC4S%20Encouraging%20Certainty%20and%20Consistency%20in%20Scribble-Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>涂鸦监督语义分割的确定性和一致性(CC4S)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/CorrMatch%20Label%20Propagation%20via%20Correlation%20Matching%20for%20Semi-Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于关联匹配的标签传播半监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LLMFormer%20Large%20LanguageModel%20for%20Open-Vocabulary%20Semantic.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于LLM的开放词汇语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Towards%20Open-Vocabulary%20Semantic%20Segmentation%20Without%20Semantic%20Labels.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/USE%20Universal%20Segment%20Embeddings%20for%20Open-Vocabulary%20Image%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向开放词汇图像分割的通用片段嵌入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Class%20Tokens%20Infusion%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>面向弱监督语义分割的类别标记注入</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/LGAD%20Local%20and%20Global%20Attention%20Distillation%20for%20Efficient%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于全局和局部注意力蒸馏的高效语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/DSMF-Net%20Dual%20Semantic%20Metric%20Learning%20Fusion%20Network%20for%20Few-Shot%20Aerial%20Image%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于双重语义度量学习的少样本航拍图像语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Kill%20Two%20Birds%20with%20One%20Stone%20Domain%20Generalization%20for%20Semantic%20Segmentation%20via%20Network%20Pruning.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于剪枝的领域泛化语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/All-pairs%20Consistency%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于全对一致性学习的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/WeakCLIP%20Adapting%20CLIP%20for%20Weakly-Supervised%20Semantic.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>weakCLIP</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/SFC%20Shared%20Feature%20Calibration%20in%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>弱监督语义分割中的共享权重校准</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于模拟图像间擦除知识迁移的弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>基于原型的像素级再分类提高弱监督语义分割</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>C-CAM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-7f773a2d data-v-7f773a2d><div class="item" data-v-7f773a2d><div class="indicator" data-v-7f773a2d></div><a class="VPLink link link" href="/blog/column/Paper/Rolling-Unet%20Revitalizing%20MLP%20Ability%20to%20Efficiently%20Extract%20Long-Distance%20Dependencies%20for%20Medical%20Image%20Segmentation.html" data-v-7f773a2d><!--[--><p class="text" data-v-7f773a2d>医学图像分割：Rolling-Net</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-f10dfbfe data-v-5dff0740><div class="VPDoc has-sidebar has-aside" data-v-5dff0740 data-v-35102dec><!--[--><!--]--><div class="container" data-v-35102dec><div class="aside" data-v-35102dec><div class="aside-curtain" data-v-35102dec></div><div class="aside-container" data-v-35102dec><div class="aside-content" data-v-35102dec><div class="VPDocAside" data-v-35102dec data-v-a63ed339><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-a63ed339 data-v-315f7e94><div class="content" data-v-315f7e94><div class="outline-marker" data-v-315f7e94></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-315f7e94>当前大纲</div><ul class="VPDocOutlineItem root" data-v-315f7e94 data-v-4f398093><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-a63ed339></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-35102dec><div class="content-container" data-v-35102dec><!--[--><!--]--><main class="main" data-v-35102dec><div style="position:relative;" class="vp-doc _blog_column_Paper_Pixel-Wise%20Reclassification%20with%20Prototypes%20for%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation" data-v-35102dec><div><h1 id="pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation" tabindex="-1">Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation <a class="header-anchor" href="#pixel-wise-reclassification-with-prototypes-for-enhancing-weakly-supervised-semantic-segmentation" aria-label="Permalink to &quot;Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation&quot;">​</a></h1><div class="word"><p><svg t="1724572866572" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="18131" width="16" height="16"><path d="M168.021333 504.192A343.253333 343.253333 0 0 1 268.629333 268.8a342.229333 342.229333 0 0 1 243.285334-100.778667A341.504 341.504 0 0 1 755.029333 268.8c9.856 9.898667 19.2 20.394667 27.733334 31.402667l-60.16 46.976a8.021333 8.021333 0 0 0 2.986666 14.122666l175.701334 43.008a8.021333 8.021333 0 0 0 9.898666-7.68l0.810667-180.906666a7.936 7.936 0 0 0-12.885333-6.314667L842.666667 253.44a418.858667 418.858667 0 0 0-330.922667-161.493333c-229.12 0-415.488 183.594667-419.797333 411.818666a8.021333 8.021333 0 0 0 8.021333 8.192H160a7.978667 7.978667 0 0 0 8.021333-7.808zM923.946667 512H864a7.978667 7.978667 0 0 0-8.021333 7.808 341.632 341.632 0 0 1-26.88 125.994667 342.186667 342.186667 0 0 1-73.685334 109.397333 342.442667 342.442667 0 0 1-243.328 100.821333 342.229333 342.229333 0 0 1-270.976-132.224l60.16-46.976a8.021333 8.021333 0 0 0-2.986666-14.122666l-175.701334-43.008a8.021333 8.021333 0 0 0-9.898666 7.68l-0.682667 181.034666c0 6.698667 7.68 10.496 12.885333 6.314667L181.333333 770.56a419.072 419.072 0 0 0 330.922667 161.408c229.205333 0 415.488-183.722667 419.797333-411.818667a8.021333 8.021333 0 0 0-8.021333-8.192z" fill="#8a8a8a" p-id="18132"></path></svg> 更新: 4/22/2025 <svg t="1724571760788" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6125" width="16" height="16"><path d="M204.8 0h477.866667l273.066666 273.066667v614.4c0 75.093333-61.44 136.533333-136.533333 136.533333H204.8c-75.093333 0-136.533333-61.44-136.533333-136.533333V136.533333C68.266667 61.44 129.706667 0 204.8 0z m307.2 607.573333l68.266667 191.146667c13.653333 27.306667 54.613333 27.306667 61.44 0l102.4-273.066667c6.826667-20.48 0-34.133333-20.48-40.96s-34.133333 0-40.96 13.653334l-68.266667 191.146666-68.266667-191.146666c-13.653333-27.306667-54.613333-27.306667-68.266666 0l-68.266667 191.146666-68.266667-191.146666c-6.826667-13.653333-27.306667-27.306667-47.786666-20.48s-27.306667 27.306667-20.48 47.786666l102.4 273.066667c13.653333 27.306667 54.613333 27.306667 61.44 0l75.093333-191.146667z" fill="#777777" p-id="6126"></path><path d="M682.666667 0l273.066666 273.066667h-204.8c-40.96 0-68.266667-27.306667-68.266666-68.266667V0z" fill="#E0E0E0" opacity=".619" p-id="6127"></path></svg> 字数: 0 字 <svg t="1724572797268" class="icon" viewBox="0 0 1060 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15031" width="16" height="16"><path d="M556.726857 0.256A493.933714 493.933714 0 0 0 121.929143 258.998857L0 135.021714v350.390857h344.649143L196.205714 334.482286a406.820571 406.820571 0 1 1-15.908571 312.649143H68.937143A505.819429 505.819429 0 1 0 556.726857 0.256z m-79.542857 269.531429v274.907428l249.197714 150.966857 42.422857-70.070857-212.114285-129.389714V269.787429h-79.542857z" fill="#8a8a8a" p-id="15032"></path></svg> 时长: 0 分钟 </p></div><p>天津大学</p><div class="tip custom-block"><p class="custom-block-title">TIP</p><p>启发、参考文献</p></div><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>Refining the seed region to obtain finely annotated <strong>pseudo masks</strong> for training a segmentation model is a crucial step in the multi-stage weakly supervised semantic segmentation (WSSS) framework. One of the most popular refinement methods, IRN, extends seed regions towards the edges in the image. However, we observed that, due to the lack of guidance from semantic information, IRN’s refinement may lead the generation of partially erroneous refinement directions. To address this issue, we leverage prototypes to recover the overlooked category semantic information in the refinement stage. We propose a prototype-based pseudo mask reclassification post-processing (PtReCl) to correct misclassified pixels in the pseudo masks, generating refined pseudo masks with more accurate coverage. Experimental evaluations demonstrate that our post-processing approach brings improvements in both pseudo mask quality and segmentation results on PASCAL VOC and MS COCO datasets, achieving state-of-the-art performance on VOC.</p><h2 id="翻译" tabindex="-1">翻译 <a class="header-anchor" href="#翻译" aria-label="Permalink to &quot;翻译&quot;">​</a></h2><p>在多阶段弱监督语义分割(WSSS)框架中，对种子区域进行细化以获得精细标注的<strong>伪掩膜</strong>用于训练分割模型是至关重要的一步。最流行的细化方法之一是IRN，它将种子区域向图像的边缘扩展。然而，我们观察到，由于缺乏语义信息的指导，IRN的细化可能导致部分错误的细化方向的产生。为了解决这个问题，我们利用原型来恢复细化阶段中被忽略的类别语义信息。我们提出了一种基于原型的伪掩码重分类后处理(PtReCl)来纠正伪掩膜中的错误分类像素，生成更精确覆盖的精细伪掩膜。实验评估表明，我们的后处理方法改善了<strong>PASCAL VOC</strong>和<strong>MS COCO</strong>数据集的伪掩码质量和分割结果，实现了最先进的VOC性能。</p><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><p>本文聚焦于弱监督语义分割（WSSS）领域，旨在解决现有方法在生成伪掩码时存在的问题，具体研究背景如下：</p><ul><li><strong>WSSS的目标与流程</strong>：WSSS旨在利用图像级标注数据集完成像素级分类任务，以降低数据标注成本。当前主流方法遵循三阶段流程，其中生成高质量伪掩码对最终分割模型的性能至关重要。</li><li><strong>现有方法的局限性</strong>：最常用的细化方法IRN在细化种子区域时，因缺乏语义信息指导，可能导致部分错误的细化方向，产生大量错误的伪掩码。</li><li><strong>原型学习的潜力</strong>：近年来，研究发现原型学习可助力语义分割，它能从少量类样本中归纳特定类别的特征，实现特征的像素级分类，还能保留更多非学习参数以预测多样特征。</li><li><strong>本文的研究动机</strong>：基于上述背景，作者提出基于<strong>原型的伪掩码重分类后处理方法</strong>（<strong>PtReCl</strong>），利用原型的类别区分性恢复伪掩码中误分类的像素，以提高伪掩码质量和分割性能。</li></ul><h2 id="研究现状" tabindex="-1">研究现状 <a class="header-anchor" href="#研究现状" aria-label="Permalink to &quot;研究现状&quot;">​</a></h2><ul><li><strong>多阶段WSSS框架</strong>：主流方法分三步，先训练分类模型生成种子区域，再用细化方法生成伪掩码，最后用伪掩码训练全监督语义分割模型。</li><li><strong>CAM方法</strong>：解决CAM作为种子区域时前景覆盖不足问题，如采用擦除、对抗学习、利用ViT上下文建模等方法。</li><li><strong>细化方法</strong>：主要分为利用显著性检测和随机游走与语义亲和两类，部分方法还借助Transformer中的注意力矩阵。</li><li><strong>原型学习</strong>：在语义分割中，部分研究将原型用于对比学习或自监督学习，部分用原型替换分类器结构。</li></ul><h2 id="提出的模型" tabindex="-1">提出的模型 <a class="header-anchor" href="#提出的模型" aria-label="Permalink to &quot;提出的模型&quot;">​</a></h2><p>多阶段弱监督语义分割框架中，细化种子区域以获得精细注释的伪掩码是训练分割模型的关键步骤。现有流行的细化方法IRN在细化过程中缺乏语义信息的引导，可能导致部分错误的细化方向。为解决这一问题，作者提出了<strong>PtReCl</strong>方法。</p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-21-22.png" alt="Snipaste_2025-04-13_19-21-22" loading="lazy"></p><h3 id="模型流程" tabindex="-1">模型流程 <a class="header-anchor" href="#模型流程" aria-label="Permalink to &quot;模型流程&quot;">​</a></h3><ol><li><strong>种子区域获取</strong>：利用原始的类激活映射（Class Activation Maps, CAM）方法获取种子区域。训练分类模型后，丢弃分类器中的全局平均池化（Global Average Pooling, GAP）层，直接在原始特征图上进行预测，忽略负预测分数并归一化生成CAM。</li><li><strong>伪掩码生成</strong>：使用IRN方法对种子区域进行细化，生成伪掩码。</li><li><strong>伪掩码恢复网络</strong>：参考Deeplab的结构构建伪分割网络，以伪掩码作为像素级注释，通过空洞空间金字塔池化层（Atrous Spatial Pyramid Pooling, ASPP）提取图像特征并获得像素级预测结果。引入标签条件策略（Label Conditioning strategy），根据图像级类别注释保留相关通道，丢弃无关通道，以减轻无关通道对后续原型准确性的影响。</li><li><strong>前景 - 背景原型获取</strong>：依次遍历训练集图像，使用骨干网络提取特征。对于伪掩码中每个类别的前景区域，收集其对应特征到前景特征集；对于非该类别区域，收集其对应特征到背景特征集。使用余弦距离作为度量，采用K - means聚类方法为每个类别获取多个前景和背景原型。</li><li><strong>多原型像素级重新分类</strong>：使用伪掩码恢复网络的骨干提取图像特征，利用特定类别的前景和背景原型对像素特征的语义信息进行重新分类。计算每个位置与前景 - 背景原型的余弦相似度，对相似度进行降序排序，选择前m个距离参与像素分类计算，生成像素级重新分类图。</li><li><strong>重新细化</strong>：将重新分类图替换IRN中的CAM，再次使用IRN进行细化，增强其边缘信息，得到后处理的伪掩码。</li><li><strong>全监督语义分割</strong>：使用后处理的伪掩码训练全监督语义分割模型，如DeeplabV2和UperNet - Swin。</li></ol><h3 id="模型贡献" tabindex="-1">模型贡献 <a class="header-anchor" href="#模型贡献" aria-label="Permalink to &quot;模型贡献&quot;">​</a></h3><ul><li><strong>解决分类错误</strong>：提出PtReCl后处理方法，利用原型的类别区分性，通过前景 - 背景特征恢复伪掩码中误分类的像素。</li><li><strong>多原型分类</strong>：设计多原型像素级分类方法，利用伪分割网络重建伪掩码并通过聚类方法获取原型，缓解不同类别有效原型数量的差异，获得准确的重新分类图。</li><li><strong>实验验证</strong>：在PASCAL VOC和MS COCO数据集上进行了广泛实验，结果表明PtReCl方法能有效提高伪掩码的准确性，从而提升分割性能，在VOC数据集上取得了最先进的结果。</li></ul><h2 id="实验-compared-with-sota" tabindex="-1">实验（Compared with SOTA） <a class="header-anchor" href="#实验-compared-with-sota" aria-label="Permalink to &quot;实验（Compared with SOTA）&quot;">​</a></h2><p>数据集：<strong>PASCAL VOC 2012</strong>、<strong>MS COCO 2014</strong></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-25-57.png" alt="Snipaste_2025-04-13_19-25-57" loading="lazy"></p><ul><li><strong>伪掩码增强</strong>：与一些先进的WSSS方法相比，经PtReCl处理后的伪掩码在VOC上提升了8.4%，在COCO上提升了3.4%，在VOC上取得了最佳性能，在COCO上也有出色表现。</li><li><strong>分割性能提升</strong>：在使用DeepLab作为全监督分割方法的VOC实验中，PtReCl在两种常用预训练ResNet101骨干网络下均取得了最先进的结果。在基于Transformer的分割方法中，使用UperNet - Swin作为骨干网络时，PtReCl也达到了最先进的性能。在COCO上，尽管受噪声影响，PtReCl仍优于除AMN和LPCAM外的其他方法，与基线IRN相比，在验证集上提升了2.2%。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-26-36.png" alt="Snipaste_2025-04-13_19-26-36" loading="lazy"></p><h2 id="实验-ablation-experiments" tabindex="-1">实验（Ablation Experiments）🥇 <a class="header-anchor" href="#实验-ablation-experiments" aria-label="Permalink to &quot;实验（Ablation Experiments）:1st_place_medal:&quot;">​</a></h2><ul><li><strong>有效性验证</strong>：PtReCl在VOC和COCO上分别将伪掩码的mIoU提高了8.4%和3.4%。通过对比不使用原型和使用不同数量原型时的像素级分类结果，验证了多原型像素级分类方法的有效性，当M设为[10, 15, 20]时，重分类图的mIoU最高可达70%。</li><li><strong>原型数量影响</strong>：研究了调整每个类别的原型数量K（范围从2到30）对重分类效果的影响。重分类图的mIoU随原型数量增加先上升后稳定，最终将类中心数量设为20。在10到30的范围内，重分类图的mIoU波动仅在1%以内，表明在合理范围内改变原型数量对重分类效果影响不大。</li></ul><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-20.png" alt="Snipaste_2025-04-13_19-28-20" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-28.png" alt="Snipaste_2025-04-13_19-28-28" loading="lazy"></p><p><img src="https://yangyang666.oss-cn-chengdu.aliyuncs.com/images/Snipaste_2025-04-13_19-28-33.png" alt="Snipaste_2025-04-13_19-28-33" loading="lazy"></p><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>作者指出广泛使用的<strong>WSSS</strong>方法IRN在细化策略上存在局限，它在不考虑特定像素级语义信息的情况下将种子区域向图像边缘扩展，导致部分错误细化。基于现有的WSSS三阶段框架，作者引入了基于原型的重分类后处理方法，以纠正伪掩码中的像素错误分类，得到更精确的后处理伪掩码。 通过在<strong>VOC和COCO</strong>数据集上的大量实验，结果表明该后处理阶段有效提高了伪掩码的质量和分割模型的性能，在VOC数据集上取得了最先进的成果。</p></div></div></main><footer class="VPDocFooter" data-v-35102dec data-v-993905e5><!--[--><!--[--><!--[--><!--[--><div style="" class="vitepress-backTop-main" title="返回顶部" data-v-16856a25><svg t="1720595052079" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4279" width="200" height="200" data-v-16856a25><path d="M752.736 431.063C757.159 140.575 520.41 8.97 504.518 0.41V0l-0.45 0.205-0.41-0.205v0.41c-15.934 8.56-252.723 140.165-248.259 430.653-48.21 31.457-98.713 87.368-90.685 184.074 8.028 96.666 101.007 160.768 136.601 157.287 35.595-3.482 25.232-30.31 25.232-30.31l12.206-50.095s52.47 80.569 69.304 80.528c15.114-1.23 87-0.123 95.6 0h0.82c8.602-0.123 80.486-1.23 95.6 0 16.794 0 69.305-80.528 69.305-80.528l12.165 50.094s-10.322 26.83 25.272 30.31c35.595 3.482 128.574-60.62 136.602-157.286 8.028-96.665-42.475-152.617-90.685-184.074z m-248.669-4.26c-6.758-0.123-94.781-3.359-102.891-107.192 2.95-98.714 95.97-107.438 102.891-107.93 6.964 0.492 99.943 9.216 102.892 107.93-8.11 103.833-96.174 107.07-102.892 107.192z m-52.019 500.531c0 11.838-9.42 21.382-21.012 21.382a21.217 21.217 0 0 1-21.054-21.34V821.74c0-11.797 9.421-21.382 21.054-21.382 11.591 0 21.012 9.585 21.012 21.382v105.635z m77.333 57.222a21.504 21.504 0 0 1-21.34 21.626 21.504 21.504 0 0 1-21.34-21.626V827.474c0-11.96 9.543-21.668 21.299-21.668 11.796 0 21.38 9.708 21.38 21.668v157.082z m71.147-82.043c0 11.796-9.42 21.34-21.053 21.34a21.217 21.217 0 0 1-21.013-21.34v-75.367c0-11.755 9.421-21.299 21.013-21.299 11.632 0 21.053 9.544 21.053 21.3v75.366z" fill="#FFF" p-id="4280" data-v-16856a25></path></svg></div><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-993905e5><div class="edit-link" data-v-993905e5><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/Louaq/blog/tree/main/docs/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation.md" target="_blank" rel="noreferrer" data-v-993905e5><!--[--><span class="vpi-square-pen edit-link-icon" data-v-993905e5></span> 在github上编辑此页面<!--]--></a></div><div class="last-updated" data-v-993905e5><p class="VPLastUpdated" data-v-993905e5 data-v-47822a2b>最后更新于: <time datetime="2025-04-22T12:21:37.000Z" data-v-47822a2b></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-993905e5><span class="visually-hidden" id="doc-footer-aria-label" data-v-993905e5>Pager</span><div class="pager" data-v-993905e5><a class="VPLink link pager-link prev" href="/blog/column/Paper/Knowledge%20Transfer%20with%20Simulated%20Inter-Image%20Erasing%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>上一页</span><span class="title" data-v-993905e5>基于模拟图像间擦除知识迁移的弱监督语义分割</span><!--]--></a></div><div class="pager" data-v-993905e5><a class="VPLink link pager-link next" href="/blog/column/Paper/C-CAM%20Causal%20CAM%20for%20Weakly%20Supervised%20Semantic%20Segmentation%20on%20Medical%20Image.html" data-v-993905e5><!--[--><span class="desc" data-v-993905e5>下一页</span><span class="title" data-v-993905e5>C-CAM</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-f10dfbfe data-v-8682a75c><div class="container" data-v-8682a75c><p class="message" data-v-8682a75c>Released under the <a href="https://mit-license.org/">MIT License.</a> | 
    本站访客数 <span id="busuanzi_value_site_uv"></span> 人次</p><p class="copyright" data-v-8682a75c>Copyright © 2024-2025</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"gpE41Exe\",\"column_paper_a transformer-basedadaptiveprototypematchingnetwork.md\":\"CQuFUXKh\",\"column_paper_all-pairs consistency learning for weakly supervised semantic segmentation.md\":\"HRiYH1DS\",\"column_paper_c-cam causal cam for weakly supervised semantic segmentation on medical image.md\":\"CKrpD2Hx\",\"column_paper_cc4s encouraging certainty and consistency in scribble-supervised semantic segmentation.md\":\"BTLHgYRx\",\"column_paper_class tokens infusion for weakly supervised semantic segmentation.md\":\"D-c4f3x1\",\"column_paper_corrmatch label propagation via correlation matching for semi-supervised semantic segmentation.md\":\"CkXJRb9i\",\"column_paper_cross-domain few-shot semantic segmentation via doubly matching transformation.md\":\"DyQi6RE_\",\"column_paper_dgss.md\":\"DQlshvPk\",\"column_paper_dsmf-net dual semantic metric learning fusion network for few-shot aerial image semantic segmentation.md\":\"IvpABnks\",\"column_paper_high_quality_segmentation.md\":\"BdXVaoZ3\",\"column_paper_kill two birds with one stone domain generalization for semantic segmentation via network pruning.md\":\"CXhqVIIP\",\"column_paper_knowledge transfer with simulated inter-image erasing for weakly supervised semantic segmentation.md\":\"DkMDx5sU\",\"column_paper_lgad local and global attention distillation for efficient semantic segmentation.md\":\"Cb4xDhBp\",\"column_paper_llmformer large languagemodel for open-vocabulary semantic.md\":\"Ccex-Kz7\",\"column_paper_learninggeneralizedmedicalimagesegmentationfromdecoupledfeaturequeries.md\":\"CdQFBI_8\",\"column_paper_night-time_semantic_segmentation.md\":\"DJzPZT2O\",\"column_paper_pat.md\":\"DKRbD8wz\",\"column_paper_pixel-wise reclassification with prototypes for enhancing weakly supervised semantic segmentation.md\":\"wjFNBiTi\",\"column_paper_progressive feature self-reinforcement for weakly supervised semantic segmentation.md\":\"Bz2Iw2_g\",\"column_paper_prompting_multi-moda_segmetation.md\":\"DmdlIrGO\",\"column_paper_relevant intrinsic feature enhancement network for few-shot semantic segmentation.md\":\"CIwYzrZq\",\"column_paper_rolling-unet revitalizing mlp ability to efficiently extract long-distance dependencies for medical image segmentation.md\":\"DEYHnjz9\",\"column_paper_sed.md\":\"DbHvUdVr\",\"column_paper_sfc shared feature calibration in weakly supervised semantic segmentation.md\":\"DDZGFZey\",\"column_paper_scaling_upmulti-domain_semantic_segmentation_with_sentence.md\":\"F2NiI7vl\",\"column_paper_scribbl_hides_class_promoting_scribble-based_weakly-supervised_semantic_segmentation_with its class label.md\":\"1bs0Ja28\",\"column_paper_scribble-supervised semantic segmentation with prototype-based feature augmentation.md\":\"BVoQReZg\",\"column_paper_segment anything.md\":\"yhSyJ7YY\",\"column_paper_self-supervised_vit.md\":\"BvYOGH04\",\"column_paper_towards open-vocabulary semantic segmentation without semantic labels.md\":\"BMOyuhP1\",\"column_paper_use universal segment embeddings for open-vocabulary image segmentation.md\":\"xpWkHt_L\",\"column_paper_visual studio code latex.md\":\"BxdQOwVJ\",\"column_paper_weakclip adapting clip for weakly-supervised semantic.md\":\"CyfcrqQl\",\"column_paper_index.md\":\"fnvh22RV\",\"column_puruse_index.md\":\"CwkP3FgJ\",\"column_puruse_template.md\":\"BwGbn-mp\",\"column_image_segmentation_20250224-语义分割概述.md\":\"B-REzy5A\",\"column_image_segmentation_20250312-pytorch教程.md\":\"z5badtzk\",\"column_image_segmentation_220250224-语义分割上采样.md\":\"B1ieAyjw\",\"column_image_segmentation_fcn模型讲解.md\":\"DYHO_Jwt\",\"column_image_segmentation_index.md\":\"Dy8LErtJ\",\"column_image_segmentation_segment algrothm.md\":\"DnHKxDrE\",\"column_image_segmentation_图像分割基础.md\":\"BEmiWSgw\",\"column_image_segmentation_语义分割基础模型.md\":\"Dx3_L7Gy\",\"index.md\":\"DZpaRy8Y\",\"markdown-examples.md\":\"DWpELabp\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"lab blog\",\"description\":\"A VitePress Site\",\"base\":\"/blog/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"darkModeSwitchLabel\":\"深浅模式\",\"logo\":\"/b.jpg\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"论文阅读笔记\",\"link\":\"/column/Paper/\"},{\"text\":\"论文精读笔记\",\"link\":\"/column/Puruse/\"},{\"text\":\"图像分割\",\"link\":\"/column/image_segmentation\"}],\"sidebarMenuLabel\":\"目录\",\"returnToTopLabel\":\"返回顶部\",\"sidebar\":{\"/column/Paper/\":[{\"text\":\"论文阅读\",\"collapsed\":true,\"items\":[{\"text\":\"latex环境配置\",\"link\":\"/column/Paper/Visual Studio Code latex\"},{\"text\":\"Segment Anything\",\"link\":\"/column/Paper/Segment Anything\"},{\"text\":\"领域泛化语义分割\",\"link\":\"/column/Paper/DGSS\"},{\"text\":\"基于分层编码器的开放词汇语义分割\",\"link\":\"/column/Paper/SED\"},{\"text\":\"超高分辨率分割\",\"link\":\"/column/Paper/High_Quality_Segmentation\"},{\"text\":\"夜间场景语义分割\",\"link\":\"/column/Paper/Night-time_Semantic_Segmentation\"},{\"text\":\"提示词迁移的少样本分割\",\"link\":\"/column/Paper/PAT\"},{\"text\":\"多模态图像分割\",\"link\":\"/column/Paper/Prompting_Multi-Moda_Segmetation\"},{\"text\":\"基于Transformer的自适应原型匹配网络\",\"link\":\"/column/Paper/A Transformer-basedAdaptivePrototypeMatchingNetwork\"},{\"text\":\"跨领域少样本语义分割\",\"link\":\"/column/Paper/Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation\"},{\"text\":\"相关内在特征增强的少样本与意义分割\",\"link\":\"/column/Paper/Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation\"},{\"text\":\"基于涂鸦的无监督语义分割\",\"link\":\"/column/Paper/Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation\"},{\"text\":\"面向弱监督语义分割的渐进式特征自增强\",\"link\":\"/column/Paper/Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于自监督Vit的语义分割\",\"link\":\"/column/Paper/Self-supervised_ViT\"},{\"text\":\"医学图像分割：基于解耦特征查询\",\"link\":\"/column/Paper/LearningGeneralizedMedicalImageSegmentationfromDecoupledFeatureQueries\"},{\"text\":\"基于语句嵌入的多领域语义分割\",\"link\":\"/column/Paper/Scaling_UpMulti-domain_Semantic_Segmentation_with_Sentence\"},{\"text\":\"基于涂鸦的弱监督语义分割\",\"link\":\"/column/Paper/Scribbl_Hides_Class_Promoting_Scribble-Based_Weakly-Supervised_Semantic_Segmentation_with Its Class Label\"}]},{\"text\":\"语义分割论文阅读\",\"collapsed\":false,\"items\":[{\"text\":\"涂鸦监督语义分割的确定性和一致性(CC4S)\",\"link\":\"/column/Paper/CC4S Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation\"},{\"text\":\"基于关联匹配的标签传播半监督语义分割\",\"link\":\"column/Paper/CorrMatch Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation\"},{\"text\":\"基于LLM的开放词汇语义分割\",\"link\":\"/column/Paper/LLMFormer Large LanguageModel for Open-Vocabulary Semantic\"},{\"text\":\"无需语义标签的开放词汇语义分割(文章晦涩，不建议阅读)\",\"link\":\"/column/Paper/Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels\"},{\"text\":\"面向开放词汇图像分割的通用片段嵌入\",\"link\":\"/column/Paper/USE Universal Segment Embeddings for Open-Vocabulary Image Segmentation\"},{\"text\":\"面向弱监督语义分割的类别标记注入\",\"link\":\"/column/Paper/Class Tokens Infusion for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于全局和局部注意力蒸馏的高效语义分割\",\"link\":\"/column/Paper/LGAD Local and Global Attention Distillation for Efficient Semantic Segmentation\"},{\"text\":\"基于双重语义度量学习的少样本航拍图像语义分割\",\"link\":\"/column/Paper/DSMF-Net Dual Semantic Metric Learning Fusion Network for Few-Shot Aerial Image Semantic Segmentation\"},{\"text\":\"基于剪枝的领域泛化语义分割\",\"link\":\"/column/Paper/Kill Two Birds with One Stone Domain Generalization for Semantic Segmentation via Network Pruning\"},{\"text\":\"基于全对一致性学习的弱监督语义分割\",\"link\":\"/column/Paper/All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation\"},{\"text\":\"weakCLIP\",\"link\":\"/column/Paper/WeakCLIP Adapting CLIP for Weakly-Supervised Semantic\"},{\"text\":\"弱监督语义分割中的共享权重校准\",\"link\":\"/column/Paper/SFC Shared Feature Calibration in Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于模拟图像间擦除知识迁移的弱监督语义分割\",\"link\":\"/column/Paper/Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation\"},{\"text\":\"基于原型的像素级再分类提高弱监督语义分割\",\"link\":\"/column/Paper/Pixel-Wise Reclassification with Prototypes for Enhancing Weakly Supervised Semantic Segmentation\"},{\"text\":\"C-CAM\",\"link\":\"/column/Paper/C-CAM Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image\"},{\"text\":\"医学图像分割：Rolling-Net\",\"link\":\"/column/Paper/Rolling-Unet Revitalizing MLP Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation\"}]}],\"/column/image_segmentation/\":[{\"text\":\"图像分割原理及概念\",\"collapsed\":false,\"items\":[{\"text\":\"语义分割概述\",\"link\":\"/column/image_segmentation/20250224-语义分割概述\"},{\"text\":\"语义分割上采样\",\"link\":\"/column/image_segmentation/220250224-语义分割上采样\"},{\"text\":\"图像分割基础\",\"link\":\"/column/image_segmentation/图像分割基础\"},{\"text\":\"语义分割基础模型\",\"link\":\"/column/image_segmentation/语义分割基础模型\"},{\"text\":\"FCN模型讲解\",\"link\":\"/column/image_segmentation/FCN模型讲解\"}]},{\"text\":\"卷积网络\",\"collapsed\":false,\"items\":[{\"text\":\"卷积网络\",\"link\":\"/column/image_segmentation/20250312-Pytorch教程\"},{\"text\":\"分割算法(同济子豪兄)\",\"link\":\"/column/image_segmentation/segment algrothm\"}]}],\"/column/Puruse/\":[{\"text\":\"论文精读\",\"collapsed\":false,\"items\":[{\"text\":\"精读模板\",\"link\":\"/column/Puruse/template\"}]}]},\"lastUpdated\":{\"text\":\"最后更新于\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}},\"editLink\":{\"pattern\":\"https://github.com/Louaq/blog/tree/main/docs/:path\",\"text\":\"在github上编辑此页面\"},\"outline\":{\"level\":[2,6],\"label\":\"当前大纲\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the <a href=\\\"https://mit-license.org/\\\">MIT License.</a> | \\n    本站访客数 <span id=\\\"busuanzi_value_site_uv\\\"></span> 人次\",\"copyright\":\"Copyright © 2024-2025\"},\"i18nRouting\":true},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>